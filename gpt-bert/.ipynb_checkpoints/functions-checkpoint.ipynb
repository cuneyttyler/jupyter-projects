{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_clusters():\n",
    "    clusters = []\n",
    "    for i,d in enumerate(data):\n",
    "        print('%.2f%%' % ((i * 100) / len(data))) if i % 10 == 0 else None\n",
    "        nodes, edges, edge_types = [], [], []\n",
    "        for dd in d['data']:\n",
    "            graph = dd['graph']\n",
    "            for node in graph['nodes']:\n",
    "                nodes.append(node)\n",
    "            \n",
    "            for edge, edge_type in zip(graph['edges'], graph['edge_types']):\n",
    "                if not any(edge['source'] == e['source'] and edge['target'] == e['target'] for e in edges):\n",
    "                    edges.append({'source': edge['source'], 'target': edge['target']}) \n",
    "                    edge_types.append(edge_type)\n",
    "        \n",
    "                                       \n",
    "        clusters.append({'nodes': nodes, 'edges': edges, 'edge_types': edge_types})\n",
    "        \n",
    "    with open('data/node_clusters_v3.pkl', 'wb') as outp:\n",
    "        pickle.dump(clusters, outp, pickle.HIGHEST_PROTOCOL)\n",
    "                                       \n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clusters(clusters):\n",
    "    result = []\n",
    "    for cluster in clusters:\n",
    "        new_cluster = []\n",
    "        counts = [(el,cluster['nodes'].count(el)) for el in set(cluster['nodes'])]\n",
    "        for count in counts:\n",
    "            new_cluster.append(count[0]) if count[1] > 13 else None\n",
    "                \n",
    "        result.append({'nodes':new_cluster})\n",
    "    \n",
    "    with open('data/node_clusters_v3_filtered.pkl', 'wb') as outp:\n",
    "        pickle.dump(result, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# clusters_v3_filtered = filter_clusters(clusters_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_with_count(clusters):\n",
    "    result = []\n",
    "    for cluster in clusters:\n",
    "        new_cluster = []\n",
    "        counts = [(el,cluster['nodes'].count(el)) for el in set(cluster['nodes'])]\n",
    "        for count in counts:\n",
    "            new_cluster.append(count)\n",
    "                \n",
    "        result.append({'nodes':new_cluster})\n",
    "    \n",
    "    with open('data/node_clusters_v3_with_count.pkl', 'wb') as outp:\n",
    "        pickle.dump(result, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# clusters_v3_with_count = cluster_with_count(clusters_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_node_cluster_mapping(clusters):\n",
    "    mapping = {}\n",
    "    for i, node in enumerate(graph['nodes']):\n",
    "        print('%.2f%%' % ((i * 100) / len(graph['nodes']))) if i % 500 == 0 else None\n",
    "\n",
    "        node_clusters = [unique_labels[i] for i,cluster in enumerate(clusters) if node in cluster['nodes']]\n",
    "        mapping[node] = node_clusters if len(node_clusters) > 0 else 'None'\n",
    "                    \n",
    "    with open('data/node_cluster_mapping_v2.pkl', 'wb') as outp:\n",
    "        pickle.dump(mapping, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return mapping\n",
    "    \n",
    "# node_cluster_mapping_v3 = get_node_cluster_mapping(clusters_v3_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_count(cluster,node):\n",
    "    return [n[1] for n in cluster['nodes'] if n[0] == node][0]\n",
    "\n",
    "def get_node_cluster_mapping_with_count(clusters):\n",
    "    mapping = {}\n",
    "    for i, node in enumerate(graph['nodes']):\n",
    "        print('%.2f%%' % ((i * 100) / len(graph['nodes']))) if i % 500 == 0 else None\n",
    "\n",
    "        node_clusters = [(unique_labels[i],cluster_count(cluster,node)) for i,cluster in enumerate(clusters) if node in [c[0] for c in cluster['nodes']]]\n",
    "        node_clusters = sorted(node_clusters, key=lambda c: c[1], reverse=True)\n",
    "        mapping[node] = node_clusters if len(node_clusters) > 0 else 'None'\n",
    "                            \n",
    "    with open('data/node_cluster_mapping_v3_with_count.pkl', 'wb') as outp:\n",
    "        pickle.dump(mapping, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "# node_cluster_mapping_v3_with_count = get_node_cluster_mapping_with_count(clusters_v3_with_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "graph_embedding_size=100\n",
    "\n",
    "def get_embedding(G):\n",
    "        walk_length = 10\n",
    "        rw = BiasedRandomWalk(G)\n",
    "        walks = rw.run(\n",
    "            nodes=G.nodes(),  # root nodes\n",
    "            length=walk_length,  # maximum length of a random walk\n",
    "            n=10,  # number of random walks per root node\n",
    "            p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "            q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "            weighted=False,  # for weighted random walks\n",
    "            seed=42,  # random seed fixed for reproducibility\n",
    "        )\n",
    "\n",
    "        model = Word2Vec(\n",
    "            walks,  vector_size=graph_embedding_size, window=5, min_count=0, sg=1, workers=1\n",
    "        )\n",
    "\n",
    "        return model.wv.vectors\n",
    "    \n",
    "    \n",
    "def get_embeddings(clusters):\n",
    "    all_embeddings = []\n",
    "    for i, cluster in clusters:\n",
    "        edges_ = pd.DataFrame({\n",
    "                'source': [e['source'] for e in cluster['edges']],\n",
    "                'target': [e['target'] for e in cluster['edges']],\n",
    "                'type': cluster['edge_types']\n",
    "            })\n",
    "\n",
    "        G = StellarGraph(IndexedArray(index=graph['nodes']), edges_, edge_type_column=\"type\")\n",
    "\n",
    "        node_embeddings = get_embedding(G)\n",
    "        \n",
    "        all_embeddings.append(node_embeddings)\n",
    "    \n",
    "    with open('data/node_embeddings_v4.pkl', 'wb') as outp:\n",
    "        pickle.dump(all_embeddings, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_embedding_size = 100\n",
    "\n",
    "def get_embedding(G):\n",
    "        walk_length = 10\n",
    "        rw = BiasedRandomWalk(G)\n",
    "        walks = rw.run(\n",
    "            nodes=G.nodes(),  # root nodes\n",
    "            length=walk_length,  # maximum length of a random walk\n",
    "            n=10,  # number of random walks per root node\n",
    "            p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "            q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "            weighted=False,  # for weighted random walks\n",
    "            seed=42,  # random seed fixed for reproducibility\n",
    "        )\n",
    "\n",
    "        model = Word2Vec(\n",
    "            walks,  vector_size=graph_embedding_size, window=5, min_count=0, sg=1, workers=1\n",
    "        )\n",
    "\n",
    "        return model.wv.vectors\n",
    "    \n",
    "def get_graph():\n",
    "    nodes, edges, edge_types = [], [], []\n",
    "    for i1,d in enumerate(data):\n",
    "        print('%.2f%%' % ((i1 * 100) / len(data))) if i1 % 10 == 0 else None\n",
    "        for dd in d['data']:\n",
    "            g = dd['context_graph']\n",
    "            for node in g['nodes']:\n",
    "                nodes.append(node) if not node in nodes else None\n",
    "\n",
    "            for i, edge in enumerate(g['edges']):\n",
    "                if not any(e['source'] == edge['source'] and e['target'] == edge['target'] for e in edges):\n",
    "                    edges.append({'source': edge['source'], 'target': edge['target']})\n",
    "                    edge_types.append(g['edge_types'][i])\n",
    "    \n",
    "    \n",
    "    with open('data/graph_v2.pkl', 'wb') as outp:\n",
    "        pickle.dump({'nodes': nodes, 'edges': edges, 'edge_types': edge_types}, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return {'nodes':nodes, 'edges': edges, 'edge_types': edge_types}\n",
    "\n",
    "def get_embeddings():\n",
    "    graph = get_graph()\n",
    "    # graph = unpickler('.','graph.pkl')\n",
    "\n",
    "    edges_ = pd.DataFrame({\n",
    "            'source': [e['source'] for e in graph['edges']],\n",
    "            'target': [e['target'] for e in graph['edges']],\n",
    "            'type': graph['edge_types']\n",
    "        })\n",
    "    \n",
    "    G = StellarGraph(IndexedArray(index=graph['nodes']), edges_, edge_type_column=\"type\")\n",
    "\n",
    "    node_embeddings = get_embedding(G)\n",
    "    \n",
    "    with open('data/node_embeddings_v4.pkl', 'wb') as outp:\n",
    "        pickle.dump(node_embeddings, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return node_embeddings, graph\n",
    "\n",
    "node_embeddings_v4, graph_v2 = get_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "X = [['Art-Painting'],['Science-Biology'],['Science-Medicine']]\n",
    "enc.fit(X)\n",
    "encodings = enc.transform([['Art-Painting'],['Science-Biology']]).toarray()\n",
    "multi_hot_encoding = np.zeros(3)\n",
    "for enc in encodings:\n",
    "    for i,val in enumerate(enc):\n",
    "        multi_hot_encoding[i] = 1 if multi_hot_encoding[i] == 1 or val == 1 else 0\n",
    "\n",
    "multi_hot_encoding"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
