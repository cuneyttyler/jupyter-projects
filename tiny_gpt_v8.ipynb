{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2424ed0-4ebc-4c6d-bff7-beaffeea95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow verions: 2.4.0\n",
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "# from tensorflow.keras import ops\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph, IndexedArray\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "from stellargraph.mapper import AdjacencyPowerGenerator\n",
    "from stellargraph.layer import WatchYourStep\n",
    "from stellargraph import datasets, utils\n",
    "from tensorflow.keras import callbacks, optimizers, losses, metrics, regularizers, Model\n",
    "\n",
    "from stellargraph.mapper import KGTripleGenerator\n",
    "from stellargraph.layer import ComplEx\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from graph_visualization import GraphVisualization\n",
    "\n",
    "print(\"Tensorflow verions:\", tf.__version__)\n",
    "print('Available GPUs:', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93b860e0-b6d6-49cc-8275-1d6b9beeb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "def causal_attention_mask_(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "    def get_weights(self):\n",
    "        return [self.att.get_weights(), self.ffn.layers[0].get_weights(), self.ffn.layers[1].get_weights()]\n",
    "\n",
    "    def set_weights(self, weights):\n",
    "        self.att.set_weights(weights[0])\n",
    "        self.ffn.layers[0].set_weights(weights[1])\n",
    "        self.ffn.layers[1].set_weights(weights[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0ae20fa-e191-4b5f-92e7-7fd3d05d971b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 files\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 30  # Only consider the top 20k words\n",
    "maxlen = 10  # Max sequence size\n",
    "batch_size = 128\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"gpt-dataset/custom-2.6\"\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "# filenames = filenames[:10000]\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf_data.TextLineDataset(filenames)\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf_strings.lower(input_string)\n",
    "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def count_occurences(text_ds):\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Custom text preprocessing\n",
    "        # For example, you can perform lowercasing, punctuation removal, etc.\n",
    "        return text.lower()\n",
    "    \n",
    "    # Preprocess the text data and convert it to a list\n",
    "    preprocessed_texts = [preprocess_text(str(text.numpy())) for text in text_ds]\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    # Fit CountVectorizer on preprocessed text data and transform it into token counts\n",
    "    token_counts = vectorizer.fit_transform(preprocessed_texts)\n",
    "    # Get the vocabulary and token counts\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    counts = token_counts.toarray().sum(axis=0)\n",
    "    # Create a dictionary to store token counts\n",
    "    token_counts_dict = dict(zip(vocab, counts))\n",
    "\n",
    "    token_counts_dict = sorted(token_counts_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(token_counts_dict)\n",
    "\n",
    "# count_occurences(text_ds)\n",
    "\n",
    "def filter_dataset(line):\n",
    "    return not tf.strings.regex_full_match(line, \".*\\\\bpiano\\\\b.*\")\n",
    "\n",
    "text_ds = text_ds.filter(lambda line: filter_dataset(line))\n",
    "\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "# for text in text_ds:\n",
    "#     print(str(text.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d95d8db-170b-4421-b463-760d5355fe51",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "\n",
    "    # zeros_column = tf.zeros_like(y[:, :1])  # Create a column of zeros with the same shape as the first column of y\n",
    "    # y = tf.concat([zeros_column, y], axis=1)  # Concatenate the zeros column with y along the column axis\n",
    "\n",
    "    return x, y\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)\n",
    "\n",
    "def prepare_node_vector_outputs(text_ds, node_embeddings):\n",
    "    def process(x,y):\n",
    "        return x, tf.gather(node_embeddings, y)\n",
    "        \n",
    "    return text_ds.map(process, num_parallel_calls=tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "046753c0-b525-48c4-9b59-b6e6e5948c5c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sources = ['guitar', 'guitar', 'guitar', 'violin', 'violin', 'violin', 'piano', 'piano', 'piano', \n",
    "           'car', 'car', 'car', 'truck', 'truck', 'truck', 'neck', 'strings', 'keys', 'pedals', \n",
    "           'wheels', 'baggage', 'trunk', 'instrument', 'vehicle', 'trombone', 'trombone', 'trombone', 'bell', 'slide']\n",
    "labels = ['is', 'has', 'has', 'is', 'has', 'has', 'is', 'has', 'has', 'is', 'has', 'has', 'is',\n",
    "          'has', 'has', 'is', 'is', 'is', 'is', 'is', 'is', 'is', 'has', 'has', 'is', 'has', 'has', 'is', 'is']\n",
    "targets = ['instrument', 'neck', 'strings', 'instrument', 'neck', 'strings', 'instrument', 'keys', \n",
    "           'pedals', 'vehicle', 'wheels', 'baggage', 'vehicle', 'wheels', 'trunk', 'part', 'part', \n",
    "           'part', 'part', 'part', 'part', 'part', 'part', 'part', 'instrument', 'bell', 'slide', 'part', 'part']\n",
    "\n",
    "sources.extend(['organ', 'organ', 'organ'])\n",
    "labels.extend(['is', 'has', 'has'])\n",
    "targets.extend(['instrument', 'keys', 'pedals'])\n",
    "\n",
    "graph_data = pd.DataFrame({'source': sources, 'label': labels, 'target': targets})\n",
    "\n",
    "rel_graphs = {}\n",
    "for index, row in graph_data.iterrows():\n",
    "    df = rel_graphs.get(row['label'], pd.DataFrame({'source': [], 'label': [], 'target': []}))\n",
    "    df = df.append({'source': row['source'], 'label': row['label'], 'target': row['target']}, ignore_index=True)\n",
    "    rel_graphs[row['label']] = df\n",
    "    \n",
    "# graph_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96655133-7e51-43fd-a1a6-ac4abcc7d9ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_embedding_size = 10\n",
    "\n",
    "def node2vec(G):\n",
    "        walk_length = 100\n",
    "        rw = BiasedRandomWalk(G)\n",
    "        walks = rw.run(\n",
    "            nodes=G.nodes(),  # root nodes\n",
    "            length=walk_length,  # maximum length of a random walk\n",
    "            n=10,  # number of random walks per root node\n",
    "            p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "            q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "            weighted=False,  # for weighted random walks\n",
    "            seed=42,  # random seed fixed for reproducibility\n",
    "        )\n",
    "\n",
    "        model = Word2Vec(\n",
    "            walks,  vector_size=graph_embedding_size, window=5, min_count=0, sg=1, workers=1\n",
    "        )\n",
    "\n",
    "        return pd.DataFrame([(key, vector) for key,vector in zip(model.wv.index_to_key, model.wv.vectors)], columns=['node', 'embedding'])\n",
    "\n",
    "def fill_graph(graph, vocab):\n",
    "    for word in vocab:\n",
    "        graph['nodes'].append(word) if word not in graph['nodes'] else None\n",
    "\n",
    "    return graph\n",
    "    \n",
    "def get_graph(graph_data):\n",
    "    nodes, edges, edge_types = [], [], []\n",
    "\n",
    "    for word in graph_data['source'].unique():\n",
    "        nodes.append(word)\n",
    "\n",
    "    for word in graph_data['target'].unique():\n",
    "        nodes.append(word) if word not in nodes else None\n",
    "\n",
    "    for index, row in graph_data.iterrows():\n",
    "        edges.append({'source': row['source'], 'target': row['target']})\n",
    "        edge_types.append(row['label'])\n",
    "\n",
    "    graph = {'nodes':nodes, 'edges': edges, 'edge_types': edge_types}\n",
    "\n",
    "    graph = fill_graph(graph, vocab)\n",
    "    \n",
    "    edges_ = pd.DataFrame({\n",
    "            'source': [e['source'] for e in graph['edges']],\n",
    "            'target': [e['target'] for e in graph['edges']],\n",
    "            'type': graph['edge_types']\n",
    "        })\n",
    "\n",
    "    G = StellarGraph(IndexedArray(index=graph['nodes']), edges_, edge_type_column=\"type\")\n",
    "    \n",
    "    return graph, G\n",
    "\n",
    "def get_embeddings(G):\n",
    "    node_embeddings = complex_embeddings(G, conceptnet_data[['source','label','target']])\n",
    "        \n",
    "    return node_embeddings\n",
    "    \n",
    "graph, G = get_graph(graph_data)\n",
    "full_embeddings = node2vec(G)\n",
    "\n",
    "tmp = []\n",
    "for i,word in enumerate(vocab):\n",
    "    lower_nodes = list(map(lambda k: k.lower(), full_embeddings['node']))\n",
    "    index = lower_nodes.index(word.lower()) if word.lower() in lower_nodes else -1\n",
    "    if index != -1:\n",
    "        tmp.append(full_embeddings.iloc[index][1])\n",
    "    else:\n",
    "        tmp.append(np.ones(graph_embedding_size) * -10)\n",
    "\n",
    "full_embeddings = np.array(tmp)\n",
    "\n",
    "graphs, node_embeddings = [], []\n",
    "for key, _graph_data in rel_graphs.items():\n",
    "    _graph, G = get_graph(_graph_data)\n",
    "    graphs.append(_graph)\n",
    "    node_embeddings.append(node2vec(G))\n",
    "\n",
    "for j, emb in enumerate(node_embeddings):\n",
    "    tmp = []\n",
    "    lower_nodes = list(map(lambda k: k.lower(), emb['node']))\n",
    "    for i,word in enumerate(vocab):\n",
    "        index = lower_nodes.index(word.lower()) if word.lower() in lower_nodes else -1\n",
    "        if index != -1:\n",
    "            tmp.append(emb.iloc[index][1])\n",
    "        else:\n",
    "            tmp.append(np.ones(graph_embedding_size) * -10)\n",
    "    \n",
    "    node_embeddings[j] = np.array(tmp).tolist()\n",
    "\n",
    "node_embeddings = np.array(node_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81deed0a-d102-43b0-a6b8-2410723048d0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings = None\n",
    "for i in range(0, node_embeddings.shape[0] - 1):\n",
    "    embeddings =  np.concatenate((node_embeddings[i], node_embeddings[i + 1]), axis=1)\n",
    "    final_embeddings = embeddings if not final_embeddings else np.concatenate((final_embeddings, embeddings), axis = 1) \n",
    "\n",
    "# final_embeddings = node_embeddings[0]\n",
    "# final_embeddings = node_embeddings[0]\n",
    "# final_embeddings = np.concatenate((final_embeddings, full_embeddings), axis=1)\n",
    "final_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c32efc5-96d2-44e4-8816-76e0efb7fae9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "_graph = graph\n",
    "# _graph = graphs[0]\n",
    "index = 0\n",
    "vis = GraphVisualization()\n",
    "for edge in _graph['edges'][index:]:\n",
    "    vis.addEdge(edge['source'], edge['target'])\n",
    "# vis.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6dd4d40-79b8-4a82-9202-3b172a79bbcb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(A,B):\n",
    "    return np.dot(A,B)/(norm(A)*norm(B))\n",
    "\n",
    "def euclidean_distance(vector1, vector2):\n",
    "    \"\"\"Calculate the Euclidean distance between two vectors.\"\"\"\n",
    "    # Ensure both vectors are NumPy arrays\n",
    "    vector1 = np.array(vector1)\n",
    "    vector2 = np.array(vector2)\n",
    "    \n",
    "    # Calculate the squared differences between corresponding elements\n",
    "    squared_diff = np.square(vector1 - vector2)\n",
    "    \n",
    "    # Sum the squared differences and take the square root\n",
    "    distance = np.sqrt(np.sum(squared_diff))\n",
    "    \n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a45ad26-9e27-4006-877c-2a722c625092",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# index_A = vocab.index('guitar')\n",
    "# index_B = vocab.index('neck')\n",
    "# index_C = vocab.index('piano')\n",
    "# index_D = vocab.index('keys')\n",
    "# index_E = vocab.index('trombone')\n",
    "# index_F = vocab.index('bell')\n",
    "# index_G = vocab.index('instrument')\n",
    "\n",
    "# embeddings = node_embeddings[0]\n",
    "# # embeddings = final_embeddings\n",
    "# A,B,C,D,E,F,G = embeddings[index_A], embeddings[index_B], embeddings[index_C], embeddings[index_D], embeddings[index_E], embeddings[index_F], embeddings[index_G]\n",
    "# print('Cosine Similarities: ')\n",
    "# print('A, B: ', cosine_sim(A,B), '\\nA, C: ', cosine_sim(A,C), '\\nA, D: ', cosine_sim(A,D), \n",
    "#       '\\nB, C: ',cosine_sim(B,C), '\\nB, D: ', cosine_sim(B,D), '\\nC, D: ', cosine_sim(C,D),\n",
    "#       '\\nA, E: ',cosine_sim(A,E), '\\nB, E: ', cosine_sim(B,E), '\\nE, F: ', cosine_sim(E,F),\n",
    "#       '\\nC, F: ',cosine_sim(C,F), '\\nD, F: ', cosine_sim(D,F), '\\nA, G: ', cosine_sim(A,G),\n",
    "#       '\\nB, G: ', cosine_sim(B,G), '\\nE, G: ', cosine_sim(E,G))\n",
    "# print('Euclidean Distances: ')\n",
    "# print('A, B: ', euclidean_distance(A,B), '\\nA, C: ', euclidean_distance(A,C), '\\nA, D: ', euclidean_distance(A,D), \n",
    "#       '\\nB, C: ',euclidean_distance(B,C), '\\nB, D: ', euclidean_distance(B,D), '\\nC, D: ', euclidean_distance(C,D),\n",
    "#       '\\nA, E: ',euclidean_distance(A,E), '\\nB, E: ', euclidean_distance(B,E), '\\nE, F: ', euclidean_distance(E,F),\n",
    "#       '\\nC, F: ',euclidean_distance(C,F), '\\nD, F: ', euclidean_distance(D,F), '\\nA, G: ', euclidean_distance(A,G),\n",
    "#       '\\nB, G: ', euclidean_distance(B,G), '\\nC, G: ', euclidean_distance(C,G), '\\nD, G: ', euclidean_distance(D,G),\n",
    "#       '\\nE, G: ', euclidean_distance(E,G), '\\nF, G: ', euclidean_distance(F,G))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7065158-ddea-4d4d-ae99-04e59c540903",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_ds = prepare_node_vector_outputs(text_ds, final_embeddings)\n",
    "graph_ds = graph_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6930c678-79a1-4f5d-95f2-88493276a45f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 10), (None, 10, 20)), types: (tf.int64, tf.float64)>,\n",
       " <PrefetchDataset shapes: ((None, 10), (None, 10)), types: (tf.int64, tf.int64)>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_ds, text_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8823209-4d3f-4a41-978a-3de62eabef51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [(word, emb) for word, emb in zip(vocab, final_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa7b441-9e76-41c3-af92-55bc9c7b1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, node_embeddings, model = None, top_k=10, print_every=1, method='euclidean'\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        if model: self.model = model\n",
    "        self.yes_count = 0\n",
    "        self.no_count = 0\n",
    "        self.activation_outputs = []\n",
    "        self.gradients = []\n",
    "        self.node_embeddings = node_embeddings\n",
    "        self.normalized_node_embeddings = tf.nn.l2_normalize(tf.constant(node_embeddings, dtype=tf.float32), axis=1)\n",
    "        self.method = method\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = tf.convert_to_tensor(indices, dtype=tf.int32)\n",
    "        preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = tf.convert_to_tensor(preds, dtype=tf.float32)\n",
    "  \n",
    "        # Reshape logits to a matrix\n",
    "        logits_matrix = tf.reshape(logits, (1, -1))\n",
    "   \n",
    "        # Sample from the softmax probabilities\n",
    "        sampled_index = tf.random.categorical(tf.math.log(preds)[tf.newaxis, :], num_samples=1)\n",
    "\n",
    "        # Return the sampled index\n",
    "        return indices[sampled_index[0, 0]]\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number] if number < len(self.index_to_word) else '---'\n",
    "\n",
    "    def euclidean_distance(self, y):\n",
    "        t_1 = tf.cast(self.node_embeddings, dtype=tf.float32)\n",
    "        t_2 = tf.cast(tf.tile(tf.expand_dims(y,0), [self.node_embeddings.shape[0], 1]), dtype=tf.float32)\n",
    "        squared_diff = tf.square(t_1 - t_2)\n",
    "        \n",
    "        # Sum the squared differences along the appropriate axis\n",
    "        distances = tf.sqrt(tf.reduce_sum(squared_diff, axis=-1))\n",
    "        distances =  tf.transpose(distances) \n",
    "\n",
    "        # distances, indices = tf.math.top_k(tf.negative(distances), k=self.k, sorted=True)\n",
    "        # preds = tf.keras.activations.softmax(tf.expand_dims(distances,0))[0]\n",
    "        # preds = tf.convert_to_tensor(preds, dtype=tf.float32)\n",
    "\n",
    "        # sampled_index = tf.random.categorical(tf.negative(tf.math.log(preds))[tf.newaxis, :], num_samples=1)\n",
    "    \n",
    "        return distances, tf.argmin(distances)\n",
    "    \n",
    "    def cosine_similarity(self, y):\n",
    "  \n",
    "        # Calculate the dot product of the matrix and the vector\n",
    "        dot_product = tf.reduce_sum(self.node_embeddings * y, axis=-1)\n",
    "        \n",
    "        # Calculate the magnitudes of the matrix and the vector\n",
    "        matrix_magnitude = tf.norm(self.node_embeddings, axis=-1)\n",
    "        vector_magnitude = tf.norm(y)\n",
    "        \n",
    "        # Calculate the cosine similarity\n",
    "        cosine_similarity = dot_product / (matrix_magnitude * vector_magnitude)\n",
    "                \n",
    "        # similarities, indices = tf.math.top_k(similarities, k=self.k, sorted=True)\n",
    "        # preds = tf.keras.activations.softmax(similarities)[0]\n",
    "        # preds = tf.convert_to_tensor(preds, dtype=tf.float32)\n",
    "\n",
    "        # indices = indices[0]\n",
    "\n",
    "        # sampled_index = tf.random.categorical(tf.math.log(preds), num_samples=1)\n",
    "        \n",
    "        # Return the sampled index\n",
    "        return cosine_similarity, tf.argmax(cosine_similarity)\n",
    "        \n",
    "    def generate_token(self, start_tokens, tokens_generated):\n",
    "        pad_len = maxlen - len(start_tokens)\n",
    "        sample_index = len(start_tokens) - 1\n",
    "        if pad_len < 0:\n",
    "            x = start_tokens[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            x = start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = start_tokens\n",
    "        x = np.array([x])\n",
    "        y = self.model.predict(x, verbose=0)\n",
    "        measurements, sample_token = self.euclidean_distance(y[0][sample_index]) if self.method == 'euclidean' else self.cosine_similarity(y[0][sample_index])\n",
    "        tokens_generated.append(sample_token)\n",
    "        start_tokens.append(sample_token)\n",
    "        num_tokens_generated = len(tokens_generated)\n",
    "\n",
    "        return start_tokens, tokens_generated, num_tokens_generated, y, measurements\n",
    "\n",
    "    def get_generated_text(self, tokens_generated):\n",
    "        return \" \".join(\n",
    "            [self.detokenize(_) for _ in tokens_generated])\n",
    "        \n",
    "    def get_text(self, tokens_generated):\n",
    "        return \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        raw_outputs = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            start_tokens, tokens_generated, num_tokens_generated, raw_output, measurements = self.generate_token(start_tokens, tokens_generated)\n",
    "            raw_outputs.append(raw_output)\n",
    "            \n",
    "        txt = self.get_text(tokens_generated)\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "        self.yes_count += 1 if 'yes' in txt else 0\n",
    "        self.no_count += 1 if 'no' in txt else 0\n",
    "\n",
    "        return txt, raw_outputs\n",
    "    \n",
    "    def generate(self):\n",
    "        return self.on_epoc_end(1)\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "def callback(start_prompt, model=None, top_k=10, method='euclidean'):\n",
    "    start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "    num_tokens_generated = 10\n",
    "    return TextGenerator(num_tokens_generated, start_tokens, vocab, final_embeddings, top_k=top_k, method=method) if not model else TextGenerator(num_tokens_generated, start_tokens, vocab, final_embeddings, model, top_k, method=method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "643dd0da-acfd-4966-aa28-9841f6395271",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]  # Modified line\n",
    "        positions = tf.range(0, maxlen, 1)  # Modified line\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7146048c-63e5-4c02-956a-9990a8c75fed",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class NodeEmbedding(layers.Layer):\n",
    "    def __init__(self, node_embed_dim, node_embeddings, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_embeddings = tf.constant(node_embeddings, dtype=tf.float32)\n",
    "        self.dense = layers.Dense(node_embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "         # Reshape x to add a new dimension for embedding\n",
    "        node_indices = tf.cast(tf.math.round(x), tf.int32)\n",
    "        node_emb = tf.gather(self.node_embeddings, node_indices)\n",
    "        node_emb = self.dense(node_emb)  # Apply dense layer to each token embedding\n",
    "        return node_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bffb5b8-3ee3-406f-83aa-4780955d4da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_output = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "55a846ea-5ac4-4567-87ec-f8fa941def77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         [(None, 10)]              0         \n",
      "_________________________________________________________________\n",
      "node_embedding_layer (NodeEm (None, 10, 256)           5376      \n",
      "_________________________________________________________________\n",
      "node_transformer_layer (Tran (None, 10, 256)           658688    \n",
      "_________________________________________________________________\n",
      "graph_embedding_output (Dens (None, 10, 20)            5140      \n",
      "=================================================================\n",
      "Total params: 669,204\n",
      "Trainable params: 669,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "node_embedding_dim = graph_embedding_size * 2\n",
    "\n",
    "def get_node_embedding_weights(model):\n",
    "    return model.get_layer('node_embedding_layer').get_weights()\n",
    "\n",
    "def set_node_embedding_weights(model, weights):\n",
    "    return model.get_layer('node_embedding_layer').set_weights(weights)\n",
    "\n",
    "def get_transformer_weights(model, transformer_name, output_name):\n",
    "    weights = []\n",
    "    weights.append(model.get_layer(transformer_name).get_weights())\n",
    "    weights.append(model.get_layer(output_name).get_weights())\n",
    "    return weights\n",
    "    \n",
    "def set_transformer_weights(model, transformer_name, output_name, weights):\n",
    "    model.get_layer(transformer_name).set_weights(weights[0])\n",
    "    model.get_layer(output_name).set_weights(weights[1])\n",
    "\n",
    "def cosine_similarity_loss(y_true, y_pred):\n",
    "    if print_output:\n",
    "        for y_p, y_t in zip(y_pred, y_true):\n",
    "            similarities_p_list, tokens_p_list, similarities_t_list, tokens_t_list = [], [], [], []\n",
    "            for i in range(maxlen):\n",
    "                similarities_p, tokens_p = generator.cosine_similarity(y_p[i])\n",
    "                similarities_t, tokens_t = generator.cosine_similarity(y_t[i])\n",
    "                similarities_p_list.append(similarities_p)\n",
    "                tokens_p_list.append(tokens_p)\n",
    "                similarities_t_list.append(similarities_t)\n",
    "                tokens_t_list.append(tokens_t)\n",
    "    \n",
    "            print('real: ', ' '.join([vocab[token] for token in tokens_t_list]))\n",
    "            print('prediction: ', ' '.join([vocab[token] for token in tokens_p_list]))\n",
    "            print('---------')\n",
    "            \n",
    "    # Normalize the vectors\n",
    "    y_true_normalized = tf.nn.l2_normalize(y_true, axis=-1)\n",
    "    y_pred_normalized = tf.nn.l2_normalize(y_pred, axis=-1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    cosine_similarities = tf.reduce_sum(tf.multiply(y_true_normalized, y_pred_normalized), axis=-1)\n",
    "\n",
    "    # if print_output:\n",
    "    # tf.print(1 - cosine_similarities, summarize=-1)\n",
    "    \n",
    "    # Return 1 - cosine similarity as the loss\n",
    "    return 1 - cosine_similarities\n",
    "    \n",
    "def euclidean_distance_loss(y_true, y_pred):\n",
    "    if print_output:\n",
    "        for y_p, y_t in zip(y_pred, y_true):\n",
    "            distances_p_list, tokens_p_list, distances_t_list, tokens_t_list = [], [], [], []\n",
    "            for i in range(maxlen):\n",
    "                distances_p, tokens_p = generator.euclidean_distance(y_p[i])\n",
    "                distances_t, tokens_t = generator.euclidean_distance(y_t[i])\n",
    "                distances_p_list.append(distances_p)\n",
    "                tokens_p_list.append(tokens_p)\n",
    "                distances_t_list.append(distances_t)\n",
    "                tokens_t_list.append(tokens_t)\n",
    "    \n",
    "            print('real: ', ' '.join([vocab[token] for token in tokens_t_list]))\n",
    "            print('prediction: ', ' '.join([vocab[token] for token in tokens_p_list]))\n",
    "            print('---------')\n",
    "\n",
    "    # Calculate the squared differences between corresponding elements\n",
    "    squared_diff = tf.square(y_true - y_pred)\n",
    "    \n",
    "    # Sum the squared differences along the appropriate axis\n",
    "    loss = tf.reduce_sum(squared_diff, axis=-1)\n",
    "\n",
    "    return loss\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"float32\")\n",
    "    \n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, name='embedding_layer')\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim, name='transformer_layer')\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    outputs_1 = layers.Dense(vocab_size, name='text_output')(x)\n",
    "\n",
    "    node_embedding_layer = NodeEmbedding(embed_dim, final_embeddings, name='node_embedding_layer')\n",
    "    x2 = node_embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim, name='node_transformer_layer')\n",
    "    x2 = transformer_block(x2)\n",
    "\n",
    "    # Calculate final output node embeddings\n",
    "    outputs_2 = layers.Dense(node_embedding_dim, name='graph_embedding_output')(x2)\n",
    "    \n",
    "    # Concatenate outputs of both branches\n",
    "    # outputs = layers.Average()([outputs_1, outputs_2])\n",
    "\n",
    "    outputs = outputs_2\n",
    "\n",
    "    # Softmax layer\n",
    "    # softmax_output = layers.Softmax()(outputs)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[outputs])\n",
    "    # loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(0.001),\n",
    "        loss=[euclidean_distance_loss],\n",
    "        metrics=['mse']\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1cce73d0-6f9a-4586-bc1c-b9087b379b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(graph_ds, verbose=0, epochs=1, callbacks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0ebde436-727e-4250-beb3-17f2fc256281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.plot(history.history['mse'])\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model Results')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "# plt.legend(['mse', 'loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9c090925-616c-42c6-85b2-389cd5a5b5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.05827279,  0.21146941, -0.85853446, -0.43476775,  0.40713334,\n",
       "        -0.25533229, -1.6402235 , -1.53358591, -1.00686014, -0.21894898,\n",
       "        -0.7027967 ,  0.25331518,  0.79552972, -0.1075513 ,  2.13654089,\n",
       "         0.67802596,  0.36790338,  0.60496593, -1.54008281,  0.52877772]),\n",
       " array([ 0.08457045,  0.07382888, -0.76764679, -0.2136212 ,  0.42639551,\n",
       "        -0.38833165, -1.69149566, -1.58441067, -0.98150045, -0.37401339,\n",
       "        -0.58786553,  0.30561456,  0.78456491, -0.25285125,  2.040241  ,\n",
       "         0.73948634,  0.36479887,  0.50576001, -1.55167174,  0.54063368]),\n",
       " 0.43025208086981603)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb =  final_embeddings[vocab.index('organ')]\n",
    "yy = final_embeddings[vocab.index('piano')]\n",
    "emb, yy, euclidean_distance(emb,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "76b301e7-6244-4849-b12d-494c12f42a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([-0.21549219, 0.09164345, 0.1297471, 0.14354843],\n",
       " [-0.5062725, 0.06012138, 1.0717416, 1.781855],\n",
       " 0.7833428784823357,\n",
       " 1.9123150590099414,\n",
       " array([8.45531887e-02, 9.93640897e-04, 8.87353638e-01, 2.68404842e+00]),\n",
       " 1.9123150590099414)"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb =  [-0.21549219,  0.09164345,  0.1297471,   0.14354843]\n",
    "yy = [-0.5062725,   0.06012138,  1.0717416,   1.781855]\n",
    "emb, yy, euclidean_distance(emb,yy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "35e7f1f5-a14d-4168-a7c4-922e4626a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'instrument'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = generator.model.predict([[2, 4, 5, 12, 3, 6, 23, 0, 0, 0]])\n",
    "_, token = generator.euclidean_distance(result[0][5])\n",
    "vocab[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c4a5f37-eb02-402f-a397-98915cf43202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "what does a piano have ? pedals .         \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = callback('what does a piano have ?', model, 1)\n",
    "output = generator.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "38a17fcb-d0d3-4944-9852-5cfa23c1eafb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  3.521147840084448\n",
      "1 [UNK] 3.5637180220057103\n",
      "2 what 3.5528076582267705\n",
      "3 have 3.7376662373197473\n",
      "4 does 3.6053381368826254\n",
      "5 a 3.7390251217838095\n",
      "6 ? 3.6108022023983826\n",
      "7 . 3.612434715123598\n",
      "8 violin 3.8921102018463283\n",
      "9 trombone 4.664912285185767\n",
      "10 strings 2.2163665233886616\n",
      "11 pedals 0.42331789089814015\n",
      "12 organ 3.219632229866614\n",
      "13 neck 2.1875814105042464\n",
      "14 keys 0.43387972183581297\n",
      "15 guitar 3.8579436373157825\n",
      "16 wheels 3.228371535210004\n",
      "17 vehicle 4.299541529900754\n",
      "18 trunk 3.3042122982151483\n",
      "19 truck 4.110114841948657\n",
      "20 slide 3.5237040754391944\n",
      "21 piano 3.218671885714027\n",
      "22 part 3.4845356924481363\n",
      "23 instrument 4.691626179236729\n",
      "24 car 4.1133903200411135\n",
      "25 bell 3.513751712540788\n",
      "26 baggage 3.3473859556285834\n",
      "11 pedals 0.42331789089814015\n"
     ]
    }
   ],
   "source": [
    "graph_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('graph_embedding_output').output)\n",
    "text = 'what does a piano have ?'\n",
    "generator = callback(text, graph_model, 1)\n",
    "start_tokens, tokens_generated, num_tokens_generated, raw_output, distances = generator.generate_token([_ for _ in generator.start_tokens], [])\n",
    "index = len(generator.start_tokens) - 1\n",
    "\n",
    "# for j in range(maxlen):\n",
    "dists = []\n",
    "for i, word in enumerate(vocab):\n",
    "    dist = euclidean_distance(raw_output[0][5],final_embeddings[i])\n",
    "    dists.append(dist)\n",
    "    print(i, word, dist)\n",
    "sims = np.array(dists)\n",
    "print(np.argmin(dists), vocab[np.argmin(dists)], np.min(dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f94e2128-0c74-43ba-95aa-9f9aa037f202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  0.87812877\n",
      "1 [UNK] 0.8876415\n",
      "2 what 0.42489213\n",
      "3 a 0.5319029\n",
      "4 ? 0.06366684\n",
      "5 have -0.08206626\n",
      "6 does -0.9477331\n",
      "7 . 0.53725916\n",
      "8 is -0.009818074\n",
      "9 violin 0.91280884\n",
      "10 piano 0.6942938\n",
      "11 instrument 0.9911488\n",
      "12 guitar 0.9265273\n",
      "13 truck 0.32573274\n",
      "14 trombone 0.8505727\n",
      "15 strings 0.9098007\n",
      "16 neck 0.8629197\n",
      "17 car 0.38960567\n",
      "18 wheels 0.4401339\n",
      "19 vehicle 0.39378682\n",
      "20 trunk 0.50748616\n",
      "21 slide 0.82245666\n",
      "22 pedals 0.6554069\n",
      "23 part 0.7562684\n",
      "24 keys 0.6387251\n",
      "25 bell 0.7870046\n",
      "26 baggage 0.5516417\n",
      "11 instrument 0.9911488\n"
     ]
    }
   ],
   "source": [
    "graph_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('graph_embedding_output').output)\n",
    "text = 'what does a guitar have ?'\n",
    "generator = callback(text, graph_model, 1)\n",
    "start_tokens, tokens_generated, num_tokens_generated, raw_output, similarities = generator.generate_token([_ for _ in generator.start_tokens], [])\n",
    "index = len(generator.start_tokens) - 1\n",
    "\n",
    "sims = []\n",
    "for i, word in enumerate(vocab):\n",
    "    sim = cosine_sim(raw_output[0][5],final_embeddings[i])\n",
    "    sims.append(sim)\n",
    "    print(i, word, sim)\n",
    "sims = np.array(sims)\n",
    "print(np.argmax(sims), vocab[np.argmax(sims)], np.max(sims))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4897c75c-bb0d-4ad4-a985-0747a9cd2581",
   "metadata": {},
   "source": [
    "### Injection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "1e450afa-302a-4300-81a0-012bdc5bbf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(model, input):\n",
    "    start_tokens = [word_to_index.get(_, 1) for _ in input.split()]\n",
    "    print(start_tokens)\n",
    "    pad_len = maxlen - len(start_tokens)\n",
    "    sample_index = len(start_tokens) - 1\n",
    "    if pad_len < 0:\n",
    "        x = start_tokens[:maxlen]\n",
    "        sample_index = maxlen - 1\n",
    "    elif pad_len > 0:\n",
    "        x = start_tokens + [0] * pad_len\n",
    "    else:\n",
    "        x = start_tokens\n",
    "    x = np.array([x])\n",
    "    print(sample_index)\n",
    "\n",
    "    return model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e46b262a-dfbb-40e1-8020-d2f95d7b2cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 3, 12, 5, 4]\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['does', 'a', 'guitar', 'have', '?', 'neck', '', '', '', '']"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output = generate_embeddings(model, 'what does a guitar have ?')\n",
    "tokens = []\n",
    "for output in final_output[0]:\n",
    "    tokens.append(vocab[tf.argmax(output)])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "411c0c5c-01cd-406b-89b9-cb275dededec",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = keras.Model(inputs=[model.input], outputs=[model.get_layer('embedding_layer').output])\n",
    "transformer_model = keras.models.Sequential([\n",
    "    keras.layers.Input((10,128)),\n",
    "    TransformerBlock(embed_dim, num_heads, feed_forward_dim, name='transformer_layer'),\n",
    "    keras.layers.Dense(vocab_size, name='text_output')\n",
    "])\n",
    "original_weights = get_transformer_weights(model, 'transformer_layer', 'text_output')\n",
    "set_transformer_weights(transformer_model, 'transformer_layer', 'text_output', original_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "7d02b0ee-c91c-4f9f-8d8c-25c97c894b3e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Tensor(\"strided_slice:0\", shape=(None, 10), dtype=int64)\n",
      "y Tensor(\"strided_slice_1:0\", shape=(None, 10), dtype=int64)\n",
      "[2, 6, 3, 12, 5, 4]\n",
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['is', 'a', 'guitar', 'have', '?', 'neck', '', '', '', '']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['what does a guitar have ?']\n",
    "dataset = tf_data.Dataset.from_tensor_slices(texts)\n",
    "dataset = dataset.map(prepare_lm_inputs_labels)\n",
    "vectors = generate_embeddings(embedding_model, 'what does a guitar have ?')\n",
    "outputs = transformer_model.predict(vectors)\n",
    "tokens = []\n",
    "for output in outputs[0]:\n",
    "    tokens.append(vocab[tf.argmax(output)])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "bd6066c8-811e-47bd-80e6-a1adb0cd0743",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomNodeEmbedding(layers.Layer):\n",
    "    def __init__(self, node_embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.node_embed_dim = node_embed_dim\n",
    "        self.dense = layers.Dense(node_embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        node_emb = self.dense(x)  # Apply dense layer to each token embedding\n",
    "        return node_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4895cfc9-be92-48b2-ba1c-2269585ab8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_node_embedding_model = keras.models.Sequential([\n",
    "    keras.layers.Input((graph_embedding_size)),\n",
    "    CustomNodeEmbedding(embed_dim)\n",
    "]\n",
    ")\n",
    "\n",
    "custom_node_embedding_model.layers[0].set_weights(model.get_layer('node_embedding_layer').get_weights())\n",
    "\n",
    "\n",
    "node_embedding_model = keras.Model(inputs=[model.input], outputs=[model.get_layer('node_embedding_layer').output])\n",
    "# set_node_embedding_weights(node_embedding_model, get_node_embedding_weights(model))\n",
    "node_transformer_model = keras.models.Sequential([\n",
    "    keras.layers.Input((10,128)),\n",
    "    TransformerBlock(embed_dim, num_heads, feed_forward_dim, name='node_transformer_layer'),\n",
    "    keras.layers.Dense(vocab_size, name='graph_output')\n",
    "])\n",
    "original_weights = get_transformer_weights(model, 'node_transformer_layer', 'graph_output')\n",
    "set_transformer_weights(node_transformer_model, 'node_transformer_layer', 'graph_output', original_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "11e6dc57-14dd-416c-a6de-ac2a250dac43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9145890235900879"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = [final_embeddings[vocab.index(word)] for word in 'what does a guitar have ?'.split(' ')]\n",
    "for i in range(maxlen - len(embeddings)):\n",
    "    embeddings.append(final_embeddings[0])\n",
    "\n",
    "embeddings = np.array(embeddings)\n",
    "# embeddings[3][0] += 0.1\n",
    "embeddings[3] = final_embeddings[vocab.index('piano']\n",
    "embeddings[3][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "9490897a-2aa3-4745-9698-7668bd775a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x Tensor(\"strided_slice:0\", shape=(None, 10), dtype=int64)\n",
      "y Tensor(\"strided_slice_1:0\", shape=(None, 10), dtype=int64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['is', 'a', 'truck', '.', '', '', '', '', '', '']"
      ]
     },
     "execution_count": 441,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = ['what does a piano have ? neck.']\n",
    "dataset = tf_data.Dataset.from_tensor_slices(texts)\n",
    "dataset = dataset.map(prepare_lm_inputs_labels)\n",
    "# vectors = generate_embeddings(node_embedding_model, 'what does a guitar have ?')\n",
    "vectors = custom_node_embedding_model.predict(embeddings)\n",
    "vectors = np.expand_dims(vectors, axis=0)\n",
    "outputs = node_transformer_model.predict(vectors)\n",
    "tokens = []\n",
    "for output in outputs[0]:\n",
    "    tokens.append(vocab[tf.argmax(output)])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "59f4c460-8fc4-41df-a3dc-8334a61d1626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8.288712\n",
      "[UNK] 0.97452706\n",
      "what -1.2253331\n",
      "a -0.7780448\n",
      "? 2.1183078\n",
      "have -0.8638337\n",
      "does -1.9659276\n",
      ". -0.2755831\n",
      "is -1.4778473\n",
      "violin -1.8833617\n",
      "piano -2.9525533\n",
      "instrument 6.613864\n",
      "guitar -1.6347598\n",
      "truck 0.025299525\n",
      "strings 5.4872427\n",
      "neck 4.991041\n",
      "car -1.252157\n",
      "wheels -2.1652482\n",
      "vehicle -0.6221498\n",
      "trunk -1.6674696\n",
      "pedals -1.3050263\n",
      "part -1.7248017\n",
      "keys -0.9800917\n",
      "baggage -1.3884528\n"
     ]
    }
   ],
   "source": [
    "for token, val in zip(vocab, outputs[0][5]):\n",
    "    print(token,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "e844789a-13df-4243-875c-9a6e6901da2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9307133"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][4][vocab.index('what')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "b8639bf1-215d-44c9-97ce-92eff7c04554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.07046904,  0.06283123, -0.02485108,  0.09587689,  0.142355  ,\n",
       "        -0.06232064, -0.00856199]),\n",
       " array([-0.01341391,  0.1197396 ,  0.12872545,  0.09337865, -0.01016603,\n",
       "         0.11014863, -0.12191909]),\n",
       " array([-0.08136626,  0.05501175,  0.03980896,  0.09844395,  0.08715851,\n",
       "         0.13626422,  0.13247739]),\n",
       " array([ 0.84344125, -0.02662537, -1.49284065,  0.77122533, -1.93575537,\n",
       "        -1.42882872,  2.14216161]))"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_embeddings[vocab.index('what')],final_embeddings[vocab.index('does')], final_embeddings[vocab.index('a')], final_embeddings[vocab.index('piano')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249b4b1c-c345-43c2-8ca6-e90b9b951ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc20091-ad10-4e12-9e93-bf6fb7b58f85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-37",
   "language": "python",
   "name": "tf-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
