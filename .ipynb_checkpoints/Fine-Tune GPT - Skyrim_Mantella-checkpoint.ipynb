{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c01ff28-9b83-4517-9a68-8c5c7d89330e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuney\\.conda\\envs\\tf-gpu-37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow verions: 2.4.0\n",
      "Number of CPUs: 8\n",
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import psutil\n",
    "import kaggle\n",
    "import tensorflow as tf\n",
    "from itertools import chain\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers.schedules import PolynomialDecay\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM, DataCollatorWithPadding\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 3\n",
    "BLOCK_SIZE = 512\n",
    "CPU_COUNT = psutil.cpu_count()\n",
    "MODEL_CHECKPOINT = 'distilgpt2'\n",
    "KAGGLE_DS_DIR = 'kaggle_dataset'\n",
    "\n",
    "print(\"Tensorflow verions:\", tf.__version__)\n",
    "print('Number of CPUs:', CPU_COUNT)\n",
    "print('Available GPUs:', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96365b71-a416-4daa-b10d-07d419c3cf41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuneyttyler\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli whoami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a81f1d63-27b4-4230-902f-dc65d1956c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████████████████████████████████████| 29.0/29.0 [00:00<00:00, 1.64kB/s]\n",
      "C:\\Users\\cuney\\.conda\\envs\\tf-gpu-37\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\cuney\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'TheBloke/Llama-2-7B-Chat-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/Llama-2-7B-Chat-GGUF' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_8540\\1284293679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"meta-llama/Llama-2-7b-chat-hf\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mMODEL_NAME\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"TheBloke/Llama-2-7B-Chat-GGUF\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu-37\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    707\u001b[0m             \u001b[0mtokenizer_class_py\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTOKENIZER_MAPPING\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0muse_fast\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 709\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer_class_fast\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    710\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    711\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtokenizer_class_py\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\tf-gpu-37\\lib\\site-packages\\transformers\\tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1808\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfull_file_name\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfull_file_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresolved_vocab_files\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1809\u001b[0m             raise EnvironmentError(\n\u001b[1;32m-> 1810\u001b[1;33m                 \u001b[1;34mf\"Can't load tokenizer for '{pretrained_model_name_or_path}'. If you were trying to load it from \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1811\u001b[0m                 \u001b[1;34m\"'https://huggingface.co/models', make sure you don't have a local directory with the same name. \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1812\u001b[0m                 \u001b[1;34mf\"Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'TheBloke/Llama-2-7B-Chat-GGUF'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'TheBloke/Llama-2-7B-Chat-GGUF' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c5d871-db0a-454a-92bd-29f2cd36ede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset if it doesn't exists\n",
    "if not os.path.exists(KAGGLE_DS_DIR):\n",
    "    kaggle.api.dataset_download_files(\n",
    "        'simiotic/github-code-snippets-development-sample', path=KAGGLE_DS_DIR, unzip=True)\n",
    "\n",
    "# load raw dataset from sqlite3\n",
    "raw_dataset = load_dataset('./sql_loading_script.py')\n",
    "if \"validation\" not in raw_dataset.keys():\n",
    "    raw_dataset[\"validation\"] = load_dataset(\n",
    "        './sql_loading_script.py',\n",
    "        split=f\"train[:5%]\",\n",
    "    )\n",
    "    raw_dataset[\"train\"] = load_dataset(\n",
    "        './sql_loading_script.py',\n",
    "        split=f\"train[5%:]\",\n",
    "    )\n",
    "\n",
    "# initiate tokenizer and model on cuda\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "max_seq_length = tokenizer.model_max_length\n",
    "\n",
    "model = TFAutoModelForCausalLM.from_pretrained(MODEL_CHECKPOINT)\n",
    "data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"tf\")\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "def tokenize_funcion(examples):\n",
    "    return tokenizer(examples['text'], truncation=True)\n",
    "\n",
    "\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {\n",
    "        k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= max_seq_length:\n",
    "        total_length = (total_length // max_seq_length) * max_seq_length\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i: i + max_seq_length]\n",
    "            for i in range(0, total_length, max_seq_length)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "# tokenize the raw dataset\n",
    "tokenized_ds = raw_dataset.map(tokenize_funcion, batched=True, num_proc=CPU_COUNT, remove_columns=[\"text\"])\n",
    "tokenized_ds = tokenized_ds.map(group_texts, batched=True, num_proc=CPU_COUNT)\n",
    "\n",
    "# convert training dataset to tf dataset\n",
    "tf_ds = tokenized_ds['train'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "\tlabel_cols=[\"labels\"],\n",
    "    # columns=[col for col in tokenized_ds['train'].features if col != \"special_tokens_mask\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "\tdrop_remainder=True,\n",
    ")\n",
    "\n",
    "eval_ds = tokenized_ds['validation'].to_tf_dataset(\n",
    "    columns=['input_ids', 'attention_mask', 'token_type_ids'],\n",
    "\tlabel_cols=[\"labels\"],\n",
    "    # columns=[col for col in tokenized_ds['validation'].features if col != \"special_tokens_mask\"],\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    "\tdrop_remainder=True,\n",
    ")\n",
    "\n",
    "num_train_steps = len(tf_ds) * NUM_EPOCHS\n",
    "lr_scheduler = PolynomialDecay(\n",
    "    initial_learning_rate=5e-5,\n",
    "    end_learning_rate=0.0,\n",
    "    decay_steps=num_train_steps,\n",
    ")\n",
    "opt = Adam(learning_rate=lr_scheduler)\n",
    "\n",
    "model.compile(\n",
    "\toptimizer=opt,\n",
    "\tloss=SparseCategoricalCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE),\n",
    "\tmetrics=['accuracy'],\n",
    ")\n",
    "\n",
    "model.fit(tf_ds, validation_data=eval_ds, epochs=NUM_EPOCHS, steps_per_epoch=len(tf_ds) // BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu-37",
   "language": "python",
   "name": "tf-gpu-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
