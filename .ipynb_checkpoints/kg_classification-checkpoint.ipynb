{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d24ba4-1c38-4c57-a7d5-1cfc6f288df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-a8ceede95d90>:32: experimental_run_functions_eagerly (from tensorflow.python.eager.def_function) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.run_functions_eagerly` instead of the experimental version.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import tempfile\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Embedding, Activation, Softmax\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv1D,Conv2D, ZeroPadding2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers import RepeatVector, Permute, Add, Concatenate, Reshape, Dot\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph, IndexedArray\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "tf.config.experimental_run_functions_eagerly(True)\n",
    "np.random.seed(100)\n",
    "\n",
    "BASE_DIR = '.'\n",
    "GLOVE_DIR = '/media/drived/Dev/Glove'\n",
    "# TEXT_DATA_DIR = os.path.join(BASE_DIR, '20_newsgroup')\n",
    "DATA_DIR = \"/home/bhargav/nlu_project/keras_n20/data/Es_Rc\"\n",
    "DOC_PKL = \"document_list.pick\"\n",
    "TARGET_PKL = \"target_list.pick\"\n",
    "\n",
    "OUTPUT_PATH = './learning_beyond_datasets/node2vec/lstm_and_kg/'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 300\n",
    "MAX_NUM_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "NUM_RELATIONS_PER_CLUSTER = 67\n",
    "NUM_ENTITIES_PER_CLUSTER = 400\n",
    "NUM_CLUSTERS = 20\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "def get_clusters(cluster_file, num_things_per_cluster):\n",
    "    clusters = []  # np.ones(shape=(NUM_RELATIONS_PER_CLUSTER,KG_EMBEDDING_DIM))\n",
    "\n",
    "    with open(cluster_file, 'r', encoding='utf8') as f:\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            elements = line.split()\n",
    "            x = [[e] for e in elements]\n",
    "            lines.append(x)\n",
    "\n",
    "    for i in range(0, len(lines) - num_things_per_cluster + 1, num_things_per_cluster):\n",
    "        # print(\"appending: {} to {}\".format(i,i+num_things_per_cluster))\n",
    "        clusters.append(lines[i:i + num_things_per_cluster])\n",
    "\n",
    "    clusters = np.asarray(clusters, dtype='float32')\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def pickler(path, pkl_name, obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def unpickler(path, pkl_name):\n",
    "    with open(os.path.join(path, pkl_name), 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        labels.append(d['label']) if d['label'] not in labels else None\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_x_and_y(data):\n",
    "    x, y = [], []\n",
    "    for d in data:\n",
    "        for dd in d['data']:\n",
    "            tmp = dd['text'].replace('\\n', '').replace('_', '')  # clean\n",
    "            x.append({'label': d['label'], 'dbpedia_uri': dd['dbpedia_uri'], 'text': tmp, 'graph': dd['graph']}) if len(tmp) > 0 else None\n",
    "            y.append(d['label']) if len(tmp) > 0 else None\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_label_index(label):\n",
    "    return [index for index, _label in enumerate(unique_labels) if label == _label][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdde0e98-5af0-4d57-9189-837ea9dfdad6",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd9cce4d-cbb3-4edb-80a0-7e1339f44667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6716 texts.\n"
     ]
    }
   ],
   "source": [
    "texts = []  # list of text samples\n",
    "labels_index = {}  # dictionary mapping label name to numeric id\n",
    "labels = []  # list of label ids\n",
    "\n",
    "with open(r'data/classification_data_with_graphs_v4.pkl', 'rb') as pickle_file:\n",
    "    data = pickle.load(pickle_file)\n",
    "\n",
    "# raw_docs = unpickler(DATA_DIR, DOC_PKL)\n",
    "# labels = unpickler(DATA_DIR, TARGET_PKL)\n",
    "\n",
    "unique_labels = get_labels(data)\n",
    "\n",
    "x, y = get_x_and_y(data)\n",
    "raw_docs = [d['text'] for d in x]\n",
    "labels = [get_label_index(label) for label in y]\n",
    "\n",
    "for doc in raw_docs:\n",
    "    texts.append(\" \".join(doc.split()[:MAX_SEQUENCE_LENGTH]))\n",
    "\n",
    "print('Found %s texts.' % len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc89d37-d751-414a-aa94-a420cf4f5633",
   "metadata": {},
   "source": [
    "### GET NODE EMBEDDINGS/GRAPH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "def2b61e-25d4-4cbc-882b-11586788df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'data/node_embeddings_v3.pkl', 'rb') as pickle_file:\n",
    "    node_embeddings = pickle.load(pickle_file)\n",
    "with open(r'data/graph.pkl', 'rb') as pickle_file:\n",
    "    graph = pickle.load(pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f14094c-b731-4ba4-8a2d-6fb50418849c",
   "metadata": {},
   "source": [
    "### GET GRAPH CLUSTERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce537a1-d479-4b8b-bab7-5218cc58a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_clusters():\n",
    "    clusters = []\n",
    "    for i,d in enumerate(data):\n",
    "        print('%.2f%%' % ((i * 100) / len(data))) if i % 10 == 0 else None\n",
    "        nodes, edges, edge_types = [], [], []\n",
    "        for dd in d['data']:\n",
    "            graph = dd['graph']\n",
    "            for node in graph['nodes']:\n",
    "                nodes.append(node)\n",
    "            \n",
    "            for edge, edge_type in zip(graph['edges'], graph['edge_types']):\n",
    "                if not any(edge['source'] == e['source'] and edge['target'] == e['target'] for e in edges):\n",
    "                    edges.append({'source': edge['source'], 'target': edge['target']}) \n",
    "                    edge_types.append(edge_type)\n",
    "        \n",
    "                                       \n",
    "        clusters.append({'nodes': nodes, 'edges': edges, 'edge_types': edge_types})\n",
    "        \n",
    "    with open('data/node_clusters_v3.pkl', 'wb') as outp:\n",
    "        pickle.dump(clusters, outp, pickle.HIGHEST_PROTOCOL)\n",
    "                                       \n",
    "    return clusters\n",
    "\n",
    "graph_embedding_size=100\n",
    "\n",
    "def get_embedding(G):\n",
    "        walk_length = 10\n",
    "        rw = BiasedRandomWalk(G)\n",
    "        walks = rw.run(\n",
    "            nodes=G.nodes(),  # root nodes\n",
    "            length=walk_length,  # maximum length of a random walk\n",
    "            n=10,  # number of random walks per root node\n",
    "            p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "            q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "            weighted=False,  # for weighted random walks\n",
    "            seed=42,  # random seed fixed for reproducibility\n",
    "        )\n",
    "\n",
    "        model = Word2Vec(\n",
    "            walks,  vector_size=graph_embedding_size, window=5, min_count=0, sg=1, workers=1\n",
    "        )\n",
    "\n",
    "        return model.wv.vectors\n",
    "    \n",
    "    \n",
    "def get_embeddings(clusters):\n",
    "    all_embeddings = []\n",
    "    for i, cluster in clusters:\n",
    "        edges_ = pd.DataFrame({\n",
    "                'source': [e['source'] for e in cluster['edges']],\n",
    "                'target': [e['target'] for e in cluster['edges']],\n",
    "                'type': cluster['edge_types']\n",
    "            })\n",
    "\n",
    "        G = StellarGraph(IndexedArray(index=graph['nodes']), edges_, edge_type_column=\"type\")\n",
    "\n",
    "        node_embeddings = get_embedding(G)\n",
    "        \n",
    "        all_embeddings.append(node_embeddings)\n",
    "    \n",
    "    with open('data/node_embeddings_v4.pkl', 'wb') as outp:\n",
    "        pickle.dump(all_embeddings, outp, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    return all_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "982cf138-2bb9-4cc1-b876-c6b44336878f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clusters_v1 = unpickler('data','node_clusters.pkl')\n",
    "clusters_v2 = unpickler('data','node_clusters_v2.pkl')\n",
    "clusters_v3 = unpickler('data','node_clusters_v3.pkl')\n",
    "# clusters_v3 = get_graph_clusters()\n",
    "# cluster_embeddings = get_embeddings(clusters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05469dd2-8318-4d11-808d-6656215fcf27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(set(clusters_v3[0]['nodes']))\n",
    "# cluster = clusters_v3[[i for i,c in enumerate(unique_labels) if c == 'Art-Sculpting'][0]]\n",
    "# counts = [(el,cluster['nodes'].count(el)) for el in set(cluster['nodes'])]\n",
    "# counts = sorted(counts, key=lambda c: c[1],reverse=True)\n",
    "# counts[0:100]    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d8d7316-f2d5-45d3-b268-75d6eae37094",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_clusters(clusters):\n",
    "    result = []\n",
    "    for cluster in clusters:\n",
    "        new_cluster = []\n",
    "        counts = [(el,cluster['nodes'].count(el)) for el in set(cluster['nodes'])]\n",
    "        for count in counts:\n",
    "            new_cluster.append(count[0]) if count[1] > 13 else None\n",
    "                \n",
    "        result.append({'nodes':new_cluster})\n",
    "    \n",
    "    with open('data/node_clusters_v3_filtered.pkl', 'wb') as outp:\n",
    "        pickle.dump(result, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# clusters_v3_filtered = filter_clusters(clusters_v3)\n",
    "clusters_v3_filtered = unpickler('data','node_clusters_v3_filtered.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55fb6ad7-c8f3-43c2-b688-530c61c47ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_with_count(clusters):\n",
    "    result = []\n",
    "    for cluster in clusters:\n",
    "        new_cluster = []\n",
    "        counts = [(el,cluster['nodes'].count(el)) for el in set(cluster['nodes'])]\n",
    "        for count in counts:\n",
    "            new_cluster.append(count)\n",
    "                \n",
    "        result.append({'nodes':new_cluster})\n",
    "    \n",
    "    with open('data/node_clusters_v3_with_count.pkl', 'wb') as outp:\n",
    "        pickle.dump(result, outp, pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    return result\n",
    "\n",
    "# clusters_v3_with_count = cluster_with_count(clusters_v3)\n",
    "clusters_v3_filtered = unpickler('data','node_clusters_v3_with_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afde8453-3807-4288-853a-869aa508fcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_node_cluster_mapping(clusters):\n",
    "    mapping = {}\n",
    "    for i, node in enumerate(graph['nodes']):\n",
    "        print('%.2f%%' % ((i * 100) / len(graph['nodes']))) if i % 500 == 0 else None\n",
    "\n",
    "        node_clusters = [unique_labels[i] for i,cluster in enumerate(clusters) if node in cluster['nodes']]\n",
    "        mapping[node] = node_clusters if len(node_clusters) > 0 else 'None'\n",
    "                    \n",
    "    with open('data/node_cluster_mapping_v2.pkl', 'wb') as outp:\n",
    "        pickle.dump(mapping, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return mapping\n",
    "    \n",
    "node_cluster_mapping_v2 = unpickler('data','node_cluster_mapping_v2.pkl')\n",
    "# node_cluster_mapping_v3 = get_node_cluster_mapping(clusters_v3_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98467da4-5a7f-452d-a068-b383af4389b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cluster_count(cluster,node):\n",
    "    return [n[1] for n in cluster['nodes'] if n[0] == node][0]\n",
    "\n",
    "def get_node_cluster_mapping_with_count(clusters):\n",
    "    mapping = {}\n",
    "    for i, node in enumerate(graph['nodes']):\n",
    "        print('%.2f%%' % ((i * 100) / len(graph['nodes']))) if i % 500 == 0 else None\n",
    "\n",
    "        node_clusters = [(unique_labels[i],cluster_count(cluster,node)) for i,cluster in enumerate(clusters) if node in [c[0] for c in cluster['nodes']]]\n",
    "        node_clusters = sorted(node_clusters, key=lambda c: c[1], reverse=True)\n",
    "        mapping[node] = node_clusters if len(node_clusters) > 0 else 'None'\n",
    "                            \n",
    "    with open('data/node_cluster_mapping_v3_with_count.pkl', 'wb') as outp:\n",
    "        pickle.dump(mapping, outp, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return mapping\n",
    "\n",
    "node_cluster_mapping_v3_with_count = unpickler('data','node_cluster_mapping_v3_with_count.pkl')\n",
    "# node_cluster_mapping_v3_with_count = get_node_cluster_mapping_with_count(clusters_v3_with_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f2e4f7d-887f-4305-abd9-c834be06daae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Science-Chemistry', 6)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_cluster_mapping_v3_with_count['Molecule']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd9f3b-92cf-49e0-849c-7c33ac5864f4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### TOKENIZE TEXTS, PREPARE GLOVE EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e41e8b9b-f0d0-435a-aca3-581df8559801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open(os.path.join(GLOVE_DIR, '/media/drived/Dev/Glove/glove.6B.300d.txt'), encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efc999b5-90a5-4f57-aa02-2b4e30dc6339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 129395 unique tokens.\n",
      "Shape of data tensor: (6716, 300)\n",
      "Shape of label tensor: (6716, 78)\n",
      "x_train:(4836, 300) y_train:(4836, 78) x_val:(671, 300) y_val:(671, 78) \n",
      "Preparing embedding matrix.\n"
     ]
    }
   ],
   "source": [
    "# finally, vectorize the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data_seq = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', data_seq.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "# split the data into a training set and a validation set\n",
    "# indices = np.arange(data.shape[0])\n",
    "# np.random.shuffle(indices)\n",
    "indices = unpickler('data','indices.pkl')\n",
    "y = np.array(y)[indices.astype(int)]\n",
    "data = data_seq[indices]\n",
    "labels = labels[indices]\n",
    "\n",
    "num_validation_samples = int(VALIDATION_SPLIT * data.shape[0])\n",
    "\n",
    "x_t = data_seq[:-num_validation_samples]\n",
    "y_t = labels[:-num_validation_samples]\n",
    "y_str_t = y[:-num_validation_samples]\n",
    "x_val = data_seq[-num_validation_samples:]\n",
    "y_val = labels[-num_validation_samples:]\n",
    "\n",
    "num_test_samples = int(TEST_SPLIT * x_t.shape[0])\n",
    "\n",
    "x_train = x_t[:-num_test_samples]\n",
    "y_train = y_t[:-num_test_samples]\n",
    "x_test = x_t[-num_test_samples:]\n",
    "y_test = y_t[-num_test_samples:]\n",
    "y_str_test = y_str_t[-num_test_samples:]\n",
    "\n",
    "print('x_train:{} y_train:{} x_val:{} y_val:{} '.format(x_train.shape, y_train.shape, x_val.shape, y_val.shape))\n",
    "print('Preparing embedding matrix.')\n",
    "\n",
    "# prepare embedding matrix\n",
    "num_words = min(MAX_NUM_WORDS, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NUM_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d456275-3193-46d0-881b-e8ef38539a4a",
   "metadata": {},
   "source": [
    "### PREPARE NEURAL NET MODEL SIMPLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24239f9b-8d10-4a6b-a42b-46b45482952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH CLUSTER      NODE_CLUSTER_MAPPING           GRAPH INPUT\n",
    "0 Philosophy       Schopenhaur, Kant              Kant, Socrates\n",
    "1 Music            Led Zeppelin, Pearl Jam\n",
    "\n",
    "For each element in graph input, find the corresponding cluster, attend over it\n",
    "\n",
    "1. Finding graph input node clusters:\n",
    "    We know cluster indexes.\n",
    "    Write a lambda function to convert graph inputs to clusters then encode them(one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a352f447-42b6-4f6f-82b9-420da3cb08e0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "[10 20]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "pyfunc_7 returns 0 values, but expects to see 1 values. [Op:EagerPyFunc]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-72a6e027afb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtensor_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_cluster\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[0;34m(func, inp, Tout, name)\u001b[0m\n\u001b[1;32m    511\u001b[0m   \"\"\"\n\u001b[1;32m    512\u001b[0m   return _eager_py_func(\n\u001b[0;32m--> 513\u001b[0;31m       func=func, inp=inp, Tout=Tout, name=name, use_tape_cache=True)\n\u001b[0m\u001b[1;32m    514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36m_eager_py_func\u001b[0;34m(func, inp, Tout, name, use_tape_cache)\u001b[0m\n\u001b[1;32m    418\u001b[0m           \u001b[0meager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m           use_tape_cache=use_tape_cache)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m   return _internal_py_func(\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\u001b[0m in \u001b[0;36m_internal_py_func\u001b[0;34m(func, inp, Tout, stateful, eager, is_grad_func, name, use_tape_cache)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mis_async\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         name=name)\n\u001b[0m\u001b[1;32m    349\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\u001b[0m in \u001b[0;36meager_py_func\u001b[0;34m(input, token, Tout, is_async, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m       return eager_py_func_eager_fallback(\n\u001b[1;32m     54\u001b[0m           \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_async\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_async\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m           ctx=_ctx)\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m       \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/ops/gen_script_ops.py\u001b[0m in \u001b[0;36meager_py_func_eager_fallback\u001b[0;34m(input, token, Tout, is_async, name, ctx)\u001b[0m\n\u001b[1;32m     99\u001b[0m   Tout)\n\u001b[1;32m    100\u001b[0m   _result = _execute.execute(b\"EagerPyFunc\", len(Tout), inputs=_inputs_flat,\n\u001b[0;32m--> 101\u001b[0;31m                              attrs=_attrs, ctx=ctx, name=name)\n\u001b[0m\u001b[1;32m    102\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     _execute.record_gradient(\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: pyfunc_7 returns 0 values, but expects to see 1 values. [Op:EagerPyFunc]"
     ]
    }
   ],
   "source": [
    "tensor = tf.constant([[10,20]])\n",
    "tensor_array = tensor.numpy()\n",
    "tf.py_function(find_cluster,tensor,'string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8d377b0a-7e78-4884-a6df-3112e2dbc859",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'>' not supported between instances of 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-b5299c16ee39>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mgraph_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'string'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'graph_input'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFindCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringLookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'multi_hot'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# m.compile(loss='categorical_crossentropy', optimizer='adam', run_eagerly=True, metrics=['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    886\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/layers/preprocessing/index_lookup.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;31m# TODO(b/190445202): remove output rank restriction.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlookups\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m       raise ValueError(\n\u001b[1;32m    632\u001b[0m           \u001b[0;34m\"Received input shape {}, which would result in output rank {}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '>' not supported between instances of 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def find_cluster(arr):\n",
    "    arr = arr.astype('str')\n",
    "    \n",
    "    result = []\n",
    "    for el in arr:\n",
    "        all_mappings = node_cluster_mapping_v3_with_count[el] if el in node_cluster_mapping_v3_with_count else []\n",
    "        filtered_mappings = [e[0] for e in all_mappings[:min(len(all_mappings),3)]]\n",
    "        result.extend(filtered_mappings)\n",
    "        \n",
    "    print(result)\n",
    "    \n",
    "    tensor = tf.convert_to_tensor(result)\n",
    "    \n",
    "    return tensor\n",
    "    \n",
    "FindCluster = keras.layers.core.Lambda(lambda x: tf.numpy_function(find_cluster,[x],'string'))\n",
    "# FindCluster = keras.layers.core.Lambda(lambda x: print(x.shape))\n",
    "\n",
    "RemoveLastCol = keras.layers.core.Lambda(lambda x: K.sum(x, axis=-1))\n",
    "cluster_names = unique_labels + ['None']\n",
    "\n",
    "graph_input = Input(shape=(), dtype='string', name='graph_input')                           \n",
    "x = FindCluster(graph_input)\n",
    "x = tf.keras.layers.StringLookup(vocabulary=cluster_names, output_mode='multi_hot')(x)\n",
    "m = Model(inputs=graph_input, outputs=x)\n",
    "# m.compile(loss='categorical_crossentropy', optimizer='adam', run_eagerly=True, metrics=['acc'])\n",
    "\n",
    "res = m.predict(['Atom','Florence'])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e665141-a4ac-4f01-b072-937bc20e3750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model.\n",
      "Shape (89539, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (89539, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-319153d1e610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# x = e_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# x = Conv1D(filters=1, kernel_size=5, strides=1, input_shape=node_embeddings.shape)(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m# x = Conv1D(filters=1, kernel_size=5, strides=1)(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# x = MaxPooling1D(pool_size=5, strides=1)(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2632\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2634\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2635\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    216\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                          str(tuple(shape)))\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (89539, 100)"
     ]
    }
   ],
   "source": [
    "LSTM_HIDDEN_SIZE = 200\n",
    "LEARNING_RATE = 0.001\n",
    "KG_EMBEDDING_DIM = 300\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Preparing model.')\n",
    "\n",
    "# entity_clusters = K.variable(embedding_clusters_2d)\n",
    "\n",
    "Avg = keras.layers.core.Lambda(lambda x: K.mean(x, axis=1))  # , output_shape=(KG_EMBEDDING_DIM, ))\n",
    "DotProduct = keras.layers.core.Lambda(lambda x: K.dot(x[0], x[1]))  # , output_shape=(KG_EMBEDDING_DIM, ))\n",
    "Sum = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1))  # , output_shape=(KG_EMBEDDING_DIM, ))\n",
    "RemoveLastCol = keras.layers.core.Lambda(lambda x: K.sum(x, axis=-1))\n",
    "Transpose = keras.layers.core.Lambda(lambda x: K.transpose(x))\n",
    "FakeEClusterIn = keras.layers.core.Lambda(lambda x: node_embedding_clusters)\n",
    "FindCluster = keras.layers.core.Lambda(lambda x: tf.map_fn(lambda el: node_cluster_mapping[el] if el in node_cluster_mapping else 0, x, dtype='string'))\n",
    "\n",
    "cluster_names = unique_labels + ['None']\n",
    "                                       \n",
    "main_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "\n",
    "x = embedding_layer(main_input)\n",
    "x = CuDNNLSTM(LSTM_HIDDEN_SIZE, return_sequences=True)(x)\n",
    "Avg = keras.layers.core.Lambda(lambda x: K.mean(x, axis=1), output_shape=(LSTM_HIDDEN_SIZE,))\n",
    "x = Avg(x)\n",
    "x = Dense(LSTM_HIDDEN_SIZE)(x)\n",
    "main_lstm_out = Activation('relu')(x)\n",
    "\n",
    "# get representation for entity clusters\n",
    "e_clusters = Input(name='e_clusters')\n",
    "# x = FakeEClusterIn(x)\n",
    "# print(\"Shape\", K.int_shape(x))\n",
    "x = e_clusters\n",
    "x = Conv1D(filters=1, kernel_size=5, strides=1, input_shape=node_embeddings.shape)(x)\n",
    "x = MaxPooling1D(pool_size=5, strides=5)(x)\n",
    "x = Conv1D(filters=1, kernel_size=5, strides=1)(x)\n",
    "x = MaxPooling1D(pool_size=5, strides=1)(x)\n",
    "print(\"Before removing last col\", K.int_shape(x))\n",
    "x = RemoveLastCol(x)\n",
    "print(\"after removing last col\", K.int_shape(x))\n",
    "entity_cluster_reps = Reshape([KG_EMBEDDING_DIM], name='entity_cluster_reps')(x)\n",
    "print(\"entity_cluster_reps(after reshape)\", K.int_shape(entity_cluster_reps))\n",
    "\n",
    "graph_input = Input(shape=(,), dtype='string', name='graph_input')                           \n",
    "x = FindCluster(graph_input)\n",
    "graph = tf.keras.layers.StringLookup(vocabulary=cluster_names, output_mode='one_hot')(x)\n",
    "                               \n",
    "# attention over entities\n",
    "att_scores = DotProduct([entity_cluster_reps, graph])\n",
    "print(\"att_scores_entities\", K.int_shape(att_scores))\n",
    "# att_normalized = Activation('softmax',name='entity_attention')(att_scores)\n",
    "att_normalized = Softmax(axis=-1, name='entity_attention')(att_scores)\n",
    "print(\"att_normalized\", K.int_shape(att_normalized))\n",
    "the_entity = DotProduct([Transpose(entity_cluster_reps), att_normalized])\n",
    "print(\"the_entity\", K.int_shape(the_entity))\n",
    "\n",
    "lstm_hidden_and_entity = Concatenate(axis=0)([Transpose(main_lstm_out), the_entity])\n",
    "print(\"lstm_hidden_and_entity\", K.int_shape(lstm_hidden_and_entity))\n",
    "# input(\"continue?\")\n",
    "\n",
    "final_output = Dense(units=len(unique_labels), activation='softmax')(Transpose(lstm_hidden_and_entity))\n",
    "\n",
    "optimizer = Adam(lr=LEARNING_RATE, clipvalue=0.25)\n",
    "\n",
    "m = Model(inputs=[main_input,e_clusters,graph_input], outputs=final_output)\n",
    "\n",
    "m.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "# Save the best model during validation and bail out of training early if we're not improving\n",
    "callbacks = [ModelCheckpoint(tmpfn, monitor='val_acc', save_best_only=True, save_weights_only=True)]\n",
    "\n",
    "m.summary()\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# plot_model(m, to_file='model.png')\n",
    "\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "# Save the best model during validation and bail out of training early if we're not improving\n",
    "callbacks = [ModelCheckpoint(tmpfn,monitor='val_acc', save_best_only=True, save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620a85f9-de8d-4e87-ba06-4a269440d537",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PREDICT SIMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe056c83-ce9d-44c7-87d3-2645b9f9323e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 349/2418 [===>..........................] - ETA: 1:03:41 - loss: 4.4996 - acc: 0.0000e+ - ETA: 1:50 - loss: 7.3437 - acc: 0.0000e+00   - ETA: 1:49 - loss: 7.8418 - acc: 0.0000e+0 - ETA: 1:48 - loss: 7.4011 - acc: 0.0000e+0 - ETA: 1:47 - loss: 7.0167 - acc: 0.0000e+0 - ETA: 1:47 - loss: 6.8506 - acc: 0.0000e+0 - ETA: 1:46 - loss: 6.8360 - acc: 0.0000e+0 - ETA: 1:46 - loss: 6.9715 - acc: 0.0000e+0 - ETA: 1:46 - loss: 7.2541 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.4608 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.5667 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.5049 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.3822 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.3906 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.2569 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.1678 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.1228 - acc: 0.0000e+0 - ETA: 1:45 - loss: 6.9626 - acc: 0.0000e+0 - ETA: 1:44 - loss: 6.8294 - acc: 0.0000e+0 - ETA: 1:44 - loss: 6.7574 - acc: 0.0000e+0 - ETA: 1:44 - loss: 6.7398 - acc: 0.0122    - ETA: 1:44 - loss: 6.6293 - acc: 0.011 - ETA: 1:44 - loss: 6.5110 - acc: 0.011 - ETA: 1:44 - loss: 6.4289 - acc: 0.010 - ETA: 1:44 - loss: 6.4174 - acc: 0.010 - ETA: 1:44 - loss: 6.4139 - acc: 0.019 - ETA: 1:44 - loss: 6.3980 - acc: 0.018 - ETA: 1:44 - loss: 6.3357 - acc: 0.018 - ETA: 1:43 - loss: 6.2800 - acc: 0.017 - ETA: 1:43 - loss: 6.2110 - acc: 0.016 - ETA: 1:43 - loss: 6.1494 - acc: 0.016 - ETA: 1:43 - loss: 6.1320 - acc: 0.015 - ETA: 1:43 - loss: 6.1183 - acc: 0.015 - ETA: 1:43 - loss: 6.1100 - acc: 0.014 - ETA: 1:43 - loss: 6.0442 - acc: 0.014 - ETA: 1:43 - loss: 6.0207 - acc: 0.014 - ETA: 1:43 - loss: 5.9870 - acc: 0.013 - ETA: 1:43 - loss: 5.9378 - acc: 0.013 - ETA: 1:43 - loss: 5.8920 - acc: 0.019 - ETA: 1:42 - loss: 5.8380 - acc: 0.019 - ETA: 1:42 - loss: 5.8185 - acc: 0.018 - ETA: 1:42 - loss: 5.8091 - acc: 0.018 - ETA: 1:42 - loss: 5.7830 - acc: 0.017 - ETA: 1:42 - loss: 5.7492 - acc: 0.017 - ETA: 1:42 - loss: 5.7209 - acc: 0.016 - ETA: 1:42 - loss: 5.7242 - acc: 0.016 - ETA: 1:42 - loss: 5.7064 - acc: 0.016 - ETA: 1:42 - loss: 5.6843 - acc: 0.015 - ETA: 1:42 - loss: 5.6536 - acc: 0.015 - ETA: 1:42 - loss: 5.6195 - acc: 0.020 - ETA: 1:41 - loss: 5.5891 - acc: 0.019 - ETA: 1:41 - loss: 5.5771 - acc: 0.019 - ETA: 1:41 - loss: 5.5470 - acc: 0.019 - ETA: 1:41 - loss: 5.5330 - acc: 0.018 - ETA: 1:41 - loss: 5.5158 - acc: 0.018 - ETA: 1:41 - loss: 5.5163 - acc: 0.018 - ETA: 1:41 - loss: 5.5025 - acc: 0.017 - ETA: 1:41 - loss: 5.4765 - acc: 0.017 - ETA: 1:41 - loss: 5.4634 - acc: 0.017 - ETA: 1:41 - loss: 5.4503 - acc: 0.016 - ETA: 1:40 - loss: 5.4268 - acc: 0.016 - ETA: 1:40 - loss: 5.4203 - acc: 0.020 - ETA: 1:40 - loss: 5.4061 - acc: 0.020 - ETA: 1:40 - loss: 5.3915 - acc: 0.019 - ETA: 1:40 - loss: 5.3671 - acc: 0.019 - ETA: 1:40 - loss: 5.3603 - acc: 0.019 - ETA: 1:40 - loss: 5.3383 - acc: 0.018 - ETA: 1:40 - loss: 5.3274 - acc: 0.018 - ETA: 1:40 - loss: 5.3174 - acc: 0.018 - ETA: 1:40 - loss: 5.3057 - acc: 0.018 - ETA: 1:40 - loss: 5.2975 - acc: 0.017 - ETA: 1:39 - loss: 5.2776 - acc: 0.017 - ETA: 1:39 - loss: 5.2629 - acc: 0.017 - ETA: 1:39 - loss: 5.2516 - acc: 0.017 - ETA: 1:39 - loss: 5.2357 - acc: 0.016 - ETA: 1:39 - loss: 5.2206 - acc: 0.016 - ETA: 1:39 - loss: 5.2055 - acc: 0.016 - ETA: 1:39 - loss: 5.1965 - acc: 0.016 - ETA: 1:39 - loss: 5.1905 - acc: 0.015 - ETA: 1:39 - loss: 5.1705 - acc: 0.018 - ETA: 1:39 - loss: 5.1592 - acc: 0.018 - ETA: 1:39 - loss: 5.1421 - acc: 0.018 - ETA: 1:39 - loss: 5.1368 - acc: 0.018 - ETA: 1:39 - loss: 5.1304 - acc: 0.018 - ETA: 1:38 - loss: 5.1182 - acc: 0.017 - ETA: 1:38 - loss: 5.1089 - acc: 0.017 - ETA: 1:38 - loss: 5.1011 - acc: 0.017 - ETA: 1:38 - loss: 5.0882 - acc: 0.017 - ETA: 1:38 - loss: 5.0867 - acc: 0.016 - ETA: 1:38 - loss: 5.0809 - acc: 0.016 - ETA: 1:38 - loss: 5.0677 - acc: 0.019 - ETA: 1:38 - loss: 5.0560 - acc: 0.019 - ETA: 1:38 - loss: 5.0465 - acc: 0.018 - ETA: 1:38 - loss: 5.0373 - acc: 0.018 - ETA: 1:38 - loss: 5.0256 - acc: 0.021 - ETA: 1:37 - loss: 5.0199 - acc: 0.020 - ETA: 1:37 - loss: 5.0098 - acc: 0.020 - ETA: 1:37 - loss: 5.0023 - acc: 0.020 - ETA: 1:37 - loss: 4.9944 - acc: 0.020 - ETA: 1:37 - loss: 4.9868 - acc: 0.022 - ETA: 1:37 - loss: 4.9806 - acc: 0.022 - ETA: 1:37 - loss: 4.9782 - acc: 0.022 - ETA: 1:37 - loss: 4.9686 - acc: 0.024 - ETA: 1:37 - loss: 4.9649 - acc: 0.024 - ETA: 1:37 - loss: 4.9566 - acc: 0.023 - ETA: 1:37 - loss: 4.9484 - acc: 0.023 - ETA: 1:36 - loss: 4.9344 - acc: 0.028 - ETA: 1:36 - loss: 4.9230 - acc: 0.027 - ETA: 1:36 - loss: 4.9198 - acc: 0.027 - ETA: 1:36 - loss: 4.9153 - acc: 0.027 - ETA: 1:36 - loss: 4.9160 - acc: 0.027 - ETA: 1:36 - loss: 4.9101 - acc: 0.026 - ETA: 1:36 - loss: 4.9061 - acc: 0.026 - ETA: 1:36 - loss: 4.9007 - acc: 0.026 - ETA: 1:36 - loss: 4.8970 - acc: 0.028 - ETA: 1:36 - loss: 4.8940 - acc: 0.028 - ETA: 1:36 - loss: 4.8848 - acc: 0.027 - ETA: 1:35 - loss: 4.8813 - acc: 0.029 - ETA: 1:35 - loss: 4.8759 - acc: 0.029 - ETA: 1:35 - loss: 4.8707 - acc: 0.029 - ETA: 1:35 - loss: 4.8584 - acc: 0.031 - ETA: 1:35 - loss: 4.8554 - acc: 0.030 - ETA: 1:35 - loss: 4.8525 - acc: 0.030 - ETA: 1:35 - loss: 4.8470 - acc: 0.030 - ETA: 1:35 - loss: 4.8423 - acc: 0.030 - ETA: 1:35 - loss: 4.8364 - acc: 0.031 - ETA: 1:35 - loss: 4.8363 - acc: 0.031 - ETA: 1:35 - loss: 4.8303 - acc: 0.031 - ETA: 1:34 - loss: 4.8227 - acc: 0.031 - ETA: 1:34 - loss: 4.8168 - acc: 0.030 - ETA: 1:34 - loss: 4.8079 - acc: 0.032 - ETA: 1:34 - loss: 4.8026 - acc: 0.032 - ETA: 1:34 - loss: 4.7939 - acc: 0.034 - ETA: 1:34 - loss: 4.7879 - acc: 0.033 - ETA: 1:34 - loss: 4.7842 - acc: 0.033 - ETA: 1:34 - loss: 4.7837 - acc: 0.033 - ETA: 1:34 - loss: 4.7760 - acc: 0.033 - ETA: 1:34 - loss: 4.7658 - acc: 0.034 - ETA: 1:34 - loss: 4.7627 - acc: 0.034 - ETA: 1:33 - loss: 4.7582 - acc: 0.035 - ETA: 1:33 - loss: 4.7553 - acc: 0.035 - ETA: 1:33 - loss: 4.7536 - acc: 0.035 - ETA: 1:33 - loss: 4.7492 - acc: 0.035 - ETA: 1:33 - loss: 4.7448 - acc: 0.034 - ETA: 1:33 - loss: 4.7429 - acc: 0.034 - ETA: 1:33 - loss: 4.7399 - acc: 0.034 - ETA: 1:33 - loss: 4.7246 - acc: 0.034 - ETA: 1:33 - loss: 4.7135 - acc: 0.035 - ETA: 1:33 - loss: 4.7039 - acc: 0.037 - ETA: 1:33 - loss: 4.7003 - acc: 0.036 - ETA: 1:32 - loss: 4.6968 - acc: 0.036 - ETA: 1:32 - loss: 4.6945 - acc: 0.036 - ETA: 1:32 - loss: 4.6917 - acc: 0.036 - ETA: 1:32 - loss: 4.6911 - acc: 0.035 - ETA: 1:32 - loss: 4.6815 - acc: 0.037 - ETA: 1:32 - loss: 4.6791 - acc: 0.037 - ETA: 1:32 - loss: 4.6775 - acc: 0.036 - ETA: 1:32 - loss: 4.6786 - acc: 0.036 - ETA: 1:32 - loss: 4.6739 - acc: 0.036 - ETA: 1:32 - loss: 4.6710 - acc: 0.036 - ETA: 1:32 - loss: 4.6720 - acc: 0.035 - ETA: 1:31 - loss: 4.6690 - acc: 0.035 - ETA: 1:31 - loss: 4.6679 - acc: 0.035 - ETA: 1:31 - loss: 4.6644 - acc: 0.036 - ETA: 1:31 - loss: 4.6556 - acc: 0.038 - ETA: 1:31 - loss: 4.6560 - acc: 0.037 - ETA: 1:31 - loss: 4.6525 - acc: 0.037 - ETA: 1:31 - loss: 4.6470 - acc: 0.037 - ETA: 1:31 - loss: 4.6420 - acc: 0.037 - ETA: 1:31 - loss: 4.6391 - acc: 0.036 - ETA: 1:31 - loss: 4.6369 - acc: 0.036 - ETA: 1:31 - loss: 4.6372 - acc: 0.036 - ETA: 1:31 - loss: 4.6327 - acc: 0.037 - ETA: 1:30 - loss: 4.6319 - acc: 0.037 - ETA: 1:30 - loss: 4.6314 - acc: 0.0372"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "In  \u001b[0;34m[26]\u001b[0m:\nLine \u001b[0;34m8\u001b[0m:     callbacks=callbacks)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m, in \u001b[0;32mfit\u001b[0m:\nLine \u001b[0;34m1189\u001b[0m:  callbacks.on_train_batch_end(end_step, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32mon_train_batch_end\u001b[0m:\nLine \u001b[0;34m435\u001b[0m:   \u001b[36mself\u001b[39;49;00m._call_batch_hook(ModeKeys.TRAIN, \u001b[33m'\u001b[39;49;00m\u001b[33mend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch, logs=logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_call_batch_hook\u001b[0m:\nLine \u001b[0;34m295\u001b[0m:   \u001b[36mself\u001b[39;49;00m._call_batch_end_hook(mode, batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_call_batch_end_hook\u001b[0m:\nLine \u001b[0;34m315\u001b[0m:   \u001b[36mself\u001b[39;49;00m._call_batch_hook_helper(hook_name, batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_call_batch_hook_helper\u001b[0m:\nLine \u001b[0;34m353\u001b[0m:   hook(batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32mon_train_batch_end\u001b[0m:\nLine \u001b[0;34m1028\u001b[0m:  \u001b[36mself\u001b[39;49;00m._batch_update_progbar(batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_batch_update_progbar\u001b[0m:\nLine \u001b[0;34m1100\u001b[0m:  logs = tf_utils.sync_to_numpy_or_python_type(logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/utils/tf_utils.py\u001b[0m, in \u001b[0;32msync_to_numpy_or_python_type\u001b[0m:\nLine \u001b[0;34m516\u001b[0m:   \u001b[34mreturn\u001b[39;49;00m tf.nest.map_structure(_to_single_numpy_or_python_type, tensors)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m, in \u001b[0;32mmap_structure\u001b[0m:\nLine \u001b[0;34m869\u001b[0m:   structure[\u001b[34m0\u001b[39;49;00m], [func(*x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m entries],\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m, in \u001b[0;32m<listcomp>\u001b[0m:\nLine \u001b[0;34m869\u001b[0m:   structure[\u001b[34m0\u001b[39;49;00m], [func(*x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m entries],\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/utils/tf_utils.py\u001b[0m, in \u001b[0;32m_to_single_numpy_or_python_type\u001b[0m:\nLine \u001b[0;34m512\u001b[0m:   x = t.numpy()\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m, in \u001b[0;32mnumpy\u001b[0m:\nLine \u001b[0;34m1094\u001b[0m:  maybe_arr = \u001b[36mself\u001b[39;49;00m._numpy()  \u001b[37m# pylint: disable=protected-access\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m, in \u001b[0;32m_numpy\u001b[0m:\nLine \u001b[0;34m1060\u001b[0m:  \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._numpy_internal()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "history = m.fit(x_train, y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                validation_data=(x_val, y_val), \n",
    "                callbacks=callbacks)\n",
    "m.save('node2vec_lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7dc995a4-3c5f-40f6-8066-3f721bf4d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605/605 [==============================] - ETA: 3:45 - loss: 3.1769e-04 - acc: 1.000 - ETA: 7s - loss: 2.7743 - acc: 0.6000      - ETA: 7s - loss: 1.5433 - acc: 0.777 - ETA: 7s - loss: 2.0892 - acc: 0.750 - ETA: 7s - loss: 2.4398 - acc: 0.710 - ETA: 7s - loss: 2.3416 - acc: 0.666 - ETA: 6s - loss: 2.2829 - acc: 0.655 - ETA: 6s - loss: 2.2964 - acc: 0.632 - ETA: 6s - loss: 2.2410 - acc: 0.628 - ETA: 6s - loss: 2.0426 - acc: 0.659 - ETA: 6s - loss: 1.9769 - acc: 0.653 - ETA: 6s - loss: 1.8795 - acc: 0.657 - ETA: 6s - loss: 1.8924 - acc: 0.644 - ETA: 6s - loss: 1.8377 - acc: 0.648 - ETA: 6s - loss: 1.8400 - acc: 0.644 - ETA: 6s - loss: 1.8671 - acc: 0.641 - ETA: 6s - loss: 1.8973 - acc: 0.645 - ETA: 5s - loss: 1.8080 - acc: 0.654 - ETA: 5s - loss: 1.8190 - acc: 0.646 - ETA: 5s - loss: 1.8212 - acc: 0.643 - ETA: 5s - loss: 1.8629 - acc: 0.636 - ETA: 5s - loss: 2.0627 - acc: 0.634 - ETA: 5s - loss: 2.0911 - acc: 0.633 - ETA: 5s - loss: 2.0960 - acc: 0.622 - ETA: 5s - loss: 2.0724 - acc: 0.621 - ETA: 5s - loss: 2.1147 - acc: 0.621 - ETA: 5s - loss: 2.0533 - acc: 0.627 - ETA: 5s - loss: 2.0496 - acc: 0.630 - ETA: 5s - loss: 2.0206 - acc: 0.636 - ETA: 5s - loss: 1.9644 - acc: 0.642 - ETA: 5s - loss: 1.9179 - acc: 0.647 - ETA: 5s - loss: 1.8881 - acc: 0.649 - ETA: 5s - loss: 1.9009 - acc: 0.647 - ETA: 5s - loss: 1.8930 - acc: 0.643 - ETA: 4s - loss: 1.8532 - acc: 0.647 - ETA: 4s - loss: 1.8444 - acc: 0.652 - ETA: 4s - loss: 1.8383 - acc: 0.653 - ETA: 4s - loss: 1.8144 - acc: 0.652 - ETA: 4s - loss: 1.7832 - acc: 0.658 - ETA: 4s - loss: 1.8224 - acc: 0.652 - ETA: 4s - loss: 1.8762 - acc: 0.650 - ETA: 4s - loss: 1.8546 - acc: 0.654 - ETA: 4s - loss: 1.8743 - acc: 0.648 - ETA: 4s - loss: 1.9150 - acc: 0.642 - ETA: 4s - loss: 1.8791 - acc: 0.646 - ETA: 4s - loss: 1.8492 - acc: 0.649 - ETA: 4s - loss: 1.8346 - acc: 0.650 - ETA: 4s - loss: 1.7976 - acc: 0.656 - ETA: 4s - loss: 1.7814 - acc: 0.659 - ETA: 4s - loss: 1.7900 - acc: 0.661 - ETA: 4s - loss: 1.7696 - acc: 0.664 - ETA: 3s - loss: 1.7643 - acc: 0.663 - ETA: 3s - loss: 1.7840 - acc: 0.656 - ETA: 3s - loss: 1.7750 - acc: 0.655 - ETA: 3s - loss: 1.7601 - acc: 0.656 - ETA: 3s - loss: 1.7292 - acc: 0.662 - ETA: 3s - loss: 1.7597 - acc: 0.657 - ETA: 3s - loss: 1.7455 - acc: 0.658 - ETA: 3s - loss: 1.7267 - acc: 0.660 - ETA: 3s - loss: 1.7069 - acc: 0.665 - ETA: 3s - loss: 1.7089 - acc: 0.663 - ETA: 3s - loss: 1.7085 - acc: 0.662 - ETA: 3s - loss: 1.7058 - acc: 0.661 - ETA: 3s - loss: 1.7087 - acc: 0.659 - ETA: 3s - loss: 1.7026 - acc: 0.656 - ETA: 3s - loss: 1.6924 - acc: 0.655 - ETA: 3s - loss: 1.6908 - acc: 0.655 - ETA: 3s - loss: 1.6915 - acc: 0.654 - ETA: 2s - loss: 1.6694 - acc: 0.657 - ETA: 2s - loss: 1.6945 - acc: 0.654 - ETA: 2s - loss: 1.6773 - acc: 0.656 - ETA: 2s - loss: 1.6773 - acc: 0.659 - ETA: 2s - loss: 1.6796 - acc: 0.657 - ETA: 2s - loss: 1.6964 - acc: 0.658 - ETA: 2s - loss: 1.7023 - acc: 0.658 - ETA: 2s - loss: 1.6961 - acc: 0.659 - ETA: 2s - loss: 1.6971 - acc: 0.657 - ETA: 2s - loss: 1.6799 - acc: 0.657 - ETA: 2s - loss: 1.6733 - acc: 0.659 - ETA: 2s - loss: 1.6662 - acc: 0.657 - ETA: 2s - loss: 1.6617 - acc: 0.659 - ETA: 2s - loss: 1.6872 - acc: 0.654 - ETA: 2s - loss: 1.6849 - acc: 0.655 - ETA: 2s - loss: 1.7451 - acc: 0.654 - ETA: 2s - loss: 1.7283 - acc: 0.656 - ETA: 2s - loss: 1.7203 - acc: 0.659 - ETA: 1s - loss: 1.7352 - acc: 0.656 - ETA: 1s - loss: 1.7283 - acc: 0.656 - ETA: 1s - loss: 1.7600 - acc: 0.653 - ETA: 1s - loss: 1.7443 - acc: 0.656 - ETA: 1s - loss: 1.7682 - acc: 0.654 - ETA: 1s - loss: 1.7648 - acc: 0.656 - ETA: 1s - loss: 1.7725 - acc: 0.656 - ETA: 1s - loss: 1.7646 - acc: 0.657 - ETA: 1s - loss: 1.7565 - acc: 0.656 - ETA: 1s - loss: 1.7701 - acc: 0.654 - ETA: 1s - loss: 1.7635 - acc: 0.653 - ETA: 1s - loss: 1.7549 - acc: 0.652 - ETA: 1s - loss: 1.7428 - acc: 0.654 - ETA: 1s - loss: 1.7372 - acc: 0.656 - ETA: 1s - loss: 1.7382 - acc: 0.657 - ETA: 1s - loss: 1.7306 - acc: 0.657 - ETA: 1s - loss: 1.7230 - acc: 0.658 - ETA: 1s - loss: 1.7270 - acc: 0.658 - ETA: 0s - loss: 1.7309 - acc: 0.656 - ETA: 0s - loss: 1.7201 - acc: 0.656 - ETA: 0s - loss: 1.7156 - acc: 0.656 - ETA: 0s - loss: 1.7117 - acc: 0.656 - ETA: 0s - loss: 1.7133 - acc: 0.657 - ETA: 0s - loss: 1.7123 - acc: 0.655 - ETA: 0s - loss: 1.7067 - acc: 0.655 - ETA: 0s - loss: 1.7015 - acc: 0.655 - ETA: 0s - loss: 1.6900 - acc: 0.657 - ETA: 0s - loss: 1.6793 - acc: 0.658 - ETA: 0s - loss: 1.6810 - acc: 0.658 - ETA: 0s - loss: 1.6737 - acc: 0.659 - ETA: 0s - loss: 1.6643 - acc: 0.661 - ETA: 0s - loss: 1.6643 - acc: 0.662 - ETA: 0s - loss: 1.6797 - acc: 0.659 - ETA: 0s - loss: 1.6699 - acc: 0.661 - ETA: 0s - loss: 1.6758 - acc: 0.660 - ETA: 0s - loss: 1.6778 - acc: 0.660 - 7s 11ms/step - loss: 1.6824 - acc: 0.6592\n",
      "Test loss / test accuracy = 1.6824 / 0.6592\n"
     ]
    }
   ],
   "source": [
    "# Restore the best found model during validation\n",
    "# m.load_weights(tmpfn)\n",
    "\n",
    "loss, acc = m.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b967276a-f801-4798-a29c-d6e093ec5ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(x_test, batch_size=BATCH_SIZE, verbose=0,\n",
    "                  steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ae998ec-eb5b-482c-9f10-49b932fc4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_labels(results):\n",
    "    return [unique_labels[np.where(row==max(row))[0][0]] for row in results]\n",
    "\n",
    "result_labels = get_result_labels(preds)\n",
    "\n",
    "print('Accuracy score: %.2f' % accuracy_score(result_labels, y_str_test))\n",
    "print(classification_report(y_str_test, result_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684383ab-27f8-4eb7-bf97-f0427fc48d50",
   "metadata": {},
   "source": [
    "### PREPARE NEURAL NET MODEL WITH EMBEDDINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7d31ce-61fc-444d-9bee-d55a8a6a0c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPH CLUSTER      NODE_CLUSTER_MAPPING           GRAPH INPUT\n",
    "0 Philosophy       Schopenhaur, Kant              Kant, Socrates\n",
    "1 Music            Led Zeppelin, Pearl Jam\n",
    "\n",
    "For each element in graph input, find the corresponding cluster, attend over it\n",
    "\n",
    "1. Finding graph input node clusters:\n",
    "    We know cluster indexes.\n",
    "    Write a lambda function to convert graph inputs to clusters then encode them(one-hot encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6081ad35-0773-4cec-b2e9-9c1065e0ed18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing model.\n",
      "Shape (89539, 100)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (89539, 100)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-319153d1e610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# x = e_clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# x = Conv1D(filters=1, kernel_size=5, strides=1, input_shape=node_embeddings.shape)(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m# x = Conv1D(filters=1, kernel_size=5, strides=1)(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# x = MaxPooling1D(pool_size=5, strides=1)(x)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 977\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1113\u001b[0m       \u001b[0;31m# Check input assumptions set after layer building, e.g. input shape.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1114\u001b[0m       outputs = self._keras_tensor_symbolic_call(\n\u001b[0;32m-> 1115\u001b[0;31m           inputs, input_masks, args, kwargs)\n\u001b[0m\u001b[1;32m   1116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1117\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_keras_tensor_symbolic_call\u001b[0;34m(self, inputs, input_masks, args, kwargs)\u001b[0m\n\u001b[1;32m    846\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeras_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKerasTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_infer_output_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_infer_output_signature\u001b[0;34m(self, inputs, args, kwargs, input_masks)\u001b[0m\n\u001b[1;32m    884\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m           \u001b[0;31m# TODO(kaftan): do we maybe_build here, or have we already done it?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m           \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2632\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m       input_spec.assert_input_compatibility(\n\u001b[0;32m-> 2634\u001b[0;31m           self.input_spec, inputs, self.name)\n\u001b[0m\u001b[1;32m   2635\u001b[0m       \u001b[0minput_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0minput_list\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    216\u001b[0m                          \u001b[0;34m'expected ndim='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m', found ndim='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                          \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'. Full shape received: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                          str(tuple(shape)))\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_ndim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m       \u001b[0mndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input 0 of layer max_pooling1d is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (89539, 100)"
     ]
    }
   ],
   "source": [
    "LSTM_HIDDEN_SIZE = 200\n",
    "LEARNING_RATE = 0.001\n",
    "KG_EMBEDDING_DIM = 300\n",
    "\n",
    "# load pre-trained word embeddings into an Embedding layer\n",
    "# note that we set trainable = False so as to keep the embeddings fixed\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=False)\n",
    "\n",
    "print('Preparing model.')\n",
    "\n",
    "# entity_clusters = K.variable(embedding_clusters_2d)\n",
    "\n",
    "Avg = keras.layers.core.Lambda(lambda x: K.mean(x, axis=1))  # , output_shape=(KG_EMBEDDING_DIM, ))\n",
    "DotProduct = keras.layers.core.Lambda(lambda x: K.dot(x[0], x[1]))  # , output_shape=(KG_EMBEDDING_DIM, ))\n",
    "Sum = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1))  # , output_shape=(KG_EMBEDDING_DIM, ))\n",
    "RemoveLastCol = keras.layers.core.Lambda(lambda x: K.sum(x, axis=-1))\n",
    "Transpose = keras.layers.core.Lambda(lambda x: K.transpose(x))\n",
    "FakeEClusterIn = keras.layers.core.Lambda(lambda x: node_embedding_clusters)\n",
    "FindCluster = keras.layers.core.Lambda(lambda x: tf.map_fn(lambda el: node_cluster_mapping[el] if el in node_cluster_mapping else 0, x, dtype='string')\n",
    "\n",
    "cluster_names = unique_labels + ['None']\n",
    "                                       \n",
    "main_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32', name='main_input')\n",
    "\n",
    "x = embedding_layer(main_input)\n",
    "x = CuDNNLSTM(LSTM_HIDDEN_SIZE, return_sequences=True)(x)\n",
    "Avg = keras.layers.core.Lambda(lambda x: K.mean(x, axis=1), output_shape=(LSTM_HIDDEN_SIZE,))\n",
    "x = Avg(x)\n",
    "x = Dense(LSTM_HIDDEN_SIZE)(x)\n",
    "main_lstm_out = Activation('relu')(x)\n",
    "\n",
    "# get representation for entity clusters\n",
    "e_clusters = Input(name='e_clusters')\n",
    "# x = FakeEClusterIn(x)\n",
    "# print(\"Shape\", K.int_shape(x))\n",
    "x = e_clusters\n",
    "x = Conv1D(filters=1, kernel_size=5, strides=1, input_shape=node_embeddings.shape)(x)\n",
    "x = MaxPooling1D(pool_size=5, strides=5)(x)\n",
    "x = Conv1D(filters=1, kernel_size=5, strides=1)(x)\n",
    "x = MaxPooling1D(pool_size=5, strides=1)(x)\n",
    "print(\"Before removing last col\", K.int_shape(x))\n",
    "x = RemoveLastCol(x)\n",
    "print(\"after removing last col\", K.int_shape(x))\n",
    "entity_cluster_reps = Reshape([KG_EMBEDDING_DIM], name='entity_cluster_reps')(x)\n",
    "print(\"entity_cluster_reps(after reshape)\", K.int_shape(entity_cluster_reps))\n",
    "\n",
    "graph_input = Input(shape=(,), dtype='string', name='graph_input')                           \n",
    "x = FindCluster(x)\n",
    "graph = tf.keras.layers.StringLookup(vocabulary=cluster_names, output_mode='one_hot')(x)\n",
    "                               \n",
    "# attention over entities\n",
    "att_scores = DotProduct([entity_cluster_reps, graph])\n",
    "print(\"att_scores_entities\", K.int_shape(att_scores))\n",
    "# att_normalized = Activation('softmax',name='entity_attention')(att_scores)\n",
    "att_normalized = Softmax(axis=-1, name='entity_attention')(att_scores)\n",
    "print(\"att_normalized\", K.int_shape(att_normalized))\n",
    "the_entity = DotProduct([Transpose(entity_cluster_reps), att_normalized])\n",
    "print(\"the_entity\", K.int_shape(the_entity))\n",
    "\n",
    "lstm_hidden_and_entity = Concatenate(axis=0)([Transpose(main_lstm_out), the_entity])\n",
    "print(\"lstm_hidden_and_entity\", K.int_shape(lstm_hidden_and_entity))\n",
    "# input(\"continue?\")\n",
    "\n",
    "final_output = Dense(units=len(unique_labels), activation='softmax')(Transpose(lstm_hidden_and_entity))\n",
    "\n",
    "optimizer = Adam(lr=LEARNING_RATE, clipvalue=0.25)\n",
    "\n",
    "m = Model(inputs=[main_input,e_clusters,graph_input], outputs=final_output)\n",
    "\n",
    "m.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\n",
    "\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "# Save the best model during validation and bail out of training early if we're not improving\n",
    "callbacks = [ModelCheckpoint(tmpfn, monitor='val_acc', save_best_only=True, save_weights_only=True)]\n",
    "\n",
    "m.summary()\n",
    "\n",
    "from tensorflow.keras.utils import plot_model\n",
    "# plot_model(m, to_file='model.png')\n",
    "\n",
    "_, tmpfn = tempfile.mkstemp()\n",
    "# Save the best model during validation and bail out of training early if we're not improving\n",
    "callbacks = [ModelCheckpoint(tmpfn,monitor='val_acc', save_best_only=True, save_weights_only=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498510eb-6952-4d35-92cf-6e4efd3e7c03",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PREDICT WITH EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60b290c8-e8a5-44e7-bf1b-0e5799d9cca9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      " 349/2418 [===>..........................] - ETA: 1:03:41 - loss: 4.4996 - acc: 0.0000e+ - ETA: 1:50 - loss: 7.3437 - acc: 0.0000e+00   - ETA: 1:49 - loss: 7.8418 - acc: 0.0000e+0 - ETA: 1:48 - loss: 7.4011 - acc: 0.0000e+0 - ETA: 1:47 - loss: 7.0167 - acc: 0.0000e+0 - ETA: 1:47 - loss: 6.8506 - acc: 0.0000e+0 - ETA: 1:46 - loss: 6.8360 - acc: 0.0000e+0 - ETA: 1:46 - loss: 6.9715 - acc: 0.0000e+0 - ETA: 1:46 - loss: 7.2541 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.4608 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.5667 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.5049 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.3822 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.3906 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.2569 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.1678 - acc: 0.0000e+0 - ETA: 1:45 - loss: 7.1228 - acc: 0.0000e+0 - ETA: 1:45 - loss: 6.9626 - acc: 0.0000e+0 - ETA: 1:44 - loss: 6.8294 - acc: 0.0000e+0 - ETA: 1:44 - loss: 6.7574 - acc: 0.0000e+0 - ETA: 1:44 - loss: 6.7398 - acc: 0.0122    - ETA: 1:44 - loss: 6.6293 - acc: 0.011 - ETA: 1:44 - loss: 6.5110 - acc: 0.011 - ETA: 1:44 - loss: 6.4289 - acc: 0.010 - ETA: 1:44 - loss: 6.4174 - acc: 0.010 - ETA: 1:44 - loss: 6.4139 - acc: 0.019 - ETA: 1:44 - loss: 6.3980 - acc: 0.018 - ETA: 1:44 - loss: 6.3357 - acc: 0.018 - ETA: 1:43 - loss: 6.2800 - acc: 0.017 - ETA: 1:43 - loss: 6.2110 - acc: 0.016 - ETA: 1:43 - loss: 6.1494 - acc: 0.016 - ETA: 1:43 - loss: 6.1320 - acc: 0.015 - ETA: 1:43 - loss: 6.1183 - acc: 0.015 - ETA: 1:43 - loss: 6.1100 - acc: 0.014 - ETA: 1:43 - loss: 6.0442 - acc: 0.014 - ETA: 1:43 - loss: 6.0207 - acc: 0.014 - ETA: 1:43 - loss: 5.9870 - acc: 0.013 - ETA: 1:43 - loss: 5.9378 - acc: 0.013 - ETA: 1:43 - loss: 5.8920 - acc: 0.019 - ETA: 1:42 - loss: 5.8380 - acc: 0.019 - ETA: 1:42 - loss: 5.8185 - acc: 0.018 - ETA: 1:42 - loss: 5.8091 - acc: 0.018 - ETA: 1:42 - loss: 5.7830 - acc: 0.017 - ETA: 1:42 - loss: 5.7492 - acc: 0.017 - ETA: 1:42 - loss: 5.7209 - acc: 0.016 - ETA: 1:42 - loss: 5.7242 - acc: 0.016 - ETA: 1:42 - loss: 5.7064 - acc: 0.016 - ETA: 1:42 - loss: 5.6843 - acc: 0.015 - ETA: 1:42 - loss: 5.6536 - acc: 0.015 - ETA: 1:42 - loss: 5.6195 - acc: 0.020 - ETA: 1:41 - loss: 5.5891 - acc: 0.019 - ETA: 1:41 - loss: 5.5771 - acc: 0.019 - ETA: 1:41 - loss: 5.5470 - acc: 0.019 - ETA: 1:41 - loss: 5.5330 - acc: 0.018 - ETA: 1:41 - loss: 5.5158 - acc: 0.018 - ETA: 1:41 - loss: 5.5163 - acc: 0.018 - ETA: 1:41 - loss: 5.5025 - acc: 0.017 - ETA: 1:41 - loss: 5.4765 - acc: 0.017 - ETA: 1:41 - loss: 5.4634 - acc: 0.017 - ETA: 1:41 - loss: 5.4503 - acc: 0.016 - ETA: 1:40 - loss: 5.4268 - acc: 0.016 - ETA: 1:40 - loss: 5.4203 - acc: 0.020 - ETA: 1:40 - loss: 5.4061 - acc: 0.020 - ETA: 1:40 - loss: 5.3915 - acc: 0.019 - ETA: 1:40 - loss: 5.3671 - acc: 0.019 - ETA: 1:40 - loss: 5.3603 - acc: 0.019 - ETA: 1:40 - loss: 5.3383 - acc: 0.018 - ETA: 1:40 - loss: 5.3274 - acc: 0.018 - ETA: 1:40 - loss: 5.3174 - acc: 0.018 - ETA: 1:40 - loss: 5.3057 - acc: 0.018 - ETA: 1:40 - loss: 5.2975 - acc: 0.017 - ETA: 1:39 - loss: 5.2776 - acc: 0.017 - ETA: 1:39 - loss: 5.2629 - acc: 0.017 - ETA: 1:39 - loss: 5.2516 - acc: 0.017 - ETA: 1:39 - loss: 5.2357 - acc: 0.016 - ETA: 1:39 - loss: 5.2206 - acc: 0.016 - ETA: 1:39 - loss: 5.2055 - acc: 0.016 - ETA: 1:39 - loss: 5.1965 - acc: 0.016 - ETA: 1:39 - loss: 5.1905 - acc: 0.015 - ETA: 1:39 - loss: 5.1705 - acc: 0.018 - ETA: 1:39 - loss: 5.1592 - acc: 0.018 - ETA: 1:39 - loss: 5.1421 - acc: 0.018 - ETA: 1:39 - loss: 5.1368 - acc: 0.018 - ETA: 1:39 - loss: 5.1304 - acc: 0.018 - ETA: 1:38 - loss: 5.1182 - acc: 0.017 - ETA: 1:38 - loss: 5.1089 - acc: 0.017 - ETA: 1:38 - loss: 5.1011 - acc: 0.017 - ETA: 1:38 - loss: 5.0882 - acc: 0.017 - ETA: 1:38 - loss: 5.0867 - acc: 0.016 - ETA: 1:38 - loss: 5.0809 - acc: 0.016 - ETA: 1:38 - loss: 5.0677 - acc: 0.019 - ETA: 1:38 - loss: 5.0560 - acc: 0.019 - ETA: 1:38 - loss: 5.0465 - acc: 0.018 - ETA: 1:38 - loss: 5.0373 - acc: 0.018 - ETA: 1:38 - loss: 5.0256 - acc: 0.021 - ETA: 1:37 - loss: 5.0199 - acc: 0.020 - ETA: 1:37 - loss: 5.0098 - acc: 0.020 - ETA: 1:37 - loss: 5.0023 - acc: 0.020 - ETA: 1:37 - loss: 4.9944 - acc: 0.020 - ETA: 1:37 - loss: 4.9868 - acc: 0.022 - ETA: 1:37 - loss: 4.9806 - acc: 0.022 - ETA: 1:37 - loss: 4.9782 - acc: 0.022 - ETA: 1:37 - loss: 4.9686 - acc: 0.024 - ETA: 1:37 - loss: 4.9649 - acc: 0.024 - ETA: 1:37 - loss: 4.9566 - acc: 0.023 - ETA: 1:37 - loss: 4.9484 - acc: 0.023 - ETA: 1:36 - loss: 4.9344 - acc: 0.028 - ETA: 1:36 - loss: 4.9230 - acc: 0.027 - ETA: 1:36 - loss: 4.9198 - acc: 0.027 - ETA: 1:36 - loss: 4.9153 - acc: 0.027 - ETA: 1:36 - loss: 4.9160 - acc: 0.027 - ETA: 1:36 - loss: 4.9101 - acc: 0.026 - ETA: 1:36 - loss: 4.9061 - acc: 0.026 - ETA: 1:36 - loss: 4.9007 - acc: 0.026 - ETA: 1:36 - loss: 4.8970 - acc: 0.028 - ETA: 1:36 - loss: 4.8940 - acc: 0.028 - ETA: 1:36 - loss: 4.8848 - acc: 0.027 - ETA: 1:35 - loss: 4.8813 - acc: 0.029 - ETA: 1:35 - loss: 4.8759 - acc: 0.029 - ETA: 1:35 - loss: 4.8707 - acc: 0.029 - ETA: 1:35 - loss: 4.8584 - acc: 0.031 - ETA: 1:35 - loss: 4.8554 - acc: 0.030 - ETA: 1:35 - loss: 4.8525 - acc: 0.030 - ETA: 1:35 - loss: 4.8470 - acc: 0.030 - ETA: 1:35 - loss: 4.8423 - acc: 0.030 - ETA: 1:35 - loss: 4.8364 - acc: 0.031 - ETA: 1:35 - loss: 4.8363 - acc: 0.031 - ETA: 1:35 - loss: 4.8303 - acc: 0.031 - ETA: 1:34 - loss: 4.8227 - acc: 0.031 - ETA: 1:34 - loss: 4.8168 - acc: 0.030 - ETA: 1:34 - loss: 4.8079 - acc: 0.032 - ETA: 1:34 - loss: 4.8026 - acc: 0.032 - ETA: 1:34 - loss: 4.7939 - acc: 0.034 - ETA: 1:34 - loss: 4.7879 - acc: 0.033 - ETA: 1:34 - loss: 4.7842 - acc: 0.033 - ETA: 1:34 - loss: 4.7837 - acc: 0.033 - ETA: 1:34 - loss: 4.7760 - acc: 0.033 - ETA: 1:34 - loss: 4.7658 - acc: 0.034 - ETA: 1:34 - loss: 4.7627 - acc: 0.034 - ETA: 1:33 - loss: 4.7582 - acc: 0.035 - ETA: 1:33 - loss: 4.7553 - acc: 0.035 - ETA: 1:33 - loss: 4.7536 - acc: 0.035 - ETA: 1:33 - loss: 4.7492 - acc: 0.035 - ETA: 1:33 - loss: 4.7448 - acc: 0.034 - ETA: 1:33 - loss: 4.7429 - acc: 0.034 - ETA: 1:33 - loss: 4.7399 - acc: 0.034 - ETA: 1:33 - loss: 4.7246 - acc: 0.034 - ETA: 1:33 - loss: 4.7135 - acc: 0.035 - ETA: 1:33 - loss: 4.7039 - acc: 0.037 - ETA: 1:33 - loss: 4.7003 - acc: 0.036 - ETA: 1:32 - loss: 4.6968 - acc: 0.036 - ETA: 1:32 - loss: 4.6945 - acc: 0.036 - ETA: 1:32 - loss: 4.6917 - acc: 0.036 - ETA: 1:32 - loss: 4.6911 - acc: 0.035 - ETA: 1:32 - loss: 4.6815 - acc: 0.037 - ETA: 1:32 - loss: 4.6791 - acc: 0.037 - ETA: 1:32 - loss: 4.6775 - acc: 0.036 - ETA: 1:32 - loss: 4.6786 - acc: 0.036 - ETA: 1:32 - loss: 4.6739 - acc: 0.036 - ETA: 1:32 - loss: 4.6710 - acc: 0.036 - ETA: 1:32 - loss: 4.6720 - acc: 0.035 - ETA: 1:31 - loss: 4.6690 - acc: 0.035 - ETA: 1:31 - loss: 4.6679 - acc: 0.035 - ETA: 1:31 - loss: 4.6644 - acc: 0.036 - ETA: 1:31 - loss: 4.6556 - acc: 0.038 - ETA: 1:31 - loss: 4.6560 - acc: 0.037 - ETA: 1:31 - loss: 4.6525 - acc: 0.037 - ETA: 1:31 - loss: 4.6470 - acc: 0.037 - ETA: 1:31 - loss: 4.6420 - acc: 0.037 - ETA: 1:31 - loss: 4.6391 - acc: 0.036 - ETA: 1:31 - loss: 4.6369 - acc: 0.036 - ETA: 1:31 - loss: 4.6372 - acc: 0.036 - ETA: 1:31 - loss: 4.6327 - acc: 0.037 - ETA: 1:30 - loss: 4.6319 - acc: 0.037 - ETA: 1:30 - loss: 4.6314 - acc: 0.0372"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "In  \u001b[0;34m[26]\u001b[0m:\nLine \u001b[0;34m8\u001b[0m:     callbacks=callbacks)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m, in \u001b[0;32mfit\u001b[0m:\nLine \u001b[0;34m1189\u001b[0m:  callbacks.on_train_batch_end(end_step, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32mon_train_batch_end\u001b[0m:\nLine \u001b[0;34m435\u001b[0m:   \u001b[36mself\u001b[39;49;00m._call_batch_hook(ModeKeys.TRAIN, \u001b[33m'\u001b[39;49;00m\u001b[33mend\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, batch, logs=logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_call_batch_hook\u001b[0m:\nLine \u001b[0;34m295\u001b[0m:   \u001b[36mself\u001b[39;49;00m._call_batch_end_hook(mode, batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_call_batch_end_hook\u001b[0m:\nLine \u001b[0;34m315\u001b[0m:   \u001b[36mself\u001b[39;49;00m._call_batch_hook_helper(hook_name, batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_call_batch_hook_helper\u001b[0m:\nLine \u001b[0;34m353\u001b[0m:   hook(batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32mon_train_batch_end\u001b[0m:\nLine \u001b[0;34m1028\u001b[0m:  \u001b[36mself\u001b[39;49;00m._batch_update_progbar(batch, logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/callbacks.py\u001b[0m, in \u001b[0;32m_batch_update_progbar\u001b[0m:\nLine \u001b[0;34m1100\u001b[0m:  logs = tf_utils.sync_to_numpy_or_python_type(logs)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/utils/tf_utils.py\u001b[0m, in \u001b[0;32msync_to_numpy_or_python_type\u001b[0m:\nLine \u001b[0;34m516\u001b[0m:   \u001b[34mreturn\u001b[39;49;00m tf.nest.map_structure(_to_single_numpy_or_python_type, tensors)\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m, in \u001b[0;32mmap_structure\u001b[0m:\nLine \u001b[0;34m869\u001b[0m:   structure[\u001b[34m0\u001b[39;49;00m], [func(*x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m entries],\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/util/nest.py\u001b[0m, in \u001b[0;32m<listcomp>\u001b[0m:\nLine \u001b[0;34m869\u001b[0m:   structure[\u001b[34m0\u001b[39;49;00m], [func(*x) \u001b[34mfor\u001b[39;49;00m x \u001b[35min\u001b[39;49;00m entries],\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/keras/utils/tf_utils.py\u001b[0m, in \u001b[0;32m_to_single_numpy_or_python_type\u001b[0m:\nLine \u001b[0;34m512\u001b[0m:   x = t.numpy()\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m, in \u001b[0;32mnumpy\u001b[0m:\nLine \u001b[0;34m1094\u001b[0m:  maybe_arr = \u001b[36mself\u001b[39;49;00m._numpy()  \u001b[37m# pylint: disable=protected-access\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
      "File \u001b[0;34m/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m, in \u001b[0;32m_numpy\u001b[0m:\nLine \u001b[0;34m1060\u001b[0m:  \u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._numpy_internal()\u001b[37m\u001b[39;49;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: \n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 2\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "history = m.fit(x_train, y_train,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                epochs=NUM_EPOCHS,\n",
    "                validation_data=(x_val, y_val), \n",
    "                callbacks=callbacks)\n",
    "m.save('node2vec_lstm_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8c957190-3827-4b59-b51d-bf7279a41ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "605/605 [==============================] - ETA: 3:45 - loss: 3.1769e-04 - acc: 1.000 - ETA: 7s - loss: 2.7743 - acc: 0.6000      - ETA: 7s - loss: 1.5433 - acc: 0.777 - ETA: 7s - loss: 2.0892 - acc: 0.750 - ETA: 7s - loss: 2.4398 - acc: 0.710 - ETA: 7s - loss: 2.3416 - acc: 0.666 - ETA: 6s - loss: 2.2829 - acc: 0.655 - ETA: 6s - loss: 2.2964 - acc: 0.632 - ETA: 6s - loss: 2.2410 - acc: 0.628 - ETA: 6s - loss: 2.0426 - acc: 0.659 - ETA: 6s - loss: 1.9769 - acc: 0.653 - ETA: 6s - loss: 1.8795 - acc: 0.657 - ETA: 6s - loss: 1.8924 - acc: 0.644 - ETA: 6s - loss: 1.8377 - acc: 0.648 - ETA: 6s - loss: 1.8400 - acc: 0.644 - ETA: 6s - loss: 1.8671 - acc: 0.641 - ETA: 6s - loss: 1.8973 - acc: 0.645 - ETA: 5s - loss: 1.8080 - acc: 0.654 - ETA: 5s - loss: 1.8190 - acc: 0.646 - ETA: 5s - loss: 1.8212 - acc: 0.643 - ETA: 5s - loss: 1.8629 - acc: 0.636 - ETA: 5s - loss: 2.0627 - acc: 0.634 - ETA: 5s - loss: 2.0911 - acc: 0.633 - ETA: 5s - loss: 2.0960 - acc: 0.622 - ETA: 5s - loss: 2.0724 - acc: 0.621 - ETA: 5s - loss: 2.1147 - acc: 0.621 - ETA: 5s - loss: 2.0533 - acc: 0.627 - ETA: 5s - loss: 2.0496 - acc: 0.630 - ETA: 5s - loss: 2.0206 - acc: 0.636 - ETA: 5s - loss: 1.9644 - acc: 0.642 - ETA: 5s - loss: 1.9179 - acc: 0.647 - ETA: 5s - loss: 1.8881 - acc: 0.649 - ETA: 5s - loss: 1.9009 - acc: 0.647 - ETA: 5s - loss: 1.8930 - acc: 0.643 - ETA: 4s - loss: 1.8532 - acc: 0.647 - ETA: 4s - loss: 1.8444 - acc: 0.652 - ETA: 4s - loss: 1.8383 - acc: 0.653 - ETA: 4s - loss: 1.8144 - acc: 0.652 - ETA: 4s - loss: 1.7832 - acc: 0.658 - ETA: 4s - loss: 1.8224 - acc: 0.652 - ETA: 4s - loss: 1.8762 - acc: 0.650 - ETA: 4s - loss: 1.8546 - acc: 0.654 - ETA: 4s - loss: 1.8743 - acc: 0.648 - ETA: 4s - loss: 1.9150 - acc: 0.642 - ETA: 4s - loss: 1.8791 - acc: 0.646 - ETA: 4s - loss: 1.8492 - acc: 0.649 - ETA: 4s - loss: 1.8346 - acc: 0.650 - ETA: 4s - loss: 1.7976 - acc: 0.656 - ETA: 4s - loss: 1.7814 - acc: 0.659 - ETA: 4s - loss: 1.7900 - acc: 0.661 - ETA: 4s - loss: 1.7696 - acc: 0.664 - ETA: 3s - loss: 1.7643 - acc: 0.663 - ETA: 3s - loss: 1.7840 - acc: 0.656 - ETA: 3s - loss: 1.7750 - acc: 0.655 - ETA: 3s - loss: 1.7601 - acc: 0.656 - ETA: 3s - loss: 1.7292 - acc: 0.662 - ETA: 3s - loss: 1.7597 - acc: 0.657 - ETA: 3s - loss: 1.7455 - acc: 0.658 - ETA: 3s - loss: 1.7267 - acc: 0.660 - ETA: 3s - loss: 1.7069 - acc: 0.665 - ETA: 3s - loss: 1.7089 - acc: 0.663 - ETA: 3s - loss: 1.7085 - acc: 0.662 - ETA: 3s - loss: 1.7058 - acc: 0.661 - ETA: 3s - loss: 1.7087 - acc: 0.659 - ETA: 3s - loss: 1.7026 - acc: 0.656 - ETA: 3s - loss: 1.6924 - acc: 0.655 - ETA: 3s - loss: 1.6908 - acc: 0.655 - ETA: 3s - loss: 1.6915 - acc: 0.654 - ETA: 2s - loss: 1.6694 - acc: 0.657 - ETA: 2s - loss: 1.6945 - acc: 0.654 - ETA: 2s - loss: 1.6773 - acc: 0.656 - ETA: 2s - loss: 1.6773 - acc: 0.659 - ETA: 2s - loss: 1.6796 - acc: 0.657 - ETA: 2s - loss: 1.6964 - acc: 0.658 - ETA: 2s - loss: 1.7023 - acc: 0.658 - ETA: 2s - loss: 1.6961 - acc: 0.659 - ETA: 2s - loss: 1.6971 - acc: 0.657 - ETA: 2s - loss: 1.6799 - acc: 0.657 - ETA: 2s - loss: 1.6733 - acc: 0.659 - ETA: 2s - loss: 1.6662 - acc: 0.657 - ETA: 2s - loss: 1.6617 - acc: 0.659 - ETA: 2s - loss: 1.6872 - acc: 0.654 - ETA: 2s - loss: 1.6849 - acc: 0.655 - ETA: 2s - loss: 1.7451 - acc: 0.654 - ETA: 2s - loss: 1.7283 - acc: 0.656 - ETA: 2s - loss: 1.7203 - acc: 0.659 - ETA: 1s - loss: 1.7352 - acc: 0.656 - ETA: 1s - loss: 1.7283 - acc: 0.656 - ETA: 1s - loss: 1.7600 - acc: 0.653 - ETA: 1s - loss: 1.7443 - acc: 0.656 - ETA: 1s - loss: 1.7682 - acc: 0.654 - ETA: 1s - loss: 1.7648 - acc: 0.656 - ETA: 1s - loss: 1.7725 - acc: 0.656 - ETA: 1s - loss: 1.7646 - acc: 0.657 - ETA: 1s - loss: 1.7565 - acc: 0.656 - ETA: 1s - loss: 1.7701 - acc: 0.654 - ETA: 1s - loss: 1.7635 - acc: 0.653 - ETA: 1s - loss: 1.7549 - acc: 0.652 - ETA: 1s - loss: 1.7428 - acc: 0.654 - ETA: 1s - loss: 1.7372 - acc: 0.656 - ETA: 1s - loss: 1.7382 - acc: 0.657 - ETA: 1s - loss: 1.7306 - acc: 0.657 - ETA: 1s - loss: 1.7230 - acc: 0.658 - ETA: 1s - loss: 1.7270 - acc: 0.658 - ETA: 0s - loss: 1.7309 - acc: 0.656 - ETA: 0s - loss: 1.7201 - acc: 0.656 - ETA: 0s - loss: 1.7156 - acc: 0.656 - ETA: 0s - loss: 1.7117 - acc: 0.656 - ETA: 0s - loss: 1.7133 - acc: 0.657 - ETA: 0s - loss: 1.7123 - acc: 0.655 - ETA: 0s - loss: 1.7067 - acc: 0.655 - ETA: 0s - loss: 1.7015 - acc: 0.655 - ETA: 0s - loss: 1.6900 - acc: 0.657 - ETA: 0s - loss: 1.6793 - acc: 0.658 - ETA: 0s - loss: 1.6810 - acc: 0.658 - ETA: 0s - loss: 1.6737 - acc: 0.659 - ETA: 0s - loss: 1.6643 - acc: 0.661 - ETA: 0s - loss: 1.6643 - acc: 0.662 - ETA: 0s - loss: 1.6797 - acc: 0.659 - ETA: 0s - loss: 1.6699 - acc: 0.661 - ETA: 0s - loss: 1.6758 - acc: 0.660 - ETA: 0s - loss: 1.6778 - acc: 0.660 - 7s 11ms/step - loss: 1.6824 - acc: 0.6592\n",
      "Test loss / test accuracy = 1.6824 / 0.6592\n"
     ]
    }
   ],
   "source": [
    "# Restore the best found model during validation\n",
    "# m.load_weights(tmpfn)\n",
    "\n",
    "loss, acc = m.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\n",
    "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dc0dd40f-e908-4c93-ac5f-0f2a554d2518",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = m.predict(x_test, batch_size=BATCH_SIZE, verbose=0,\n",
    "                  steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "453c14b0-12fd-4b07-a4e1-572a44cb6ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result_labels(results):\n",
    "    return [unique_labels[np.where(row==max(row))[0][0]] for row in results]\n",
    "\n",
    "result_labels = get_result_labels(preds)\n",
    "\n",
    "print('Accuracy score: %.2f' % accuracy_score(result_labels, y_str_test))\n",
    "print(classification_report(y_str_test, result_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
