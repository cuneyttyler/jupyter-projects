{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2424ed0-4ebc-4c6d-bff7-beaffeea95d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow verions: 2.4.0\n",
      "Available GPUs: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "# from tensorflow.keras import ops\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import numpy as np\n",
    "import os\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph, IndexedArray\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stellargraph.mapper import AdjacencyPowerGenerator\n",
    "from stellargraph.layer import WatchYourStep\n",
    "from stellargraph import datasets, utils\n",
    "from tensorflow.keras import callbacks, optimizers, losses, metrics, regularizers, Model\n",
    "\n",
    "from stellargraph.mapper import KGTripleGenerator\n",
    "from stellargraph.layer import ComplEx\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from graph_visualization import GraphVisualization\n",
    "\n",
    "print(\"Tensorflow verions:\", tf.__version__)\n",
    "print('Available GPUs:', tf.config.experimental.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "507cf6c6-401b-483d-bcf1-d14055c3484f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to C:\\Users\\cuney\\tensorflow_datasets\\gpt3\\cycle_letters_in_word\\1.0.0...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dl Completed...: 0 url [00:00, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                         | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Dl Completed...:   0%|                                                                         | 0/1 [00:00<?, ? url/s]\n",
      "Dl Size...: 0 MiB [00:00, ? MiB/s]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:00, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|                                                                         | 0/1 [00:01<?, ? url/s]\n",
      "Dl Size...: 1 MiB [00:01,  1.88s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:01, ? file/s]\u001b[A\u001b[A\n",
      "Dl Completed...:   0%|                                                                         | 0/1 [00:02<?, ? url/s]\n",
      "Dl Size...: 2 MiB [00:02,  1.33s/ MiB]\u001b[A\n",
      "\n",
      "Dl Completed...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.04s/ url]\n",
      "Dl Size...: 2 MiB [00:03,  1.33s/ MiB]\u001b[A\n",
      "\n",
      "Extraction completed...: 0 file [00:03, ? file/s]\u001b[A\u001b[A\n",
      "Dl Size...: 2 MiB [00:03,  1.52s/ MiB]\n",
      "Dl Completed...: 100%|█████████████████████████████████████████████████████████████████| 1/1 [00:03<00:00,  3.05s/ url]\n"
     ]
    },
    {
     "ename": "NonMatchingChecksumError",
     "evalue": "Artifact https://github.com/openai/gpt-3/archive/master.zip, downloaded to C:\\Users\\cuney\\tensorflow_datasets\\downloads\\openai_gpt-3_archive_masterdtAH15zvsBtMveV6rnbk1RhnL6DZKawgogG3MNb82ZQ.zip.tmp.d1db91bcf8f34df99965d51ef903ba06\\gpt-3-master.zip, has wrong checksum:\n* Expected: UrlInfo(size=2.15 MiB, checksum='a200cd633fb51190c61c6ad128c31e7ef7d03a00ab03baf6feff300b0fbdedab', filename='gpt-3-master.zip')\n* Got: UrlInfo(size=2.16 MiB, checksum='e8290ff0e57be5b81a29a9ae4670a4e8dfcde8142caaf96012e7c70813cb3a30', filename='gpt-3-master.zip')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNonMatchingChecksumError\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12696\\4264446601.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow_datasets\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mgpt3_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'gpt3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\load.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[0;32m    615\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    616\u001b[0m     \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdownload_and_prepare_kwargs\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 617\u001b[1;33m     \u001b[0mdbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    618\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mas_dataset_kwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\logging\\__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m       \u001b[0mmetadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[1;34m(self, download_dir, download_config, file_format)\u001b[0m\n\u001b[0;32m    628\u001b[0m           self._download_and_prepare(\n\u001b[0;32m    629\u001b[0m               \u001b[0mdl_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 630\u001b[1;33m               \u001b[0mdownload_config\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    631\u001b[0m           )\n\u001b[0;32m    632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\dataset_builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[1;34m(self, dl_manager, download_config)\u001b[0m\n\u001b[0;32m   1435\u001b[0m         \u001b[0moptional_pipeline_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1436\u001b[0m       split_generators = self._split_generators(  # pylint: disable=unexpected-keyword-arg\n\u001b[1;32m-> 1437\u001b[1;33m           \u001b[0mdl_manager\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptional_pipeline_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1438\u001b[0m       )\n\u001b[0;32m   1439\u001b[0m       \u001b[1;31m# TODO(tfds): Could be removed once all datasets are migrated.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\text\\gpt3.py\u001b[0m in \u001b[0;36m_split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[1;34m\"\"\"Returns SplitGenerator.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m     \u001b[0mdirectory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_DATA_URL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return [\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_downloader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extractor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_map_promise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_download_extract\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m_map_promise\u001b[1;34m(map_fn, all_inputs)\u001b[0m\n\u001b[0;32m    828\u001b[0m   )  # Apply the function\n\u001b[0;32m    829\u001b[0m   res = tree_utils.map_structure(\n\u001b[1;32m--> 830\u001b[1;33m       \u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m   )  # Wait promises\n\u001b[0;32m    832\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tree\\__init__.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m   return unflatten_as(structures[0],\n\u001b[1;32m--> 435\u001b[1;33m                       [func(*args) for args in zip(*map(flatten, structures))])\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tree\\__init__.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    433\u001b[0m     \u001b[0massert_same_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m   return unflatten_as(structures[0],\n\u001b[1;32m--> 435\u001b[1;33m                       [func(*args) for args in zip(*map(flatten, structures))])\n\u001b[0m\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m    828\u001b[0m   )  # Apply the function\n\u001b[0;32m    829\u001b[0m   res = tree_utils.map_structure(\n\u001b[1;32m--> 830\u001b[1;33m       \u001b[1;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mall_promises\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m   )  # Wait promises\n\u001b[0;32m    832\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mDEFAULT_TIMEOUT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 512\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36m_target_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    514\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         \u001b[1;31m# type: (bool) -> Any\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_settled_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_raise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m     \u001b[0m_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_reason\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_target_settled_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36m_settled_value\u001b[1;34m(self, _raise)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0m_raise\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m                 \u001b[0mraise_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m                 \u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraise_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_traceback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fulfillment_handler0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\promise\\promise.py\u001b[0m in \u001b[0;36mtry_catch\u001b[1;34m(handler, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[1;31m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(dl_result)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[0mexpected_url_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_url_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[0mchecksum_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mchecksum_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[0murl_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m         )\n\u001b[0;32m    414\u001b[0m     )\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m_register_or_validate_checksums\u001b[1;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001b[0m\n\u001b[0;32m    466\u001b[0m           \u001b[0mexpected_url_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexpected_url_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m           \u001b[0mcomputed_url_info\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcomputed_url_info\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m           \u001b[0mforce_checksums_validation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_force_checksums_validation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m       )\n\u001b[0;32m    470\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\tensorflow_datasets\\core\\download\\download_manager.py\u001b[0m in \u001b[0;36m_validate_checksums\u001b[1;34m(url, path, computed_url_info, expected_url_info, force_checksums_validation)\u001b[0m\n\u001b[0;32m    805\u001b[0m         \u001b[1;34m'https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    806\u001b[0m     )\n\u001b[1;32m--> 807\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mNonMatchingChecksumError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    808\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNonMatchingChecksumError\u001b[0m: Artifact https://github.com/openai/gpt-3/archive/master.zip, downloaded to C:\\Users\\cuney\\tensorflow_datasets\\downloads\\openai_gpt-3_archive_masterdtAH15zvsBtMveV6rnbk1RhnL6DZKawgogG3MNb82ZQ.zip.tmp.d1db91bcf8f34df99965d51ef903ba06\\gpt-3-master.zip, has wrong checksum:\n* Expected: UrlInfo(size=2.15 MiB, checksum='a200cd633fb51190c61c6ad128c31e7ef7d03a00ab03baf6feff300b0fbdedab', filename='gpt-3-master.zip')\n* Got: UrlInfo(size=2.16 MiB, checksum='e8290ff0e57be5b81a29a9ae4670a4e8dfcde8142caaf96012e7c70813cb3a30', filename='gpt-3-master.zip')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "gpt3_dataset = tfds.load('gpt3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b860e0-b6d6-49cc-8275-1d6b9beeb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = tf.cast(m, dtype)\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a0ae20fa-e191-4b5f-92e7-7fd3d05d971b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 files\n",
      "x Tensor(\"strided_slice:0\", shape=(None, 200), dtype=int64)\n",
      "y Tensor(\"strided_slice_1:0\", shape=(None, 200), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5000  # Only consider the top 20k words\n",
    "maxlen = 200  # Max sequence size\n",
    "batch_size = 128\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"data/aclImdb/train/pos\",\n",
    "    \"data/aclImdb/train/neg\",\n",
    "    \"data/aclImdb/test/pos\",\n",
    "    \"data/aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "filenames = filenames[:1000]\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf_data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf_strings.lower(input_string)\n",
    "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def count_occurences(text_ds):\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # Custom text preprocessing\n",
    "        # For example, you can perform lowercasing, punctuation removal, etc.\n",
    "        return text.lower()\n",
    "    \n",
    "    # Preprocess the text data and convert it to a list\n",
    "    preprocessed_texts = [preprocess_text(str(text.numpy())) for text in text_ds]\n",
    "    # Initialize CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    # Fit CountVectorizer on preprocessed text data and transform it into token counts\n",
    "    token_counts = vectorizer.fit_transform(preprocessed_texts)\n",
    "    # Get the vocabulary and token counts\n",
    "    vocab = vectorizer.get_feature_names_out()\n",
    "    counts = token_counts.toarray().sum(axis=0)\n",
    "    # Create a dictionary to store token counts\n",
    "    token_counts_dict = dict(zip(vocab, counts))\n",
    "\n",
    "    token_counts_dict = sorted(token_counts_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    print(token_counts_dict)\n",
    "\n",
    "# count_occurences(text_ds)\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "\n",
    "    # zeros_column = tf.zeros_like(y[:, :1])  # Create a column of zeros with the same shape as the first column of y\n",
    "    # y = tf.concat([zeros_column, y], axis=1)  # Concatenate the zeros column with y along the column axis\n",
    "    \n",
    "    print('x',x)\n",
    "    print('y',y)\n",
    "    return x, y\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f2832a45-8d27-4442-aa7f-6eaa179fef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "afcaa500-39f7-493d-b082-b8f4e8debec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conceptnet_data():\n",
    "    conceptnet_data = pd.DataFrame()\n",
    "    for i,file in enumerate(os.listdir('data/conceptnet')):\n",
    "        print(f'Reading file {i}...')\n",
    "        df = pd.read_parquet(os.path.join('data/conceptnet',file), engine='fastparquet')\n",
    "        conceptnet_data = pd.concat([conceptnet_data, df[df['lang'] == 'en'][['arg1','rel','arg2']]])\n",
    "        # if i == 4: break\n",
    "    \n",
    "    conceptnet_data['type'] = conceptnet_data['arg1'].map(lambda d: d.split('/')[4] if d.count('/') == 4 else '')\n",
    "    conceptnet_data['arg1'] = conceptnet_data['arg1'].map(lambda d: d.split('/')[3])\n",
    "    conceptnet_data['rel'] = conceptnet_data['rel'].map(lambda d: d.split('/')[2])\n",
    "    conceptnet_data['arg2'] = conceptnet_data['arg2'].map(lambda d: d.split('/')[3])\n",
    "    conceptnet_data['arg1'] = conceptnet_data['arg1'].map(lambda d: str(d).replace('_',' '))\n",
    "    conceptnet_data['arg2'] = conceptnet_data['arg2'].map(lambda d: str(d).replace('_',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4c7413c1-bf1a-465a-a87b-65e263ddb067",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\miniconda3\\envs\\tf-37\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3553: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "conceptnet_data = pd.read_csv('conceptnet_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83483a6e-9c4e-431f-b767-04421b0690cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cuney\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "      <th>target</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19087</th>\n",
       "      <td>abu</td>\n",
       "      <td>AtLocation</td>\n",
       "      <td>japan</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19184</th>\n",
       "      <td>actor</td>\n",
       "      <td>AtLocation</td>\n",
       "      <td>movie</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19185</th>\n",
       "      <td>actor</td>\n",
       "      <td>AtLocation</td>\n",
       "      <td>show</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19186</th>\n",
       "      <td>actor</td>\n",
       "      <td>AtLocation</td>\n",
       "      <td>television</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19187</th>\n",
       "      <td>actor</td>\n",
       "      <td>AtLocation</td>\n",
       "      <td>theater</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422303</th>\n",
       "      <td>rose</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>singing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422401</th>\n",
       "      <td>tim</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>singing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422407</th>\n",
       "      <td>tony</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>singing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422466</th>\n",
       "      <td>wing</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>singing</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422884</th>\n",
       "      <td>seat</td>\n",
       "      <td>dbpedia</td>\n",
       "      <td>car</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>78775 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        source       label      target type\n",
       "19087      abu  AtLocation       japan  NaN\n",
       "19184    actor  AtLocation       movie  NaN\n",
       "19185    actor  AtLocation        show  NaN\n",
       "19186    actor  AtLocation  television  NaN\n",
       "19187    actor  AtLocation     theater  NaN\n",
       "...        ...         ...         ...  ...\n",
       "3422303   rose     dbpedia     singing  NaN\n",
       "3422401    tim     dbpedia     singing  NaN\n",
       "3422407   tony     dbpedia     singing  NaN\n",
       "3422466   wing     dbpedia     singing  NaN\n",
       "3422884   seat     dbpedia         car  NaN\n",
       "\n",
       "[78775 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_conceptnet_data(conceptnet_data):\n",
    "    return conceptnet_data[conceptnet_data['source'].isin(vocab) & conceptnet_data['target'].isin(vocab)][conceptnet_data['label'] != 'Antonym']#[conceptnet_data['label'] != 'Synonym']\n",
    "\n",
    "conceptnet_data = filter_conceptnet_data(conceptnet_data)\n",
    "conceptnet_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "96655133-7e51-43fd-a1a6-ac4abcc7d9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4399 nodes found.\n",
      "78775 edges found.\n"
     ]
    }
   ],
   "source": [
    "graph_embedding_size = 100\n",
    "\n",
    "def node2vec(G):\n",
    "        walk_length = 100\n",
    "        rw = BiasedRandomWalk(G)\n",
    "        walks = rw.run(\n",
    "            nodes=G.nodes(),  # root nodes\n",
    "            length=walk_length,  # maximum length of a random walk\n",
    "            n=10,  # number of random walks per root node\n",
    "            p=0.5,  # Defines (unormalised) probability, 1/p, of returning to source node\n",
    "            q=2.0,  # Defines (unormalised) probability, 1/q, for moving away from source node\n",
    "            weighted=False,  # for weighted random walks\n",
    "            seed=42,  # random seed fixed for reproducibility\n",
    "        )\n",
    "\n",
    "        model = Word2Vec(\n",
    "            walks,  vector_size=graph_embedding_size, window=5, min_count=0, sg=1, workers=1\n",
    "        )\n",
    "\n",
    "        return pd.DataFrame([(key, vector) for key,vector in zip(model.wv.index_to_key, model.wv.vectors)], columns=['node', 'embedding'])\n",
    "\n",
    "def complex_embeddings(G, data):\n",
    "    epochs = 50\n",
    "    embedding_dimension = graph_embedding_size\n",
    "    negative_samples = 10\n",
    "\n",
    "    wn18_gen = KGTripleGenerator(\n",
    "        G, batch_size=len(data) // 100  # ~100 batches per epoch\n",
    "    )\n",
    "    \n",
    "    wn18_complex = ComplEx(\n",
    "        wn18_gen,\n",
    "        embedding_dimension=embedding_dimension,\n",
    "        embeddings_regularizer=regularizers.l2(1e-7),\n",
    "    )\n",
    "    \n",
    "    wn18_inp, wn18_out = wn18_complex.in_out_tensors()\n",
    "    \n",
    "    wn18_model = Model(inputs=wn18_inp, outputs=wn18_out)\n",
    "    \n",
    "    wn18_model.compile(\n",
    "        optimizer=optimizers.Adam(lr=0.001),\n",
    "        loss=losses.BinaryCrossentropy(from_logits=True),\n",
    "        metrics=[metrics.BinaryAccuracy(threshold=0.0)],\n",
    "    )\n",
    "\n",
    "    wn18_train_gen = wn18_gen.flow(\n",
    "    data, negative_samples=negative_samples, shuffle=True\n",
    "    )\n",
    "    # wn18_valid_gen = wn18_gen.flow(wn18_valid, negative_samples=negative_samples)\n",
    "\n",
    "    wn18_es = callbacks.EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "    wn18_history = wn18_model.fit(\n",
    "        wn18_train_gen, epochs=epochs, callbacks=[wn18_es]\n",
    "    )\n",
    "\n",
    "    utils.plot_history(wn18_history)\n",
    "\n",
    "    return wn18_complex.embeddings()[0]\n",
    "\n",
    "def get_graph():\n",
    "    nodes, edges, edge_types = [], [], []\n",
    "\n",
    "    for word in conceptnet_data['source'].unique():\n",
    "        nodes.append(word)\n",
    "\n",
    "    for word in conceptnet_data['target'].unique():\n",
    "        nodes.append(word) if word not in nodes else None\n",
    "\n",
    "    for index, row in conceptnet_data.iterrows():\n",
    "        edges.append({'source': row['source'], 'target': row['target']})\n",
    "        edge_types.append(row['label'])\n",
    "\n",
    "    graph = {'nodes':nodes, 'edges': edges, 'edge_types': edge_types}\n",
    "    \n",
    "    edges_ = pd.DataFrame({\n",
    "            'source': [e['source'] for e in graph['edges']],\n",
    "            'target': [e['target'] for e in graph['edges']],\n",
    "            'type': graph['edge_types']\n",
    "        })\n",
    "    \n",
    "    G = StellarGraph(IndexedArray(index=graph['nodes']), edges_, edge_type_column=\"type\")\n",
    "    \n",
    "    return graph, G\n",
    "\n",
    "def get_embeddings(G):\n",
    "    node_embeddings = complex_embeddings(G, conceptnet_data[['source','label','target']])\n",
    "        \n",
    "    return node_embeddings\n",
    "    \n",
    "graph, G = get_graph()\n",
    "print('%s nodes found.' % len(graph['nodes']))\n",
    "print('%s edges found.' % len(graph['edges']))\n",
    "node_embeddings = node2vec(G)\n",
    "\n",
    "tmp = []\n",
    "for i,word in enumerate(vocab):\n",
    "    lower_nodes = list(map(lambda k: k.lower(), node_embeddings['node']))\n",
    "    index = lower_nodes.index(word.lower()) if word.lower() in lower_nodes else -1\n",
    "    if index != -1:\n",
    "        tmp.append(node_embeddings.iloc[index][1])\n",
    "    else:\n",
    "        tmp.append(np.ones(graph_embedding_size) * -10)\n",
    "\n",
    "node_embeddings = np.array(tmp).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "4f86fa58-5fac-42f6-8146-482407379115",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tmp = []\n",
    "# for i,word in enumerate(vocab):\n",
    "#     lower_nodes = list(map(lambda k: k.lower(), graph['nodes']))\n",
    "#     index = lower_nodes.index(word.lower()) if word.lower() in lower_nodes else -1\n",
    "#     if index != -1:\n",
    "#         tmp.append(node_embeddings[index])\n",
    "#     else:\n",
    "#         tmp.append(np.ones(graph_embedding_size) * -10)\n",
    "\n",
    "# node_embeddings = np.array(tmp).astype('float32').tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "b7a20ec3-9f7b-440f-8956-84d69cb6286e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import sklearn\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "# nodelist = list(G.nodes())\n",
    "\n",
    "# # labels = conceptnet_data['type'].values()\n",
    "# target_encoding = OneHotEncoder(sparse=False)\n",
    "# # label_vectors = target_encoding.fit_transform(labels.values.reshape(-1, 1))\n",
    "\n",
    "# transform = TSNE\n",
    "\n",
    "# embeddings = node_embeddings.astype('float32').tolist()\n",
    "\n",
    "# trans = transform(n_components=2)\n",
    "# emb_transformed = pd.DataFrame(trans.fit_transform(embeddings), index=nodelist)\n",
    "\n",
    "# # emb_transformed[\"label\"] = np.argmax(label_vectors, 1)\n",
    "\n",
    "# alpha = 0.7\n",
    "\n",
    "# fig, ax = plt.subplots(figsize=(7, 7))\n",
    "# ax.scatter(\n",
    "#     emb_transformed[0],\n",
    "#     emb_transformed[1],\n",
    "#     # c=emb_transformed[\"label\"].astype(\"category\"),\n",
    "#     cmap=\"jet\",\n",
    "#     alpha=alpha,\n",
    "# )\n",
    "\n",
    "# ax.set(aspect=\"equal\", xlabel=\"$X_1$\", ylabel=\"$X_2$\")\n",
    "# plt.title(\n",
    "#     \"{} visualization of embeddings for cora dataset\".format(\n",
    "#         transform.__name__\n",
    "#     )\n",
    "# )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c32efc5-96d2-44e4-8816-76e0efb7fae9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "index = 0\n",
    "vis = GraphVisualization()\n",
    "for edge in graph['edges'][index:index+30]:\n",
    "    vis.addEdge(edge['source'], edge['target'])\n",
    "vis.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45ad26-9e27-4006-877c-2a722c625092",
   "metadata": {},
   "outputs": [],
   "source": [
    "conceptnet_data.loc[conceptnet_data['source'] == 'never']\n",
    "nodes = conceptnet_data['source'].unique()\n",
    "index_A = vocab.index('movie')\n",
    "index_B = vocab.index('film')\n",
    "index_C = vocab.index('great')\n",
    "index_D = vocab.index('bad')\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_sim(A,B):\n",
    "    return np.dot(A,B)/(norm(A)*norm(B))\n",
    "\n",
    "A,B,C,D = node_embeddings[index_A], node_embeddings[index_B], node_embeddings[index_C], node_embeddings[index_D]\n",
    "cosine_sim(A,B), cosine_sim(A,C), cosine_sim(B,C), cosine_sim(B,D)\n",
    "# index_never, index_always, index_little\n",
    "# A,B,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "c8823209-4d3f-4a41-978a-3de62eabef51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# [(word, emb) for word, emb in zip(vocab, node_embeddings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "e1602d26-f50c-47cb-8800-7cf9da68de10",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# conceptnet_data.loc[conceptnet_data['source'] == 'never']\n",
    "# nodes = conceptnet_data['source'].unique()\n",
    "# index_A = graph['nodes'].index('after')\n",
    "# index_B = graph['nodes'].index('before')\n",
    "# index_C = graph['nodes'].index('later')\n",
    "# index_D = graph['nodes'].index('by')\n",
    "\n",
    "# from numpy.linalg import norm\n",
    "\n",
    "# def cosine_sim(A,B):\n",
    "#     return np.dot(A,B)/(norm(A)*norm(B))\n",
    "\n",
    "# A,B,C,D = node_embeddings[index_A].astype('float32'), node_embeddings[index_B].astype('float32'), node_embeddings[index_C].astype('float32'), node_embeddings[index_D].astype('float32')\n",
    "# cosine_sim(A,B), cosine_sim(A,C), cosine_sim(B,C), cosine_sim(B,D)\n",
    "# # index_never, index_always, index_little\n",
    "# # A,B,C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "643dd0da-acfd-4966-aa28-9841f6395271",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]  # Modified line\n",
    "        positions = tf.range(0, maxlen, 1)  # Modified line\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7146048c-63e5-4c02-956a-9990a8c75fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbedding(layers.Layer):\n",
    "    def __init__(self, node_embed_dim, node_embeddings):\n",
    "        super().__init__()\n",
    "        self.node_embeddings = tf.constant(node_embeddings, dtype=tf.float32)\n",
    "        self.node_embed_dim = node_embed_dim\n",
    "        self.dense = layers.Dense(node_embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "         # Reshape x to add a new dimension for embedding\n",
    "        node_indices = tf.cast(tf.math.round(x), tf.int32)\n",
    "        node_emb = tf.gather(self.node_embeddings, node_indices)\n",
    "        node_emb = self.dense(node_emb)  # Apply dense layer to each token embedding\n",
    "        return node_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ffa7b441-9e76-41c3-af92-55bc9c7b1b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, model = None, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "        if model: self.model = model\n",
    "        self.yes_count = 0\n",
    "        self.no_count = 0\n",
    "        self.activation_outputs = []\n",
    "        self.gradients = []\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = tf.convert_to_tensor(indices, dtype=tf.int32)\n",
    "        preds = tf.keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = tf.convert_to_tensor(preds, dtype=tf.float32)\n",
    "  \n",
    "        # Reshape logits to a matrix\n",
    "        logits_matrix = tf.reshape(logits, (1, -1))\n",
    "   \n",
    "        # Sample from the softmax probabilities\n",
    "        sampled_index = tf.random.categorical(tf.math.log(preds)[tf.newaxis, :], num_samples=1)\n",
    "\n",
    "        # Return the sampled index\n",
    "        return indices[sampled_index[0, 0]]\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number] if number < len(self.index_to_word) else '---'\n",
    "\n",
    "    def generate_token(self, _start_tokens, tokens_generated):\n",
    "        pad_len = maxlen - len(_start_tokens)\n",
    "        sample_index = len(_start_tokens) - 1\n",
    "        if pad_len < 0:\n",
    "            x = _start_tokens[:maxlen]\n",
    "            sample_index = maxlen - 1\n",
    "        elif pad_len > 0:\n",
    "            x = _start_tokens + [0] * pad_len\n",
    "        else:\n",
    "            x = _start_tokens\n",
    "        x = np.array([x])\n",
    "        y = self.model.predict(x, verbose=0)\n",
    "        sample_token = self.sample_from(y[0][sample_index])\n",
    "        tokens_generated.append(sample_token)\n",
    "        _start_tokens.append(sample_token)\n",
    "        num_tokens_generated = len(tokens_generated)\n",
    "\n",
    "        return _start_tokens, tokens_generated, num_tokens_generated, y\n",
    "\n",
    "    def get_generated_text(self, tokens_generated):\n",
    "        return \" \".join(\n",
    "            [self.detokenize(_) for _ in tokens_generated])\n",
    "        \n",
    "    def get_text(self, tokens_generated):\n",
    "        return \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated])\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _start_tokens = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        raw_outputs = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            _start_tokens, tokens_generated, num_tokens_generated, raw_output = self.generate_token(_start_tokens, tokens_generated)\n",
    "            raw_outputs.append(raw_output)\n",
    "            \n",
    "        txt = self.get_text(tokens_generated)\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "        self.yes_count += 1 if 'yes' in txt else 0\n",
    "        self.no_count += 1 if 'no' in txt else 0\n",
    "\n",
    "        return txt, raw_outputs\n",
    "    \n",
    "    def generate(self):\n",
    "        return self.on_epoc_end(1)\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "def callback(start_prompt, model=None, top_k=10):\n",
    "    start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "    num_tokens_generated = 200\n",
    "    return TextGenerator(num_tokens_generated, start_tokens, vocab, top_k=top_k) if not model else TextGenerator(num_tokens_generated, start_tokens, vocab, model, top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "55a846ea-5ac4-4567-87ec-f8fa941def77",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"float32\")\n",
    "    \n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    outputs_1 = layers.Dense(vocab_size, name='text_output')(x)\n",
    "\n",
    "    node_embedding_layer = NodeEmbedding(embed_dim, node_embeddings)\n",
    "    x2 = node_embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x2 = transformer_block(x2)\n",
    "\n",
    "    outputs_2 = layers.Dense(vocab_size, name='graph_output')(x2)\n",
    "\n",
    "    # Concatenate outputs of both branches\n",
    "    outputs = layers.Average()([outputs_1, outputs_2])\n",
    "\n",
    "    # outputs = outputs_1\n",
    "\n",
    "    # Softmax layer\n",
    "    # softmax_output = layers.Softmax()(outputs)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=inputs, outputs=[outputs])\n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(0.001),\n",
    "        loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "d8a89464-95aa-4f85-a45b-16177b35e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 200)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "token_and_position_embedding_4  (None, 200, 128)     665600      input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "node_embedding_4 (NodeEmbedding (None, 200, 128)     12928       input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_8 (Transforme (None, 200, 128)     165504      token_and_position_embedding_4[0]\n",
      "__________________________________________________________________________________________________\n",
      "transformer_block_9 (Transforme (None, 200, 128)     165504      node_embedding_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "text_output (Dense)             (None, 200, 5000)    645000      transformer_block_8[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "graph_output (Dense)            (None, 200, 5000)    645000      transformer_block_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "average_4 (Average)             (None, 200, 5000)    0           text_output[0][0]                \n",
      "                                                                 graph_output[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 2,299,536\n",
      "Trainable params: 2,299,536\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "bee3dc5a-315b-4583-ade5-551edbc6c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_gen_callback = callback(\"this movie is \", model, top_k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1cce73d0-6f9a-4586-bc1c-b9087b379b6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 10s 971ms/step - loss: 8.1619\n",
      "generated text:\n",
      "this movie is a [UNK]   by .     .\n",
      "\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 8s 987ms/step - loss: 6.9949\n",
      "generated text:\n",
      "this movie is is is the . this is the [UNK]  is [UNK]\n",
      "\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 9s 1s/step - loss: 6.0717\n",
      "generated text:\n",
      "this movie is the [UNK] . the [UNK] the [UNK] and the [UNK] is\n",
      "\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 5.4548\n",
      "generated text:\n",
      "this movie is the a a [UNK] .  , and .  \n",
      "\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 5.1042\n",
      "generated text:\n",
      "this movie is , [UNK]       ,  \n",
      "\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 4.8799\n",
      "generated text:\n",
      "this movie is the a [UNK]        \n",
      "\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 4.7512\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] [UNK] , but ,    \n",
      "\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 8s 975ms/step - loss: 4.5738\n",
      "generated text:\n",
      "this movie is a film [UNK] .       \n",
      "\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 8s 996ms/step - loss: 4.4816\n",
      "generated text:\n",
      "this movie is the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] . the [UNK]\n",
      "\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 8s 990ms/step - loss: 4.3713\n",
      "generated text:\n",
      "this movie is [UNK] and the [UNK] , i was [UNK] , and [UNK]\n",
      "\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 8s 990ms/step - loss: 4.3070\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] [UNK] . i have [UNK] [UNK] of his\n",
      "\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 4.1732\n",
      "generated text:\n",
      "this movie is an amazing [UNK] . it is [UNK] [UNK] [UNK] , but\n",
      "\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 8s 966ms/step - loss: 4.1355\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] [UNK] [UNK] [UNK] and [UNK] [UNK] of [UNK]\n",
      "\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 8s 951ms/step - loss: 4.0526\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] , the [UNK] [UNK] of [UNK] of the\n",
      "\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 8s 969ms/step - loss: 4.0494\n",
      "generated text:\n",
      "this movie is a good film is a [UNK] , and i think of\n",
      "\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 8s 966ms/step - loss: 4.0008\n",
      "generated text:\n",
      "this movie is one of this film was a lot of the [UNK] [UNK]\n",
      "\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 8s 968ms/step - loss: 3.9528\n",
      "generated text:\n",
      "this movie is a great movie was the [UNK] [UNK] [UNK] [UNK] [UNK] [UNK]\n",
      "\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 8s 959ms/step - loss: 3.9576\n",
      "generated text:\n",
      "this movie is a [UNK] and it is a great movie , and the\n",
      "\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 8s 948ms/step - loss: 3.8765\n",
      "generated text:\n",
      "this movie is a [UNK] . i was a [UNK] . it is a\n",
      "\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 8s 948ms/step - loss: 3.8460\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] , and i was very good , i\n",
      "\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 7s 910ms/step - loss: 3.8300\n",
      "generated text:\n",
      "this movie is the film . it is one of [UNK] of a great\n",
      "\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 8s 962ms/step - loss: 3.7652\n",
      "generated text:\n",
      "this movie is a [UNK] and [UNK] [UNK] [UNK] of [UNK] [UNK] [UNK] [UNK]\n",
      "\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.7356\n",
      "generated text:\n",
      "this movie is the film is a great movie , but it is a\n",
      "\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 7s 943ms/step - loss: 3.7026\n",
      "generated text:\n",
      "this movie is a [UNK] , [UNK] [UNK] [UNK] and [UNK] [UNK] \" [UNK]\n",
      "\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 3.6510\n",
      "generated text:\n",
      "this movie is a lot of the film is a good [UNK] and [UNK]\n",
      "\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 8s 934ms/step - loss: 3.6457\n",
      "generated text:\n",
      "this movie is a good film is a great movie , and it 's\n",
      "\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 8s 965ms/step - loss: 3.6315\n",
      "generated text:\n",
      "this movie is the film was a [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] of\n",
      "\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 8s 928ms/step - loss: 3.5995\n",
      "generated text:\n",
      "this movie is the movie that 's a [UNK] [UNK] [UNK] [UNK] [UNK] in\n",
      "\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 8s 964ms/step - loss: 3.5486\n",
      "generated text:\n",
      "this movie is an interesting to be the movie was [UNK] of the first\n",
      "\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 8s 950ms/step - loss: 3.5212\n",
      "generated text:\n",
      "this movie is the movie is one is a great , and it is\n",
      "\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 8s 943ms/step - loss: 3.5397\n",
      "generated text:\n",
      "this movie is a movie . a little movie , but the movie ,\n",
      "\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 8s 978ms/step - loss: 3.4861\n",
      "generated text:\n",
      "this movie is a movie . the film and very good story and funny\n",
      "\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 8s 968ms/step - loss: 3.4626\n",
      "generated text:\n",
      "this movie is a very good movie , but a [UNK] [UNK] and the\n",
      "\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 8s 958ms/step - loss: 3.4308\n",
      "generated text:\n",
      "this movie is not a [UNK] , a movie , but i have to\n",
      "\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 7s 939ms/step - loss: 3.4203\n",
      "generated text:\n",
      "this movie is not the movie , a good story , a movie is\n",
      "\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 8s 984ms/step - loss: 3.3662\n",
      "generated text:\n",
      "this movie is the movie that has some people that it 's a great\n",
      "\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 8s 950ms/step - loss: 3.3650\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] and a movie . a good job of\n",
      "\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 7s 907ms/step - loss: 3.3157\n",
      "generated text:\n",
      "this movie is not a [UNK] of the best performances , the film [UNK]\n",
      "\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 8s 932ms/step - loss: 3.2866\n",
      "generated text:\n",
      "this movie is not a [UNK] of the best film . the story is\n",
      "\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 8s 905ms/step - loss: 3.2517\n",
      "generated text:\n",
      "this movie is a movie , and very entertaining and enjoyed it has been\n",
      "\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 7s 911ms/step - loss: 3.2653\n",
      "generated text:\n",
      "this movie is a great film . the film . but it is about\n",
      "\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 8s 921ms/step - loss: 3.2360\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] and a great job of the movie of\n",
      "\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 7s 937ms/step - loss: 3.2079\n",
      "generated text:\n",
      "this movie is a very interesting and very funny movie . i loved it\n",
      "\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 8s 929ms/step - loss: 3.1752\n",
      "generated text:\n",
      "this movie is very good , a little movie . i have to be\n",
      "\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 8s 939ms/step - loss: 3.1370\n",
      "generated text:\n",
      "this movie is a [UNK] [UNK] [UNK] for example . i was [UNK] of\n",
      "\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 7s 838ms/step - loss: 3.1352\n",
      "generated text:\n",
      "this movie is not for a great movie . i have seen the movie\n",
      "\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 8s 937ms/step - loss: 3.0986\n",
      "generated text:\n",
      "this movie is a great movie . it is very funny and it is\n",
      "\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 8s 986ms/step - loss: 3.0765\n",
      "generated text:\n",
      "this movie is one of it 's a great plot of an old ladies\n",
      "\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 8s 943ms/step - loss: 3.0554\n",
      "generated text:\n",
      "this movie is not a movie about [UNK] [UNK] performance . sure that the\n",
      "\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 7s 930ms/step - loss: 3.0261\n",
      "generated text:\n",
      "this movie is a great movie , funny and very funny , but not\n",
      "\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 7s 915ms/step - loss: 3.0034\n",
      "generated text:\n",
      "this movie is a very hard to be a great movie . it is\n",
      "\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 8s 953ms/step - loss: 2.9734\n",
      "generated text:\n",
      "this movie is very good , and funny , but it 's good ,\n",
      "\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 7s 901ms/step - loss: 2.9443\n",
      "generated text:\n",
      "this movie is one of a very good story about the first , it\n",
      "\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 7s 900ms/step - loss: 2.9186\n",
      "generated text:\n",
      "this movie is a very well worth watching this film , a very well\n",
      "\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 7s 919ms/step - loss: 2.9055\n",
      "generated text:\n",
      "this movie is a very funny [UNK] , and very entertaining and entertaining and\n",
      "\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 7s 906ms/step - loss: 2.8662\n",
      "generated text:\n",
      "this movie is a very good movie . it has been an hour and\n",
      "\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 8s 987ms/step - loss: 2.8642\n",
      "generated text:\n",
      "this movie is a [UNK] a little bit on . i [UNK] of a\n",
      "\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 8s 927ms/step - loss: 2.8410\n",
      "generated text:\n",
      "this movie is a little more . i was a few spoilers * *\n",
      "\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 8s 907ms/step - loss: 2.7861\n",
      "generated text:\n",
      "this movie is a [UNK] . a very good movie . i 've seen\n",
      "\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 8s 923ms/step - loss: 2.7693\n",
      "generated text:\n",
      "this movie is very funny , it 's an amazing , and very funny\n",
      "\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 8s 979ms/step - loss: 2.7288\n",
      "generated text:\n",
      "this movie is very entertaining . the best actress , i enjoyed seeing this\n",
      "\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 8s 932ms/step - loss: 2.6950\n",
      "generated text:\n",
      "this movie is funny and entertaining comedy that 's not have seen , but\n",
      "\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 8s 962ms/step - loss: 2.6910\n",
      "generated text:\n",
      "this movie is not a very funny as good as well , and entertaining\n",
      "\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 7s 902ms/step - loss: 2.6803\n",
      "generated text:\n",
      "this movie is a [UNK] by alexander korda and [UNK] of the three 's\n",
      "\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 8s 944ms/step - loss: 2.6562\n",
      "generated text:\n",
      "this movie is funny , it is a little more interesting , and the\n",
      "\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 8s 947ms/step - loss: 2.6111\n",
      "generated text:\n",
      "this movie is a movie . the best movies about a movie to get\n",
      "\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 7s 922ms/step - loss: 2.6034\n",
      "generated text:\n",
      "this movie is a movie for that has a lot of [UNK] . a\n",
      "\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 8s 932ms/step - loss: 2.5724\n",
      "generated text:\n",
      "this movie is a very good film in a great , as far superior\n",
      "\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 8s 970ms/step - loss: 2.5614\n",
      "generated text:\n",
      "this movie is very good movie , but it was excellent in the most\n",
      "\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 8s 985ms/step - loss: 2.5136\n",
      "generated text:\n",
      "this movie is about a powerful message and a very well -written and as\n",
      "\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 8s 905ms/step - loss: 2.4942\n",
      "generated text:\n",
      "this movie is a movie to be pretty bad at first movie is not\n",
      "\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 8s 911ms/step - loss: 2.4650\n",
      "generated text:\n",
      "this movie is a good movie . it 's not only star and that\n",
      "\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 8s 919ms/step - loss: 2.4434\n",
      "generated text:\n",
      "this movie is not for about a family life in canada in this small\n",
      "\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 8s 948ms/step - loss: 2.4367\n",
      "generated text:\n",
      "this movie is very well worth the action movie .    \n",
      "\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 8s 913ms/step - loss: 2.4067\n",
      "generated text:\n",
      "this movie is extremely well with my favorite character is [UNK] of the first\n",
      "\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 7s 890ms/step - loss: 2.3802\n",
      "generated text:\n",
      "this movie is very entertaining , in the best known for the film was\n",
      "\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 8s 961ms/step - loss: 2.3564\n",
      "generated text:\n",
      "this movie is not , a [UNK] for entertainment ; [UNK] of [UNK] ,\n",
      "\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 7s 921ms/step - loss: 2.3307\n",
      "generated text:\n",
      "this movie is a very funny , and entertaining than most romantic movies that\n",
      "\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 7s 891ms/step - loss: 2.3151\n",
      "generated text:\n",
      "this movie is not to not . not expecting something that is not supposed\n",
      "\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 7s 894ms/step - loss: 2.2777\n",
      "generated text:\n",
      "this movie is a very good , funny and i would love to my\n",
      "\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 8s 926ms/step - loss: 2.2637\n",
      "generated text:\n",
      "this movie is very entertaining . a good comedy with good story and funny\n",
      "\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 8s 929ms/step - loss: 2.2371\n",
      "generated text:\n",
      "this movie is a very well worth watching it for a film . the\n",
      "\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 8s 965ms/step - loss: 2.2085\n",
      "generated text:\n",
      "this movie is extremely [UNK] about being told and then this movie with a\n",
      "\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 7s 910ms/step - loss: 2.2074\n",
      "generated text:\n",
      "this movie is a very entertaining movie is a funny , and entertaining and\n",
      "\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 8s 935ms/step - loss: 2.1736\n",
      "generated text:\n",
      "this movie is funny movie that doesn 't get back in the acting and\n",
      "\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 8s 978ms/step - loss: 2.1541\n",
      "generated text:\n",
      "this movie is very well worth it 's and it , the graphics are\n",
      "\n",
      "Epoch 87/300\n",
      "8/8 [==============================] - 8s 920ms/step - loss: 2.1375\n",
      "generated text:\n",
      "this movie is not a movie , so much about a good story .\n",
      "\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 7s 919ms/step - loss: 2.0905\n",
      "generated text:\n",
      "this movie is not to be a [UNK] a very entertaining - - a\n",
      "\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 8s 971ms/step - loss: 2.0718\n",
      "generated text:\n",
      "this movie is a [UNK] . a very good though , but there are\n",
      "\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 7s 882ms/step - loss: 2.0581\n",
      "generated text:\n",
      "this movie is an interesting character is a very much of the very good\n",
      "\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 8s 964ms/step - loss: 2.0434\n",
      "generated text:\n",
      "this movie is a [UNK] . the best movie is set the cinema in\n",
      "\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 8s 948ms/step - loss: 2.0221\n",
      "generated text:\n",
      "this movie is a fairly decent , that at all -time favorite characters and\n",
      "\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 8s 935ms/step - loss: 2.0025\n",
      "generated text:\n",
      "this movie is very entertaining in the first movie was funny , and profoundly\n",
      "\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 1.9910\n",
      "generated text:\n",
      "this movie is extremely well known , 's and i would have seen this\n",
      "\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 8s 945ms/step - loss: 1.9580\n",
      "generated text:\n",
      "this movie is very good movie , actually . what it has some of\n",
      "\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 8s 949ms/step - loss: 1.9484\n",
      "generated text:\n",
      "this movie is a very funny , [UNK] classic , i watched [UNK] by\n",
      "\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 8s 917ms/step - loss: 1.9433\n",
      "generated text:\n",
      "this movie is a very good . the best performance and i went through\n",
      "\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 7s 911ms/step - loss: 1.9065\n",
      "generated text:\n",
      "this movie is a fairly decent , about a little romp with cast also\n",
      "\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 8s 975ms/step - loss: 1.8872\n",
      "generated text:\n",
      "this movie is not an excellent movie since i had seen the [UNK] .\n",
      "\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.8675\n",
      "generated text:\n",
      "this movie is very entertaining , in a good acting was brilliant are quite\n",
      "\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.8488\n",
      "generated text:\n",
      "this movie is a better than many [UNK] on tv . if you could\n",
      "\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 9s 1s/step - loss: 1.8448\n",
      "generated text:\n",
      "this movie is an excellent movie . i had an excellent . it made\n",
      "\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.8213\n",
      "generated text:\n",
      "this movie is very entertaining . a predictable on watching this movie and profoundly\n",
      "\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.7835\n",
      "generated text:\n",
      "this movie is a very funny , but it was funny and interesting to\n",
      "\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 10s 1s/step - loss: 1.7730\n",
      "generated text:\n",
      "this movie is very entertaining and predictable in the genre of this movie 's\n",
      "\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7699\n",
      "generated text:\n",
      "this movie is a very well done . very funny . it 's good\n",
      "\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7471\n",
      "generated text:\n",
      "this movie is a good . [UNK] movie for entertainment , or has seen\n",
      "\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.7329\n",
      "generated text:\n",
      "this movie is tremendous for uplifting and this [UNK] i have to everyone .\n",
      "\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7056\n",
      "generated text:\n",
      "this movie is a fairly decent , about two friends and the time and\n",
      "\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 1.7050\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy in entertaining account of the most in\n",
      "\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 12s 1s/step - loss: 1.6813\n",
      "generated text:\n",
      "this movie is a good movie that 's not nominated . best actor at\n",
      "\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.6639\n",
      "generated text:\n",
      "this movie is a fairly entertaining if the best performances are decent , but\n",
      "\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.6423\n",
      "generated text:\n",
      "this movie is inspiring to is about people in my wife and [UNK] in\n",
      "\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 12s 2s/step - loss: 1.6390\n",
      "generated text:\n",
      "this movie is truly truly fantastic , i 've ever seen gwyneth paltrow in\n",
      "\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.6089\n",
      "generated text:\n",
      "this movie is a very entertaining comedy in my opinion , gabriel finds himself\n",
      "\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.5956\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy as entertaining if you 'd find out\n",
      "\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.5865\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5631\n",
      "generated text:\n",
      "this movie is a very good . a movie . i watched it very\n",
      "\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 1.5455\n",
      "generated text:\n",
      "this movie is a good . a few movies about [UNK] - - one\n",
      "\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5321\n",
      "generated text:\n",
      "this movie is a fairly decent , about two old sitcom and entertaining .\n",
      "\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5288\n",
      "generated text:\n",
      "this movie is an [UNK] a film to see that best films . it\n",
      "\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5218\n",
      "generated text:\n",
      "this movie is a very funny movie , but i didn 't feel like\n",
      "\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.5052\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . its rating . it is\n",
      "\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.4884\n",
      "generated text:\n",
      "this movie is very funny , but it isn 't just a good as\n",
      "\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.4742\n",
      "generated text:\n",
      "this movie is very predictable and [UNK] movie . some of the acting very\n",
      "\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.4597\n",
      "generated text:\n",
      "this movie is not a very good for entertainment value i watched time .\n",
      "\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.4404\n",
      "generated text:\n",
      "this movie is a little kid [UNK] festival in it i went on and\n",
      "\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.4163\n",
      "generated text:\n",
      "this movie is very good , funny and entertaining . i watched [UNK] about\n",
      "\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.4121\n",
      "generated text:\n",
      "this movie is a very good . i 've seen in ages and [UNK]\n",
      "\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3890\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is that has been in a few\n",
      "\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3962\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] , i [UNK] [UNK] . every\n",
      "\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3822\n",
      "generated text:\n",
      "this movie is a [UNK] . i must see a few incredible . when\n",
      "\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3650\n",
      "generated text:\n",
      "this movie is a [UNK] for best episode of all , you 've seen\n",
      "\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3479\n",
      "generated text:\n",
      "this movie is a very good . actually i loved the actors , but\n",
      "\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3488\n",
      "generated text:\n",
      "this movie is a very slow that funny and children 's its quality .\n",
      "\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3277\n",
      "generated text:\n",
      "this movie is a [UNK] . a few movies about [UNK] are not seen\n",
      "\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.3280\n",
      "generated text:\n",
      "this movie is a good movie , but it isn 't seen is a\n",
      "\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.3028\n",
      "generated text:\n",
      "this movie is a fairly entertaining story about two extremely entertaining things with about\n",
      "\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2977\n",
      "generated text:\n",
      "this movie is a better than most important story of movies , but after\n",
      "\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2905\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i don 't\n",
      "\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2903\n",
      "generated text:\n",
      "this movie is very predictable movie . a [UNK] plot i [UNK] of the\n",
      "\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2709\n",
      "generated text:\n",
      "this movie is a very good movie , i had seen by all time\n",
      "\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2695\n",
      "generated text:\n",
      "this movie is very well worth watching this film . i had to my\n",
      "\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2620\n",
      "generated text:\n",
      "this movie is not a supposed to be a true , which also makes\n",
      "\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2459\n",
      "generated text:\n",
      "this movie is a very good film for about it [UNK] and not only\n",
      "\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2300\n",
      "generated text:\n",
      "this movie is a very entertaining and that said about the most interesting story\n",
      "\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2306\n",
      "generated text:\n",
      "this movie is a very entertaining that 's plot in 1976 and thought that\n",
      "\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.2203\n",
      "generated text:\n",
      "this movie is very funny . a great plot . although i went by\n",
      "\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2223\n",
      "generated text:\n",
      "this movie is a fairly decent [UNK] for about two crime films and about\n",
      "\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.2026\n",
      "generated text:\n",
      "this movie is a very good film . i expected plot and is to\n",
      "\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1993\n",
      "generated text:\n",
      "this movie is very good for about a year old house [UNK] of all\n",
      "\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1722\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy in interesting humour and have seen the\n",
      "\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1698\n",
      "generated text:\n",
      "this movie is an old , some hilarious [UNK] debut of stuff , whose\n",
      "\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1543\n",
      "generated text:\n",
      "this movie is a fairly entertaining period piece of good performance and entertaining and\n",
      "\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 16s 2s/step - loss: 1.1566\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time it , everyone\n",
      "\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1471\n",
      "generated text:\n",
      "this movie is very funny , but you really loved the british comedies about\n",
      "\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1377\n",
      "generated text:\n",
      "this movie is very intelligent and that is . it has the first one\n",
      "\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1151\n",
      "generated text:\n",
      "this movie is a better than most 15 year old many [UNK] and just\n",
      "\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1088\n",
      "generated text:\n",
      "this movie is a good . a bit to review of film , and\n",
      "\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1130\n",
      "generated text:\n",
      "this movie is a very good movie for that 's actually very interesting ,\n",
      "\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.1024\n",
      "generated text:\n",
      "this movie is an [UNK] . the best reason i although [UNK] of it\n",
      "\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0926\n",
      "generated text:\n",
      "this movie is a very good movie in the killer . the [UNK] 's\n",
      "\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0835\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] to use of this [UNK] for\n",
      "\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0748\n",
      "generated text:\n",
      "this movie is definitely an odd love story , though it made the guy\n",
      "\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0612\n",
      "generated text:\n",
      "this movie is a very good . it 's about a warm and really\n",
      "\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0556\n",
      "generated text:\n",
      "this movie is a very good movie , but actually i have seen it\n",
      "\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0485\n",
      "generated text:\n",
      "this movie is a great story . toby malone plays an flat who his\n",
      "\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0478\n",
      "generated text:\n",
      "this movie is a fairly entertaining if not some of action flicks and really\n",
      "\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0358\n",
      "generated text:\n",
      "this movie is a fairly decent little unusual for the action film and the\n",
      "\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0416\n",
      "generated text:\n",
      "this movie is a very good story , some kind of jane austen 's\n",
      "\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 1.0323\n",
      "generated text:\n",
      "this movie is very entertaining . i would go see it if it got\n",
      "\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0255\n",
      "generated text:\n",
      "this movie is definitely not be until [UNK] through -out , , i was\n",
      "\n",
      "Epoch 173/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0244\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is or has been in a day\n",
      "\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0190\n",
      "generated text:\n",
      "this movie is a very good film . i had seen in all ages\n",
      "\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0037\n",
      "generated text:\n",
      "this movie is definitely for the intriguing performance by a [UNK] in the [UNK]\n",
      "\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 1.0023\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time you see things\n",
      "\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9873\n",
      "generated text:\n",
      "this movie is definitely a be impressed with doug mcgrath 's film and entertaining\n",
      "\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9764\n",
      "generated text:\n",
      "this movie is a fairly entertaining if a good story was good and entertaining\n",
      "\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9727\n",
      "generated text:\n",
      "this movie is definitely an odd character is a very good . it ,\n",
      "\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9635\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is or has been in a tough\n",
      "\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9573\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is or not only known . '\n",
      "\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9526\n",
      "generated text:\n",
      "this movie is definitely an odd love story . nicole kidman (i love for\n",
      "\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9362\n",
      "generated text:\n",
      "this movie is inspiring in anyone who is amazing story . where we are\n",
      "\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9336\n",
      "generated text:\n",
      "this movie is very intelligent , despite the most other actors [UNK] , however\n",
      "\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9270\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i have a\n",
      "\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9207\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] to use of this . this\n",
      "\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9275\n",
      "generated text:\n",
      "this movie is very funny , and entertaining . i would have rated it\n",
      "\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.9151\n",
      "generated text:\n",
      "this movie is very intelligent , despite the most precious little , [UNK] young\n",
      "\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9040\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i don 't\n",
      "\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.9055\n",
      "generated text:\n",
      "this movie is a [UNK] of being truly interesting . from the film for\n",
      "\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 15s 2s/step - loss: 0.8976\n",
      "generated text:\n",
      "this movie is very intelligent , but that 's all the most shocking piece\n",
      "\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8941\n",
      "generated text:\n",
      "this movie is a little bit , but it actually had a great .\n",
      "\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8776\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time it , every\n",
      "\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8870\n",
      "generated text:\n",
      "this movie is a little unusual in that it 's been the fans of\n",
      "\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8754\n",
      "generated text:\n",
      "this movie is inspiring to spectacular years ,but i [UNK] games is it 's\n",
      "\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8736\n",
      "generated text:\n",
      "this movie is a little bit , as a very good film and i\n",
      "\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8644\n",
      "generated text:\n",
      "this movie is a great story . i went into an extraordinary . time\n",
      "\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8600\n",
      "generated text:\n",
      "this movie is a very good film . i expected obvious for a [UNK]\n",
      "\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8621\n",
      "generated text:\n",
      "this movie is very funny , i couldn 't stop smiling when watching it\n",
      "\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8532\n",
      "generated text:\n",
      "this movie is intelligent . that is , it 's many well . it\n",
      "\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8502\n",
      "generated text:\n",
      "this movie is very old , some very funny . a romantic movie of\n",
      "\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8442\n",
      "generated text:\n",
      "this movie is a great movie , i didn 't up from watching this\n",
      "\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8421\n",
      "generated text:\n",
      "this movie is an old , some beautiful european ideas that will ever seen\n",
      "\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8359\n",
      "generated text:\n",
      "this movie is a great movie . very good for all the acting very\n",
      "\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8281\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] to go up . it .\n",
      "\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8253\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i 'll love\n",
      "\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8150\n",
      "generated text:\n",
      "this movie is a very slow pace and full of marijuana has many new\n",
      "\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8109\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch every\n",
      "\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.8093\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8060\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time it , it\n",
      "\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.8023\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is or has been in all time\n",
      "\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7890\n",
      "generated text:\n",
      "this movie is a [UNK] hour of [UNK] to be interesting and then most\n",
      "\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7796\n",
      "generated text:\n",
      "this movie is very funny , i would definitely it to see the best\n",
      "\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7848\n",
      "generated text:\n",
      "this movie is very intelligent , but that is a better . it is\n",
      "\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7766\n",
      "generated text:\n",
      "this movie is a better than average silent , it , it is still\n",
      "\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7785\n",
      "generated text:\n",
      "this movie is a great story . i really impressed . and sweet and\n",
      "\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7678\n",
      "generated text:\n",
      "this movie is intelligent , that is , more about its very engaging and\n",
      "\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7711\n",
      "generated text:\n",
      "this movie is worth seeing for the visual beauty and moving acting and the\n",
      "\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7591\n",
      "generated text:\n",
      "this movie is a great movie , i watch if the cast have seen\n",
      "\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7553\n",
      "generated text:\n",
      "this movie is a [UNK] . the beginning . it is amazing , wonderful\n",
      "\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7615\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch this\n",
      "\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7566\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7472\n",
      "generated text:\n",
      "this movie is a fairly interesting . the very good story and entertaining .\n",
      "\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7398\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7414\n",
      "generated text:\n",
      "this movie is great fun to the greatest movies ever seen , but the\n",
      "\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7342\n",
      "generated text:\n",
      "this movie is great fun to the next , of sorts - ? ?\n",
      "\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7386\n",
      "generated text:\n",
      "this movie is great little comedy , as it 's simply takes you how\n",
      "\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7285\n",
      "generated text:\n",
      "this movie is a great story , i went to make it and out\n",
      "\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7229\n",
      "generated text:\n",
      "this movie is great story of the end . . i still , i\n",
      "\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7215\n",
      "generated text:\n",
      "this movie is very funny , but it is predictable slasher flick with the\n",
      "\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7099\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i don 't\n",
      "\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7064\n",
      "generated text:\n",
      "this movie is worth seeing for the visual beauty and moving acting alone ,\n",
      "\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.7043\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] to see the time . every\n",
      "\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.7039\n",
      "generated text:\n",
      "this movie is great story in the end . i admire is told the\n",
      "\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6976\n",
      "generated text:\n",
      "this movie is a very good movie for that come up , but also\n",
      "\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6906\n",
      "generated text:\n",
      "this movie is very good . the screenplay is enchanting .   \n",
      "\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6849\n",
      "generated text:\n",
      "this movie is an old [UNK] about a beautiful soundtrack . where people have\n",
      "\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6819\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch every\n",
      "\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6764\n",
      "generated text:\n",
      "this movie is very good . i have seen this excellent movie and my\n",
      "\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6757\n",
      "generated text:\n",
      "this movie is very entertaining . that 's just found this film takes place\n",
      "\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6760\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch this\n",
      "\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6630\n",
      "generated text:\n",
      "this movie is a very slow of drama . it . it is the\n",
      "\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6692\n",
      "generated text:\n",
      "this movie is a very [UNK] about yakuza , it 's how world about\n",
      "\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6538\n",
      "generated text:\n",
      "this movie is great at first seen by some of sorts ago , you\n",
      "\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6542\n",
      "generated text:\n",
      "this movie is very entertaining . i have just watched it in and a\n",
      "\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6580\n",
      "generated text:\n",
      "this movie is intelligent . that is , more than most other movies ,\n",
      "\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6535\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6463\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the this [UNK] rating , it , every\n",
      "\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6355\n",
      "generated text:\n",
      "this movie is very entertaining . i just watched the movie went on dvd\n",
      "\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6447\n",
      "generated text:\n",
      "this movie is a little unusual in that it was actually filmed on the\n",
      "\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6348\n",
      "generated text:\n",
      "this movie is a very i mean . the first time . the most\n",
      "\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6391\n",
      "generated text:\n",
      "this movie is very old , some not about an action roles there made\n",
      "\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6244\n",
      "generated text:\n",
      "this movie is very entertaining . a pretty [UNK] but that the most successful\n",
      "\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6303\n",
      "generated text:\n",
      "this movie is a very i watched it on the contrary to see who\n",
      "\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6299\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6338\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . unfortunately , i 'll mention\n",
      "\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6278\n",
      "generated text:\n",
      "this movie is a bad . . it 's worth seeing for me .\n",
      "\n",
      "Epoch 258/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6274\n",
      "generated text:\n",
      "this movie is worth watching it , if again , a chick roles they\n",
      "\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6223\n",
      "generated text:\n",
      "this movie is very funny , but it isn 't all i always be\n",
      "\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6230\n",
      "generated text:\n",
      "this movie is very good . i watched the most unbelievable , impossible .\n",
      "\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6181\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.6159\n",
      "generated text:\n",
      "this movie is a little unusual in that it 's got a very [UNK]\n",
      "\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6063\n",
      "generated text:\n",
      "this movie is very good , despite the [UNK] fi production , but its\n",
      "\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6024\n",
      "generated text:\n",
      "this movie is a very [UNK] about an american country fell under [UNK] with\n",
      "\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.6051\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch this\n",
      "\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5978\n",
      "generated text:\n",
      "this movie is a very good film . it about a chick girl doesn\n",
      "\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5878\n",
      "generated text:\n",
      "this movie is very good . the most movies about yakuza [UNK] [UNK] research\n",
      "\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5863\n",
      "generated text:\n",
      "this movie is great at most shocking [UNK] and children who watched [UNK] than\n",
      "\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 14s 2s/step - loss: 0.5829\n",
      "generated text:\n",
      "this movie is very good . the screenplay is enchanting . but meryl streep\n",
      "\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5821\n",
      "generated text:\n",
      "this movie is [UNK] for about a powerful way which may not many other\n",
      "\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 8s 1s/step - loss: 0.5799\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 8s 976ms/step - loss: 0.5711\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 8s 970ms/step - loss: 0.5684\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 8s 941ms/step - loss: 0.5725\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 8s 989ms/step - loss: 0.5710\n",
      "generated text:\n",
      "this movie is a great story . in the usual face of him but\n",
      "\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 8s 967ms/step - loss: 0.5640\n",
      "generated text:\n",
      "this movie is great fun to my favorite scenes but it was better than\n",
      "\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 8s 967ms/step - loss: 0.5606\n",
      "generated text:\n",
      "this movie is a better than average silent movie and the [UNK] and it\n",
      "\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 8s 975ms/step - loss: 0.5605\n",
      "generated text:\n",
      "this movie is a better than average silent , but still a story of\n",
      "\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 8s 933ms/step - loss: 0.5519\n",
      "generated text:\n",
      "this movie is a very little , it 's about 18 or so bad\n",
      "\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 13s 1s/step - loss: 0.5573\n",
      "generated text:\n",
      "this movie is inspiring to anyone who has been said about movies , where\n",
      "\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5581\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every little this [UNK] i\n",
      "\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5536\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is or has been in the tough\n",
      "\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 10s 1s/step - loss: 0.5481\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5490\n",
      "generated text:\n",
      "this movie is a fairly entertaining comedy about murphy 's law being applied to\n",
      "\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5494\n",
      "generated text:\n",
      "this movie is very good . the screenplay is enchanting . but meryl streep\n",
      "\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5509\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 11s 1s/step - loss: 0.5507\n",
      "generated text:\n",
      "this movie is great intelligent , but most amazing . that 's shakti -\n",
      "\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5454\n",
      "generated text:\n",
      "this movie is a very good movie . i had about it in all\n",
      "\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5410\n",
      "generated text:\n",
      "this movie is [UNK] by friends , and worth watching the for all .\n",
      "\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 12s 1s/step - loss: 0.5390\n",
      "generated text:\n",
      "this movie is great fun to watch this movie . i went on video\n",
      "\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5376\n",
      "generated text:\n",
      "this movie is worth seeing for the visual beauty and moving acting alone ,\n",
      "\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5341\n",
      "generated text:\n",
      "this movie is inspiring to anyone who is or has been in a tough\n",
      "\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5234\n",
      "generated text:\n",
      "this movie is a great story . in the children [UNK] place and video\n",
      "\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 12s 2s/step - loss: 0.5274\n",
      "generated text:\n",
      "this movie is great cast , funny [UNK] and that well played by this\n",
      "\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5227\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5286\n",
      "generated text:\n",
      "this movie is the best movie ever . i have seen . it may\n",
      "\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5253\n",
      "generated text:\n",
      "this movie is tremendous for uplifting the [UNK] . every time i watch it\n",
      "\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5247\n",
      "generated text:\n",
      "this movie is amazing when i am not a very best movie i ,\n",
      "\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5116\n",
      "generated text:\n",
      "this movie is intelligent . that has , more than most other movies and\n",
      "\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 13s 2s/step - loss: 0.5165\n",
      "generated text:\n",
      "this movie is very funny , i couldn 't especially impressed with movies like\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(text_ds, verbose=1, epochs=300, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9b060e47-a3d2-439a-9a90-025a25eb42d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('text_output').output)\n",
    "graph_model = tf.keras.Model(inputs=model.input, outputs=model.get_layer('graph_output').output)\n",
    "text = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "7aac840f-0bf1-4142-aaea-9b7bfff8b7f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "i found [UNK] crazy ' to be [UNK] entertaining , and not to\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = callback(text, text_model, 1)\n",
    "output = generator.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "4b3269d6-79df-4d12-949f-acba6f191642",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "i found dvd dvd dvd dvd dvd vhs missing my find ! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = callback(text, graph_model, 5)\n",
    "output = generator.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "1d546c53-a2fd-4e3d-a9ed-a35bf2747f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated text:\n",
      "i found this movie to be suspenseful almost from the get -go .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "generator = callback(text, model, 1)\n",
    "output = generator.on_epoch_end(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "38a17fcb-d0d3-4944-9852-5cfa23c1eafb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.3238374,  9.122115 , 25.268015 , ..., -3.314001 , -8.441878 ,\n",
       "        -5.321046 ], dtype=float32),\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=13>,\n",
       " 31.088318,\n",
       " 'that',\n",
       " [<tf.Tensor: shape=(), dtype=int32, numpy=13>],\n",
       " 'this movie is a great example that')"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_tokens, tokens_generated, num_tokens_generated, raw_output = generator.generate_token([_ for _ in generator.start_tokens], [])\n",
    "index = len(generator.start_tokens) - 1\n",
    "raw_output[0][index], tf.argmax(raw_output[0][index]), np.max(raw_output[0][index]), vocab[tf.argmax(raw_output[0][index])], tokens_generated, generator.get_text(tokens_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "4895cfc9-be92-48b2-ba1c-2269585ab8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(     index      value\n",
       " 0        0  10.567556\n",
       " 1       18   4.778914\n",
       " 2        3   3.885499\n",
       " 3       26   3.815091\n",
       " 4       16   3.443519\n",
       " ..     ...        ...\n",
       " 295    243  -7.266131\n",
       " 296    142  -7.344137\n",
       " 297    246  -7.488056\n",
       " 298    237  -7.561741\n",
       " 299     37  -8.187204\n",
       " \n",
       " [300 rows x 2 columns],\n",
       " 'banjo')"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_indexes = np.argsort(raw_output[0][index])[::-1]\n",
    "sorted = raw_output[0][index][sorted_indexes]\n",
    "df = pd.DataFrame({'index': sorted_indexes, 'value': sorted})\n",
    "df, vocab[df.iloc[1]['index'].astype('int')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aad146-54d2-414b-a5b8-46fdef28d854",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-37",
   "language": "python",
   "name": "tf-37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
