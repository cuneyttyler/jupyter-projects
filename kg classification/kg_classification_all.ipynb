{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb0a91a7-478f-4718-9d04-83add12ecf3d",
   "metadata": {},
   "source": [
    "# CLASSIFICATION OF DBPEDIA DESCRIPTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcc24160-0955-4d99-bf99-05080446e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import tempfile\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph, IndexedArray\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging, csv\n",
    "from determine_topics import *\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "NUM_RELATIONS_PER_CLUSTER = 67\n",
    "NUM_ENTITIES_PER_CLUSTER = 400\n",
    "NUM_CLUSTERS = 20\n",
    "\n",
    "\n",
    "class HashTable:\n",
    "\n",
    "    # Create empty bucket list of given size\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.hash_table = self.create_buckets()\n",
    "\n",
    "    def create_buckets(self):\n",
    "        return [[] for _ in range(self.size)]\n",
    "\n",
    "    # Insert values into hash map\n",
    "    def set_val(self, key, val):\n",
    "\n",
    "        # Get the index from the key\n",
    "        # using hash function\n",
    "        hashed_key = hash(key) % self.size\n",
    "\n",
    "        # Get the bucket corresponding to index\n",
    "        bucket = self.hash_table[hashed_key]\n",
    "\n",
    "        found_key = False\n",
    "        for index, record in enumerate(bucket):\n",
    "            record_key, record_val = record\n",
    "\n",
    "            # check if the bucket has same key as\n",
    "            # the key to be inserted\n",
    "            if record_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    "\n",
    "        # If the bucket has same key as the key to be inserted,\n",
    "        # Update the key value\n",
    "        # Otherwise append the new key-value pair to the bucket\n",
    "        if found_key:\n",
    "            bucket[index] = (key, val)\n",
    "        else:\n",
    "            bucket.append((key, val))\n",
    "\n",
    "    # Return searched value with specific key\n",
    "    def get_val(self, key):\n",
    "\n",
    "        # Get the index from the key using\n",
    "        # hash function\n",
    "        hashed_key = hash(key) % self.size\n",
    "\n",
    "        # Get the bucket corresponding to index\n",
    "        bucket = self.hash_table[hashed_key]\n",
    "\n",
    "        found_key = False\n",
    "        for index, record in enumerate(bucket):\n",
    "            record_key, record_val = record\n",
    "\n",
    "            # check if the bucket has same key as\n",
    "            # the key being searched\n",
    "            if record_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    "\n",
    "        # If the bucket has same key as the key being searched,\n",
    "        # Return the value found\n",
    "        # Otherwise indicate there was no record found\n",
    "        if found_key:\n",
    "            return record_val\n",
    "        else:\n",
    "            raise ValueError('No record found.')\n",
    "\n",
    "    # Remove a value with specific key\n",
    "    def delete_val(self, key):\n",
    "\n",
    "        # Get the index from the key using\n",
    "        # hash function\n",
    "        hashed_key = hash(key) % self.size\n",
    "\n",
    "        # Get the bucket corresponding to index\n",
    "        bucket = self.hash_table[hashed_key]\n",
    "\n",
    "        found_key = False\n",
    "        for index, record in enumerate(bucket):\n",
    "            record_key, record_val = record\n",
    "\n",
    "            # check if the bucket has same key as\n",
    "            # the key to be deleted\n",
    "            if record_key == key:\n",
    "                found_key = True\n",
    "                break\n",
    "        if found_key:\n",
    "            bucket.pop(index)\n",
    "        return\n",
    "\n",
    "    # To print the items of hash map\n",
    "    def __str__(self):\n",
    "        return \"\".join(str(item) for item in self.hash_table)\n",
    "\n",
    "\n",
    "\n",
    "def pickler(path, pkl_name, obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def unpickler(path, pkl_name):\n",
    "    with open(os.path.join(path, pkl_name), 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        labels.append(d['label']) if d['label'] not in labels else None\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_x_and_y(data):\n",
    "    x, y = [], []\n",
    "    for d in data:\n",
    "        for dd in d['data']:\n",
    "            tmp = dd['text'].replace('\\n', '').replace('_', '')  # clean\n",
    "            x.append({'label': d['label'], 'dbpedia_uri': dd['dbpedia_uri'], 'context_data': dd['context_data'], 'text': tmp, 'graph': dd['graph']}) if len(tmp) > 0 else None\n",
    "            y.append(d['label']) if len(tmp) > 0 else None\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_label_index(label):\n",
    "    return [index for index, _label in enumerate(unique_labels) if label == _label][0]\n",
    "\n",
    "def get_context_data(data):\n",
    "    for d in data:\n",
    "        for dd in d['data']:\n",
    "            dd['context_data'] = [c for c in set(find_clusters(dd['context_graph']['nodes']))]\n",
    "\n",
    "\n",
    "def get_context_data_from_nodes(nodes):\n",
    "    return find_clusters([node.replace('http://dbpedia.org/resource/','').replace('_',' ') for node in nodes])\n",
    "\n",
    "\n",
    "def find_clusters(arr):\n",
    "    result = []\n",
    "    for el in arr:\n",
    "        all_mappings = node_cluster_map.get_val(el) if el in node_cluster_mapping_with_count else []\n",
    "        filtered_mappings = [e[0] for e in all_mappings[:min(len(all_mappings), 3)]]\n",
    "        result.extend(filtered_mappings)\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedc2b91-3c08-45e1-9003-e6e8c600dcca",
   "metadata": {
    "tags": []
   },
   "source": [
    "### PROCESS BATCH FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fee8283-b90b-4040-a2b2-1bd704e94268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_data_from_dbpedia_properties_v2(properties, dbpedia_uris):\n",
    "    context_data = [['Other'] for i in range(len(dbpedia_uris))]\n",
    "\n",
    "    logging.info(\"Extracting context data from dbpedia properties...\")\n",
    "    for i,property in enumerate(properties):\n",
    "        try:\n",
    "            index = dbpedia_uris.index(property['source'])\n",
    "            context_data[index] = get_context_data_from_nodes([target for target in property['targets'] if target != 'http://www.w3.org/2002/07/owl#Thing'])\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "    pickler('data','context_data_all.pkl',context_data)\n",
    "\n",
    "    return context_data\n",
    "\n",
    "def process_batch_v2(file_index, tfidf, model):\n",
    "    logging.info('Processing file %d...' % (file_index))\n",
    "    with open(os.path.join('data/dbpedia_properties', \"{}.pkl\".format(file_index)), 'rb') as f:\n",
    "        properties = pickle5.load(f)\n",
    "\n",
    "    new_texts,new_dbpedia_uris,new_properties = [], [], []\n",
    "    for i,property in enumerate(properties):\n",
    "        try:\n",
    "            # logging.info('Processing property (%d-%d)' % (file_index,i)) if i % 1000 == 0 else None\n",
    "            index = dbpedia_uris_map.get_val(property['source'])\n",
    "            new_texts.append(texts[index])\n",
    "            new_properties.append(property)\n",
    "            new_dbpedia_uris.append(property['source'])\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    context_data_input = get_context_data_from_dbpedia_properties_v2(new_properties, new_dbpedia_uris)\n",
    "    context_data_input = pd.DataFrame(context_data_input).values\n",
    "\n",
    "    logging.info('Encoding context data...')\n",
    "    for c in context_data_input:\n",
    "        for i, v in enumerate(c):\n",
    "            c[i] = 'Other' if not v else v\n",
    "\n",
    "    cluster_names = unique_labels + ['None']\n",
    "\n",
    "    BATCH_SIZE, result_labels = 10000, []\n",
    "    for i in range(0,len(new_texts), BATCH_SIZE):\n",
    "        logging.info('Processing batch %d...' % i)\n",
    "        batch_texts = new_texts[i: min(i + BATCH_SIZE, len(new_texts))]\n",
    "        batch_context_data_input = context_data_input[i: min(i + BATCH_SIZE, len(new_texts))]\n",
    "\n",
    "        context_data_encodings = m.predict(batch_context_data_input)\n",
    "\n",
    "        for encoding in context_data_encodings:\n",
    "            encoding[78] = 0.0  # other\n",
    "\n",
    "        x_vectors = tfidf.transform(batch_texts)\n",
    "        x_svc = np.concatenate((x_vectors.toarray(), context_data_encodings), axis=1)\n",
    "\n",
    "        logging.info(\"Running the algorithm... %d\" % (file_index))\n",
    "        batch_result_labels = model.predict(x_svc)\n",
    "        result_labels.extend(batch_result_labels)\n",
    "\n",
    "    logging.info(\"Writing results...\")\n",
    "    with open('determine_topics_results_v2/{}.csv'.format(file_index), 'w', encoding='UTF8') as f:\n",
    "        writer = csv.writer(f)\n",
    "        header = ['Dbpedia Uri', 'Topic', 'Category', 'SubCategory']\n",
    "        writer.writerow(header)\n",
    "\n",
    "        i = 0\n",
    "        for uri, label in zip(new_dbpedia_uris, result_labels):\n",
    "            i += 1\n",
    "\n",
    "            splitted_label = label.split('-')\n",
    "            topic_label = splitted_label[0]\n",
    "            category_label = splitted_label[1] if len(splitted_label) > 1 else None\n",
    "            subcategory_label = splitted_label[2] if len(splitted_label) > 2 else None\n",
    "\n",
    "            topic = [topic for topic in topics if topic['name'] == topic_label]\n",
    "            topic = topic[0] if len(topic) > 0 else None\n",
    "            category = [c for c in topic['categories'] if c['name'] == category_label]\n",
    "            category = category[0] if len(category) > 0 else None\n",
    "            subcategory = None\n",
    "\n",
    "            result_topic = topic['name'] if topic else topic_label\n",
    "            result_category = category['name'] if category else category_label\n",
    "            result_subcategory = subcategory['name'] if subcategory else subcategory_label\n",
    "            writer.writerow([uri, result_topic, result_category, result_subcategory])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5222967-8f37-41b4-9b04-cfc8b4f002f5",
   "metadata": {},
   "source": [
    "### LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6c729f3-7f47-469b-9a09-053c20a988c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle5 as pickle5\n",
    "import os\n",
    "with open(os.path.join('data', 'classification_data_with_graphs_v5.pkl'), 'rb') as f:\n",
    "    data = pickle5.load(f)\n",
    "with open(os.path.join('data', 'node_cluster_mapping_v5_with_count.pkl'), 'rb') as f:\n",
    "    node_cluster_mapping_with_count = pickle5.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83ca515d-bed3-4ff9-82a1-028abc240239",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_cluster_map = HashTable(len(node_cluster_mapping_with_count.keys()))\n",
    "for key in node_cluster_mapping_with_count.keys():\n",
    "    mapping = node_cluster_mapping_with_count[key]\n",
    "    node_cluster_map.set_val(key,mapping)\n",
    "\n",
    "get_context_data(data)\n",
    "x,y = get_x_and_y(data)\n",
    "unique_labels = get_labels(data)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_val, y_val,x_test, y_test = x_test[:500], y_test[:500], x_test[500:], y_test[500:]\n",
    "y_train_int, y_test_int = [get_label_index(label) for label in y_train],[get_label_index(label) for label in y_test]\n",
    "x_train_text,x_val_text,x_test_text = [xx['text'] for xx in x_train],[xx['text'] for xx in x_val], [xx['text'] for xx in x_test]\n",
    "x_train_context, x_val_context, x_test_context = [xx['context_data'] for xx in x_train],[xx['context_data'] for xx in x_val], [xx['context_data'] for xx in x_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0587a102-0fc2-4d2f-8c2e-8c88858ead83",
   "metadata": {},
   "source": [
    "### LOAD DBPEDIA TEXTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "753e0efd-aa7c-43ee-9845-bf252ab700ff",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-22 13:40:54,935:INFO:Reading dbpedia articles...\n",
      "2023-01-22 13:43:27,738:INFO:Processing dbpedia uris...\n",
      "2023-01-22 13:43:31,154:INFO:Processing dbpediauri 0...\n",
      "2023-01-22 13:43:31,253:INFO:Processing dbpediauri 100000...\n",
      "2023-01-22 13:43:31,354:INFO:Processing dbpediauri 200000...\n",
      "2023-01-22 13:43:31,455:INFO:Processing dbpediauri 300000...\n",
      "2023-01-22 13:43:31,558:INFO:Processing dbpediauri 400000...\n",
      "2023-01-22 13:43:31,663:INFO:Processing dbpediauri 500000...\n",
      "2023-01-22 13:43:31,765:INFO:Processing dbpediauri 600000...\n",
      "2023-01-22 13:43:31,869:INFO:Processing dbpediauri 700000...\n",
      "2023-01-22 13:43:31,973:INFO:Processing dbpediauri 800000...\n",
      "2023-01-22 13:43:32,084:INFO:Processing dbpediauri 900000...\n",
      "2023-01-22 13:43:32,194:INFO:Processing dbpediauri 1000000...\n",
      "2023-01-22 13:43:32,304:INFO:Processing dbpediauri 1100000...\n",
      "2023-01-22 13:43:32,415:INFO:Processing dbpediauri 1200000...\n",
      "2023-01-22 13:43:32,526:INFO:Processing dbpediauri 1300000...\n",
      "2023-01-22 13:43:32,638:INFO:Processing dbpediauri 1400000...\n",
      "2023-01-22 13:43:32,751:INFO:Processing dbpediauri 1500000...\n",
      "2023-01-22 13:43:32,866:INFO:Processing dbpediauri 1600000...\n",
      "2023-01-22 13:43:32,981:INFO:Processing dbpediauri 1700000...\n",
      "2023-01-22 13:43:33,097:INFO:Processing dbpediauri 1800000...\n",
      "2023-01-22 13:43:33,215:INFO:Processing dbpediauri 1900000...\n",
      "2023-01-22 13:43:33,333:INFO:Processing dbpediauri 2000000...\n",
      "2023-01-22 13:43:33,455:INFO:Processing dbpediauri 2100000...\n",
      "2023-01-22 13:43:33,582:INFO:Processing dbpediauri 2200000...\n",
      "2023-01-22 13:43:33,713:INFO:Processing dbpediauri 2300000...\n",
      "2023-01-22 13:43:33,845:INFO:Processing dbpediauri 2400000...\n",
      "2023-01-22 13:43:33,980:INFO:Processing dbpediauri 2500000...\n",
      "2023-01-22 13:43:34,117:INFO:Processing dbpediauri 2600000...\n",
      "2023-01-22 13:43:34,250:INFO:Processing dbpediauri 2700000...\n",
      "2023-01-22 13:43:34,385:INFO:Processing dbpediauri 2800000...\n",
      "2023-01-22 13:43:34,525:INFO:Processing dbpediauri 2900000...\n",
      "2023-01-22 13:43:34,660:INFO:Processing dbpediauri 3000000...\n",
      "2023-01-22 13:43:34,814:INFO:Processing dbpediauri 3100000...\n",
      "2023-01-22 13:43:34,949:INFO:Processing dbpediauri 3200000...\n",
      "2023-01-22 13:43:35,086:INFO:Processing dbpediauri 3300000...\n",
      "2023-01-22 13:43:35,226:INFO:Processing dbpediauri 3400000...\n"
     ]
    }
   ],
   "source": [
    "dbpedia_uris, texts = read_dbpedia()\n",
    "\n",
    "logging.info('Processing dbpedia uris...')\n",
    "dbpedia_uris_map = HashTable(len(dbpedia_uris))\n",
    "for i,dbpedia_uri in enumerate(dbpedia_uris):\n",
    "    logging.info('Processing dbpediauri %d...' % i) if i % 100000 == 0 else None\n",
    "    dbpedia_uris_map.set_val(dbpedia_uri, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0bf3c-580a-4eb9-ba20-0a75af5868fa",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd48ae3c-6640-4d1d-9684-7d84294fbf1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='graph_input'), name='graph_input', description=\"created by layer 'graph_input'\"), but it was called on an input with incompatible shape (None, 36).\n",
      "2023-01-22 13:48:30,058:WARNING:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='graph_input'), name='graph_input', description=\"created by layer 'graph_input'\"), but it was called on an input with incompatible shape (None, 36).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_names = unique_labels + ['None']\n",
    "\n",
    "context_input = Input(shape=(1,), dtype='string', name='graph_input')\n",
    "context_layer = tf.keras.layers.StringLookup(vocabulary=cluster_names, output_mode='multi_hot')(context_input)\n",
    "\n",
    "m = tf.keras.models.Model(inputs=[context_input], outputs=context_layer)\n",
    "\n",
    "context_data_train_input = pd.DataFrame(x_train_context).values\n",
    "\n",
    "for c in context_data_train_input:\n",
    "    for i, v in enumerate(c):\n",
    "        c[i] = 'Other' if not v else v\n",
    "\n",
    "context_train_encodings = m.predict(context_data_train_input)\n",
    "\n",
    "for encoding in context_train_encodings:\n",
    "    encoding[78] = 0.0 # other\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "tfidf = vectorizer.fit(x_train_text)\n",
    "x_train_vectors = tfidf.transform(x_train_text)\n",
    "x_train_svc = np.concatenate((x_train_vectors.toarray(), context_train_encodings), axis=1)\n",
    "\n",
    "model = LinearSVC()\n",
    "model.fit(x_train_svc, y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "011a23c4-6cc0-4348-a7e9-27c256db8ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_text = LinearSVC()\n",
    "model_text.fit(x_train_vectors, y=y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79f525a-086a-4e08-ac0a-191b8f52e07e",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f208d4ff-6fd1-43bc-acc6-7c3c490ac886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-22 17:04:36,625:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 17:05:06,476:INFO:Encoding context data...\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('data/dbpedia_properties', \"{}.pkl\".format(1)), 'rb') as f:\n",
    "    properties = pickle5.load(f)\n",
    "\n",
    "new_texts,new_dbpedia_uris,new_properties = [], [], []\n",
    "for i,property in enumerate(properties):\n",
    "    try:\n",
    "        logging.info('Processing property (%d-%d)' % (file_index,i)) if i % 1000 == 0 else None\n",
    "        index = dbpedia_uris_map.get_val(property['source'])\n",
    "        new_texts.append(texts[index])\n",
    "        new_properties.append(property)\n",
    "        new_dbpedia_uris.append(property['source'])\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "context_data_input = get_context_data_from_dbpedia_properties_v2(new_properties, new_dbpedia_uris)\n",
    "context_data_input = pd.DataFrame(context_data_input).values\n",
    "\n",
    "logging.info('Encoding context data...')\n",
    "for c in context_data_input:\n",
    "    for i, v in enumerate(c):\n",
    "        c[i] = 'Other' if not v else v\n",
    "\n",
    "cluster_names = unique_labels + ['None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2928d0dc-b40b-44fb-ad5f-91c4e794206e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-22 13:46:09,409:INFO:Processing batch 0...\n",
      "2023-01-22 13:46:19,428:INFO:Running the algorithm...\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE, result_labels = 10000, []\n",
    "i = 0\n",
    "logging.info('Processing batch %d...' % i)\n",
    "batch_texts = new_texts[i: min(i + BATCH_SIZE, len(new_texts))]\n",
    "batch_context_data_input = context_data_input[i: min(i + BATCH_SIZE, len(new_texts))]\n",
    "\n",
    "context_data_encodings = m.predict(batch_context_data_input)\n",
    "\n",
    "for encoding in context_data_encodings:\n",
    "    encoding[78] = 0.0  # other\n",
    "\n",
    "x_vectors = tfidf.transform(batch_texts)\n",
    "x_svc = np.concatenate((x_vectors.toarray(), context_data_encodings), axis=1)\n",
    "\n",
    "logging.info(\"Running the algorithm...\")\n",
    "batch_result_labels = model.predict(x_svc)\n",
    "result_labels.extend(batch_result_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "73ebeb5e-abe7-464e-988c-a86dc9bfbbea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!Women Art Revolution is a 2010 documentary film directed by Lynn Hershman Leeson and distributed by Zeitgeist Films. It tracks the feminist art movement over 40 years through interviews with artists, curators, critics, and historians.\"@e',\n",
       " {'source': 'http://dbpedia.org/resource/!Women_Art_Revolution',\n",
       "  'targets': ['http://dbpedia.org/resource/Lynn_Hershman_Leeson',\n",
       "   'http://dbpedia.org/resource/Zeitgeist_Films',\n",
       "   'http://dbpedia.org/resource/Lynn_Hershman_Leeson',\n",
       "   'http://dbpedia.org/resource/Carrie_Brownstein',\n",
       "   'http://dbpedia.org/resource/Cláudia_Pascoal']},\n",
       " array(['Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other', 'Other', 'Other', 'Other', 'Other', 'Other', 'Other',\n",
       "        'Other'], dtype=object))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "new_texts[i], new_properties[i], context_data_input[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fd1aad-e31b-4ed4-93e9-4f8414a626a5",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://dbpedia.org/resource/!!!_(album) Art-Music\n",
      "http://dbpedia.org/resource/!Action_Pact! Art-Music\n",
      "http://dbpedia.org/resource/!Hero_(album) Religion\n",
      "http://dbpedia.org/resource/!Oka_Tokat Mythology\n",
      "http://dbpedia.org/resource/!PAUS3 Art-Music\n",
      "http://dbpedia.org/resource/!Women_Art_Revolution Science-Social Sciences\n",
      "http://dbpedia.org/resource/$1.99_Romances Art-Fashion\n",
      "http://dbpedia.org/resource/$100,000_Fortune_Hunt Philosophy\n",
      "http://dbpedia.org/resource/$24_in_24 Culture-Country\n",
      "http://dbpedia.org/resource/$25_Million_Dollar_Hoax Media-TV Series & Shows\n",
      "http://dbpedia.org/resource/$9.99 Nature-Animal\n",
      "http://dbpedia.org/resource/$O$ Art-Fashion-Designer\n",
      "http://dbpedia.org/resource/$_(Mark_Sultan_album) Art-Photography-Photographer\n",
      "http://dbpedia.org/resource/$ell_Out Art-Music\n",
      "http://dbpedia.org/resource/$h*!_My_Dad_Says Art-Cinema\n",
      "http://dbpedia.org/resource/$pent Art-Theatre\n",
      "http://dbpedia.org/resource/%22900%22,_Cahiers_d'Italie_et_d'Europe Science-Social Sciences\n",
      "http://dbpedia.org/resource/%22900%22,_Cahiers_d'Italie_et_d'Europe Art-Theatre\n",
      "http://dbpedia.org/resource/%22A%22_Is_for_Alibi Art-Theatre\n",
      "http://dbpedia.org/resource/%22B%22_Is_for_Burglar Art-Theatre\n",
      "http://dbpedia.org/resource/%22Babbacombe%22_Lee Religion\n",
      "http://dbpedia.org/resource/%22Bund%22_in_Latvia Science-Politics\n",
      "http://dbpedia.org/resource/%22Buzz!!%22_The_Movie Science-Chemistry\n",
      "http://dbpedia.org/resource/%22C%22_Is_for_(Please_Insert_Sophomoric_Genitalia_Reference_Here) Science-Chemistry\n",
      "http://dbpedia.org/resource/%22C%22_Is_for_Corpse Art-Theatre\n",
      "http://dbpedia.org/resource/%22Country%22_Johnny_Mathis Art-Music\n",
      "http://dbpedia.org/resource/%22Country%22_Johnny_Mathis Art-Music-Instrument\n",
      "http://dbpedia.org/resource/%22Crocodylus%22_affinis Nature-Plant\n",
      "http://dbpedia.org/resource/%22Crocodylus%22_affinis Nature-Animal\n",
      "http://dbpedia.org/resource/%22Crocodylus%22_megarhinus Nature-Animal\n",
      "http://dbpedia.org/resource/%22D%22_Is_for_Deadbeat Art-Theatre\n",
      "http://dbpedia.org/resource/%22E%22_Is_for_Evidence Art-Theatre\n",
      "http://dbpedia.org/resource/%22E%22_Is_for_Evidence Mythology-Supernatural\n",
      "http://dbpedia.org/resource/%22F%22_Is_for_Fugitive Art-Theatre\n",
      "http://dbpedia.org/resource/%22FF.SS.%22_–_Cioè:_%22...che_mi_hai_portato_a_fare_sopra_a_Posillipo_se_non_mi_vuoi_più_bene%3F%22 Art-Theatre\n",
      "http://dbpedia.org/resource/%22Freeway%22_Rick_Ross Culture\n",
      "http://dbpedia.org/resource/%22G%22_Is_for_Gumshoe Art-Theatre\n",
      "http://dbpedia.org/resource/%22GDD_CUP%22_International_Challenger_Guangzhou Technology-Civil Engineering\n",
      "http://dbpedia.org/resource/%22Gjergj_Kastrioti_Skënderbeu%22_Decoration Philosophy\n",
      "http://dbpedia.org/resource/%22H%22_Is_for_Homicide Art-Theatre\n",
      "http://dbpedia.org/resource/%22H%22_Is_for_Homicide Mythology-Supernatural\n",
      "http://dbpedia.org/resource/%22Happy%22_in_Galoshes Art-Music\n",
      "http://dbpedia.org/resource/%22Happy%22_in_Galoshes Mythology-Supernatural\n",
      "http://dbpedia.org/resource/%22Heroes%22_(David_Bowie_song) Science-Politics\n",
      "http://dbpedia.org/resource/%22I%22_Is_for_Innocent Art-Theatre\n",
      "http://dbpedia.org/resource/%22I%22_Is_for_Innocent Mythology-Supernatural\n",
      "http://dbpedia.org/resource/%22If_This_Goes_On—%22 Art-Theatre\n",
      "http://dbpedia.org/resource/%22In%22_Jazz_for_the_Culture_Set Art-Music\n",
      "http://dbpedia.org/resource/%22It%22_the_Album Art-Music\n",
      "http://dbpedia.org/resource/%22J%22_Is_for_Judgment Art-Theatre\n"
     ]
    }
   ],
   "source": [
    "for dbpedia_uri, label in zip(new_dbpedia_uris[:50], batch_result_labels[:50]):\n",
    "    print(dbpedia_uri,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ce9cfa44-f8b4-436f-8ee3-eb6cd4768830",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Art-Photography-Photographer', 13),\n",
       " ('Art-Painting-Artist', 12),\n",
       " ('Art-Sculpting-Artist', 11),\n",
       " ('Media-Documentary', 7),\n",
       " ('Art-Painting', 5),\n",
       " ('Art-Literature-Writer', 5),\n",
       " ('Technology-Video Game', 5),\n",
       " ('Media-TV Series & Shows', 5),\n",
       " ('Culture-Historical Figure', 4),\n",
       " ('Sports', 4),\n",
       " ('Transportation-Railway', 4),\n",
       " ('Art-Cinema', 3),\n",
       " ('Art-Cinema-Actor', 3),\n",
       " ('Art-Fashion-Designer', 3),\n",
       " ('Science-Politics', 3),\n",
       " ('Media-News', 3),\n",
       " ('Military-Aviation', 3),\n",
       " ('Transportation-Land', 3),\n",
       " ('Transportation-Naval', 3),\n",
       " ('Art-Theatre-Actor', 2),\n",
       " ('Art-Fashion', 2),\n",
       " ('Art-Fashion-Model', 2),\n",
       " ('Art-Dance-Dancer', 2),\n",
       " ('Technology-Electronics', 2),\n",
       " ('Religion', 2),\n",
       " ('Mythology', 2),\n",
       " ('Media', 2),\n",
       " ('Military-Weapon', 2),\n",
       " ('Transportation-Aviation', 2),\n",
       " ('Art-Music', 1),\n",
       " ('Art-Music-Instrument', 1),\n",
       " ('Art-Literature', 1),\n",
       " ('Art-Dance', 1),\n",
       " ('Science-Physics', 1),\n",
       " ('Science-Mathematics', 1),\n",
       " ('Science-Agriculture', 1),\n",
       " ('Science-Archeology', 1),\n",
       " ('Science-Antropology', 1),\n",
       " ('Science-Economics', 1),\n",
       " ('Science-Psychology', 1),\n",
       " ('Technology-Computer Science', 1),\n",
       " ('Technology-Mechanics', 1),\n",
       " ('Technology-Civil Engineering', 1),\n",
       " ('Technology-Robotics', 1),\n",
       " ('Nature-Plant', 1),\n",
       " ('Culture-Country', 1),\n",
       " ('Media-Anime', 1),\n",
       " ('Media-Cartoon', 1),\n",
       " ('Military-Land', 1)]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_cluster_map.get_val('Netherlands')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619bd231-7d01-44c3-8916-4cc04829be8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_uri = \"https://dbpedia.org/page/Eilema_tricolorana\"\n",
    "index = [i for i, uri in enumerate(dbpedia_uris) if uri == dbpedia_uri][0]\n",
    "index, len(result_labels)\n",
    "properties_ = [p for p in properties if p['source'] == dbpedia_uri]\n",
    "# properties_,get_context_data_from_dbpedia_properties_v2(properties_, [dbpedia_uri])\n",
    "properties_,get_context_data_from_nodes([p['targets'] for p in properties if p['source'] == dbpedia_uri][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7226fe2d-6615-49f7-a61a-eb592f189524",
   "metadata": {},
   "source": [
    "### RUN ON BATCHES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "13ec6c13-0772-490c-8424-11405c7ef564",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-01-22 14:46:39,157:INFO:Processing file 1...\n",
      "2023-01-22 14:46:39,549:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:47:03,854:INFO:Encoding context data...\n",
      "2023-01-22 14:47:05,387:INFO:Processing batch 0...\n",
      "WARNING:tensorflow:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='graph_input'), name='graph_input', description=\"created by layer 'graph_input'\"), but it was called on an input with incompatible shape (None, 393).\n",
      "2023-01-22 14:47:05,538:WARNING:Model was constructed with shape (None, 1) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.string, name='graph_input'), name='graph_input', description=\"created by layer 'graph_input'\"), but it was called on an input with incompatible shape (None, 393).\n",
      "2023-01-22 14:47:13,043:INFO:Running the algorithm... 1\n",
      "2023-01-22 14:47:13,601:INFO:Processing batch 10000...\n",
      "2023-01-22 14:47:22,531:INFO:Running the algorithm... 1\n",
      "2023-01-22 14:47:22,933:INFO:Processing batch 20000...\n",
      "2023-01-22 14:47:30,709:INFO:Running the algorithm... 1\n",
      "2023-01-22 14:47:31,055:INFO:Processing batch 30000...\n",
      "2023-01-22 14:47:39,926:INFO:Running the algorithm... 1\n",
      "2023-01-22 14:47:40,313:INFO:Processing batch 40000...\n",
      "2023-01-22 14:47:44,773:INFO:Running the algorithm... 1\n",
      "2023-01-22 14:47:45,021:INFO:Writing results...\n",
      "2023-01-22 14:47:45,383:INFO:Processing file 2...\n",
      "2023-01-22 14:47:46,998:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:48:01,803:INFO:Encoding context data...\n",
      "2023-01-22 14:48:02,546:INFO:Processing batch 0...\n",
      "2023-01-22 14:48:07,534:INFO:Running the algorithm... 2\n",
      "2023-01-22 14:48:08,140:INFO:Processing batch 10000...\n",
      "2023-01-22 14:48:14,187:INFO:Running the algorithm... 2\n",
      "2023-01-22 14:48:14,589:INFO:Processing batch 20000...\n",
      "2023-01-22 14:48:19,716:INFO:Running the algorithm... 2\n",
      "2023-01-22 14:48:20,106:INFO:Processing batch 30000...\n",
      "2023-01-22 14:48:24,975:INFO:Running the algorithm... 2\n",
      "2023-01-22 14:48:25,276:INFO:Writing results...\n",
      "2023-01-22 14:48:25,644:INFO:Processing file 3...\n",
      "2023-01-22 14:48:26,463:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:48:45,491:INFO:Encoding context data...\n",
      "2023-01-22 14:48:45,961:INFO:Processing batch 0...\n",
      "2023-01-22 14:48:49,322:INFO:Running the algorithm... 3\n",
      "2023-01-22 14:48:49,698:INFO:Processing batch 10000...\n",
      "2023-01-22 14:48:53,976:INFO:Running the algorithm... 3\n",
      "2023-01-22 14:48:54,393:INFO:Processing batch 20000...\n",
      "2023-01-22 14:48:57,692:INFO:Running the algorithm... 3\n",
      "2023-01-22 14:48:58,075:INFO:Processing batch 30000...\n",
      "2023-01-22 14:49:02,258:INFO:Running the algorithm... 3\n",
      "2023-01-22 14:49:02,621:INFO:Processing batch 40000...\n",
      "2023-01-22 14:49:03,542:INFO:Running the algorithm... 3\n",
      "2023-01-22 14:49:03,669:INFO:Writing results...\n",
      "2023-01-22 14:49:03,963:INFO:Processing file 4...\n",
      "2023-01-22 14:49:04,491:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:49:34,341:INFO:Encoding context data...\n",
      "2023-01-22 14:49:34,840:INFO:Processing batch 0...\n",
      "2023-01-22 14:49:37,764:INFO:Running the algorithm... 4\n",
      "2023-01-22 14:49:38,352:INFO:Processing batch 10000...\n",
      "2023-01-22 14:49:42,460:INFO:Running the algorithm... 4\n",
      "2023-01-22 14:49:42,853:INFO:Processing batch 20000...\n",
      "2023-01-22 14:49:45,981:INFO:Running the algorithm... 4\n",
      "2023-01-22 14:49:46,351:INFO:Processing batch 30000...\n",
      "2023-01-22 14:49:50,564:INFO:Running the algorithm... 4\n",
      "2023-01-22 14:49:50,964:INFO:Processing batch 40000...\n",
      "2023-01-22 14:49:54,008:INFO:Running the algorithm... 4\n",
      "2023-01-22 14:49:54,530:INFO:Processing batch 50000...\n",
      "2023-01-22 14:49:55,077:INFO:Running the algorithm... 4\n",
      "2023-01-22 14:49:55,166:INFO:Writing results...\n",
      "2023-01-22 14:49:55,495:INFO:Processing file 5...\n",
      "2023-01-22 14:49:57,351:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:50:32,222:INFO:Encoding context data...\n",
      "2023-01-22 14:50:33,447:INFO:Processing batch 0...\n",
      "2023-01-22 14:50:39,404:INFO:Running the algorithm... 5\n",
      "2023-01-22 14:50:39,806:INFO:Processing batch 10000...\n",
      "2023-01-22 14:50:46,761:INFO:Running the algorithm... 5\n",
      "2023-01-22 14:50:47,168:INFO:Processing batch 20000...\n",
      "2023-01-22 14:50:52,955:INFO:Running the algorithm... 5\n",
      "2023-01-22 14:50:53,333:INFO:Processing batch 30000...\n",
      "2023-01-22 14:51:00,397:INFO:Running the algorithm... 5\n",
      "2023-01-22 14:51:00,797:INFO:Processing batch 40000...\n",
      "2023-01-22 14:51:06,662:INFO:Running the algorithm... 5\n",
      "2023-01-22 14:51:07,032:INFO:Processing batch 50000...\n",
      "2023-01-22 14:51:08,740:INFO:Running the algorithm... 5\n",
      "2023-01-22 14:51:08,892:INFO:Writing results...\n",
      "2023-01-22 14:51:09,251:INFO:Processing file 6...\n",
      "2023-01-22 14:51:09,784:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:51:43,712:INFO:Encoding context data...\n",
      "2023-01-22 14:51:44,351:INFO:Processing batch 0...\n",
      "2023-01-22 14:51:47,777:INFO:Running the algorithm... 6\n",
      "2023-01-22 14:51:48,293:INFO:Processing batch 10000...\n",
      "2023-01-22 14:51:53,213:INFO:Running the algorithm... 6\n",
      "2023-01-22 14:51:53,666:INFO:Processing batch 20000...\n",
      "2023-01-22 14:51:57,337:INFO:Running the algorithm... 6\n",
      "2023-01-22 14:51:57,701:INFO:Processing batch 30000...\n",
      "2023-01-22 14:52:02,282:INFO:Running the algorithm... 6\n",
      "2023-01-22 14:52:02,677:INFO:Processing batch 40000...\n",
      "2023-01-22 14:52:06,183:INFO:Running the algorithm... 6\n",
      "2023-01-22 14:52:06,558:INFO:Processing batch 50000...\n",
      "2023-01-22 14:52:08,376:INFO:Running the algorithm... 6\n",
      "2023-01-22 14:52:08,560:INFO:Writing results...\n",
      "2023-01-22 14:52:08,908:INFO:Processing file 7...\n",
      "2023-01-22 14:52:09,472:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:52:53,732:INFO:Encoding context data...\n",
      "2023-01-22 14:52:56,093:INFO:Processing batch 0...\n",
      "2023-01-22 14:53:04,999:INFO:Running the algorithm... 7\n",
      "2023-01-22 14:53:05,601:INFO:Processing batch 10000...\n",
      "2023-01-22 14:53:15,548:INFO:Running the algorithm... 7\n",
      "2023-01-22 14:53:15,964:INFO:Processing batch 20000...\n",
      "2023-01-22 14:53:25,073:INFO:Running the algorithm... 7\n",
      "2023-01-22 14:53:25,549:INFO:Processing batch 30000...\n",
      "2023-01-22 14:53:35,906:INFO:Running the algorithm... 7\n",
      "2023-01-22 14:53:36,283:INFO:Processing batch 40000...\n",
      "2023-01-22 14:53:45,146:INFO:Running the algorithm... 7\n",
      "2023-01-22 14:53:45,576:INFO:Processing batch 50000...\n",
      "2023-01-22 14:53:54,193:INFO:Running the algorithm... 7\n",
      "2023-01-22 14:53:54,594:INFO:Writing results...\n",
      "2023-01-22 14:53:55,030:INFO:Processing file 8...\n",
      "2023-01-22 14:53:56,371:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:54:31,624:INFO:Encoding context data...\n",
      "2023-01-22 14:54:32,292:INFO:Processing batch 0...\n",
      "2023-01-22 14:54:35,844:INFO:Running the algorithm... 8\n",
      "2023-01-22 14:54:36,246:INFO:Processing batch 10000...\n",
      "2023-01-22 14:54:40,940:INFO:Running the algorithm... 8\n",
      "2023-01-22 14:54:41,330:INFO:Processing batch 20000...\n",
      "2023-01-22 14:54:44,927:INFO:Running the algorithm... 8\n",
      "2023-01-22 14:54:45,285:INFO:Processing batch 30000...\n",
      "2023-01-22 14:54:49,787:INFO:Running the algorithm... 8\n",
      "2023-01-22 14:54:50,165:INFO:Processing batch 40000...\n",
      "2023-01-22 14:54:53,697:INFO:Running the algorithm... 8\n",
      "2023-01-22 14:54:54,072:INFO:Processing batch 50000...\n",
      "2023-01-22 14:54:56,767:INFO:Running the algorithm... 8\n",
      "2023-01-22 14:54:57,032:INFO:Writing results...\n",
      "2023-01-22 14:54:57,413:INFO:Processing file 9...\n",
      "2023-01-22 14:54:58,950:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:55:33,396:INFO:Encoding context data...\n",
      "2023-01-22 14:55:34,277:INFO:Processing batch 0...\n",
      "2023-01-22 14:55:38,609:INFO:Running the algorithm... 9\n",
      "2023-01-22 14:55:39,133:INFO:Processing batch 10000...\n",
      "2023-01-22 14:55:44,574:INFO:Running the algorithm... 9\n",
      "2023-01-22 14:55:44,932:INFO:Processing batch 20000...\n",
      "2023-01-22 14:55:49,273:INFO:Running the algorithm... 9\n",
      "2023-01-22 14:55:49,627:INFO:Processing batch 30000...\n",
      "2023-01-22 14:55:55,148:INFO:Running the algorithm... 9\n",
      "2023-01-22 14:55:55,535:INFO:Processing batch 40000...\n",
      "2023-01-22 14:56:00,044:INFO:Running the algorithm... 9\n",
      "2023-01-22 14:56:00,411:INFO:Processing batch 50000...\n",
      "2023-01-22 14:56:02,378:INFO:Running the algorithm... 9\n",
      "2023-01-22 14:56:02,567:INFO:Writing results...\n",
      "2023-01-22 14:56:02,917:INFO:Processing file 10...\n",
      "2023-01-22 14:56:03,466:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:56:40,797:INFO:Encoding context data...\n",
      "2023-01-22 14:56:42,028:INFO:Processing batch 0...\n",
      "2023-01-22 14:56:47,682:INFO:Running the algorithm... 10\n",
      "2023-01-22 14:56:48,186:INFO:Processing batch 10000...\n",
      "2023-01-22 14:56:55,469:INFO:Running the algorithm... 10\n",
      "2023-01-22 14:56:55,927:INFO:Processing batch 20000...\n",
      "2023-01-22 14:57:02,286:INFO:Running the algorithm... 10\n",
      "2023-01-22 14:57:02,732:INFO:Processing batch 30000...\n",
      "2023-01-22 14:57:10,167:INFO:Running the algorithm... 10\n",
      "2023-01-22 14:57:10,637:INFO:Processing batch 40000...\n",
      "2023-01-22 14:57:17,118:INFO:Running the algorithm... 10\n",
      "2023-01-22 14:57:17,616:INFO:Processing batch 50000...\n",
      "2023-01-22 14:57:21,454:INFO:Running the algorithm... 10\n",
      "2023-01-22 14:57:21,850:INFO:Writing results...\n",
      "2023-01-22 14:57:22,244:INFO:Processing file 11...\n",
      "2023-01-22 14:57:22,969:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:58:09,996:INFO:Encoding context data...\n",
      "2023-01-22 14:58:10,718:INFO:Processing batch 0...\n",
      "2023-01-22 14:58:15,028:INFO:Running the algorithm... 11\n",
      "2023-01-22 14:58:15,601:INFO:Processing batch 10000...\n",
      "2023-01-22 14:58:20,864:INFO:Running the algorithm... 11\n",
      "2023-01-22 14:58:21,366:INFO:Processing batch 20000...\n",
      "2023-01-22 14:58:25,495:INFO:Running the algorithm... 11\n",
      "2023-01-22 14:58:26,008:INFO:Processing batch 30000...\n",
      "2023-01-22 14:58:31,266:INFO:Running the algorithm... 11\n",
      "2023-01-22 14:58:31,791:INFO:Processing batch 40000...\n",
      "2023-01-22 14:58:35,975:INFO:Running the algorithm... 11\n",
      "2023-01-22 14:58:36,465:INFO:Processing batch 50000...\n",
      "2023-01-22 14:58:38,196:INFO:Running the algorithm... 11\n",
      "2023-01-22 14:58:38,604:INFO:Writing results...\n",
      "2023-01-22 14:58:39,001:INFO:Processing file 12...\n",
      "2023-01-22 14:58:41,360:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 14:59:31,753:INFO:Encoding context data...\n",
      "2023-01-22 14:59:33,509:INFO:Processing batch 0...\n",
      "2023-01-22 14:59:41,648:INFO:Running the algorithm... 12\n",
      "2023-01-22 14:59:42,253:INFO:Processing batch 10000...\n",
      "2023-01-22 14:59:51,185:INFO:Running the algorithm... 12\n",
      "2023-01-22 14:59:51,695:INFO:Processing batch 20000...\n",
      "2023-01-22 14:59:59,549:INFO:Running the algorithm... 12\n",
      "2023-01-22 15:00:00,039:INFO:Processing batch 30000...\n",
      "2023-01-22 15:00:08,759:INFO:Running the algorithm... 12\n",
      "2023-01-22 15:00:09,246:INFO:Processing batch 40000...\n",
      "2023-01-22 15:00:17,400:INFO:Running the algorithm... 12\n",
      "2023-01-22 15:00:17,886:INFO:Processing batch 50000...\n",
      "2023-01-22 15:00:20,749:INFO:Running the algorithm... 12\n",
      "2023-01-22 15:00:20,924:INFO:Writing results...\n",
      "2023-01-22 15:00:21,372:INFO:Processing file 13...\n",
      "2023-01-22 15:00:21,937:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:01:08,642:INFO:Encoding context data...\n",
      "2023-01-22 15:01:09,822:INFO:Processing batch 0...\n",
      "2023-01-22 15:01:15,387:INFO:Running the algorithm... 13\n",
      "2023-01-22 15:01:15,949:INFO:Processing batch 10000...\n",
      "2023-01-22 15:01:22,762:INFO:Running the algorithm... 13\n",
      "2023-01-22 15:01:23,258:INFO:Processing batch 20000...\n",
      "2023-01-22 15:01:29,006:INFO:Running the algorithm... 13\n",
      "2023-01-22 15:01:29,501:INFO:Processing batch 30000...\n",
      "2023-01-22 15:01:36,320:INFO:Running the algorithm... 13\n",
      "2023-01-22 15:01:36,854:INFO:Processing batch 40000...\n",
      "2023-01-22 15:01:42,653:INFO:Running the algorithm... 13\n",
      "2023-01-22 15:01:43,152:INFO:Processing batch 50000...\n",
      "2023-01-22 15:01:44,922:INFO:Running the algorithm... 13\n",
      "2023-01-22 15:01:45,160:INFO:Writing results...\n",
      "2023-01-22 15:01:45,546:INFO:Processing file 14...\n",
      "2023-01-22 15:01:47,748:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:02:38,692:INFO:Encoding context data...\n",
      "2023-01-22 15:02:39,985:INFO:Processing batch 0...\n",
      "2023-01-22 15:02:46,380:INFO:Running the algorithm... 14\n",
      "2023-01-22 15:02:46,873:INFO:Processing batch 10000...\n",
      "2023-01-22 15:02:54,009:INFO:Running the algorithm... 14\n",
      "2023-01-22 15:02:54,500:INFO:Processing batch 20000...\n",
      "2023-01-22 15:03:00,808:INFO:Running the algorithm... 14\n",
      "2023-01-22 15:03:01,170:INFO:Processing batch 30000...\n",
      "2023-01-22 15:03:08,342:INFO:Running the algorithm... 14\n",
      "2023-01-22 15:03:08,899:INFO:Processing batch 40000...\n",
      "2023-01-22 15:03:15,092:INFO:Running the algorithm... 14\n",
      "2023-01-22 15:03:15,584:INFO:Processing batch 50000...\n",
      "2023-01-22 15:03:18,585:INFO:Running the algorithm... 14\n",
      "2023-01-22 15:03:18,914:INFO:Writing results...\n",
      "2023-01-22 15:03:19,318:INFO:Processing file 15...\n",
      "2023-01-22 15:03:19,891:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:04:11,819:INFO:Encoding context data...\n",
      "2023-01-22 15:04:12,534:INFO:Processing batch 0...\n",
      "2023-01-22 15:04:16,576:INFO:Running the algorithm... 15\n",
      "2023-01-22 15:04:17,211:INFO:Processing batch 10000...\n",
      "2023-01-22 15:04:22,256:INFO:Running the algorithm... 15\n",
      "2023-01-22 15:04:22,722:INFO:Processing batch 20000...\n",
      "2023-01-22 15:04:26,875:INFO:Running the algorithm... 15\n",
      "2023-01-22 15:04:27,351:INFO:Processing batch 30000...\n",
      "2023-01-22 15:04:32,344:INFO:Running the algorithm... 15\n",
      "2023-01-22 15:04:32,883:INFO:Processing batch 40000...\n",
      "2023-01-22 15:04:37,007:INFO:Running the algorithm... 15\n",
      "2023-01-22 15:04:37,518:INFO:Processing batch 50000...\n",
      "2023-01-22 15:04:41,024:INFO:Running the algorithm... 15\n",
      "2023-01-22 15:04:41,424:INFO:Writing results...\n",
      "2023-01-22 15:04:41,919:INFO:Processing file 16...\n",
      "2023-01-22 15:04:43,085:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:05:32,232:INFO:Encoding context data...\n",
      "2023-01-22 15:05:32,966:INFO:Processing batch 0...\n",
      "2023-01-22 15:05:36,995:INFO:Running the algorithm... 16\n",
      "2023-01-22 15:05:37,564:INFO:Processing batch 10000...\n",
      "2023-01-22 15:05:42,771:INFO:Running the algorithm... 16\n",
      "2023-01-22 15:05:43,276:INFO:Processing batch 20000...\n",
      "2023-01-22 15:05:47,439:INFO:Running the algorithm... 16\n",
      "2023-01-22 15:05:47,900:INFO:Processing batch 30000...\n",
      "2023-01-22 15:05:53,152:INFO:Running the algorithm... 16\n",
      "2023-01-22 15:05:53,697:INFO:Processing batch 40000...\n",
      "2023-01-22 15:05:57,963:INFO:Running the algorithm... 16\n",
      "2023-01-22 15:05:58,444:INFO:Processing batch 50000...\n",
      "2023-01-22 15:06:00,721:INFO:Running the algorithm... 16\n",
      "2023-01-22 15:06:01,054:INFO:Writing results...\n",
      "2023-01-22 15:06:01,428:INFO:Processing file 17...\n",
      "2023-01-22 15:06:05,140:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:06:56,965:INFO:Encoding context data...\n",
      "2023-01-22 15:06:59,290:INFO:Processing batch 0...\n",
      "2023-01-22 15:07:09,625:INFO:Running the algorithm... 17\n",
      "2023-01-22 15:07:10,231:INFO:Processing batch 10000...\n",
      "2023-01-22 15:07:22,151:INFO:Running the algorithm... 17\n",
      "2023-01-22 15:07:22,780:INFO:Processing batch 20000...\n",
      "2023-01-22 15:07:33,366:INFO:Running the algorithm... 17\n",
      "2023-01-22 15:07:33,834:INFO:Processing batch 30000...\n",
      "2023-01-22 15:07:44,837:INFO:Running the algorithm... 17\n",
      "2023-01-22 15:07:45,345:INFO:Processing batch 40000...\n",
      "2023-01-22 15:07:55,605:INFO:Running the algorithm... 17\n",
      "2023-01-22 15:07:56,109:INFO:Processing batch 50000...\n",
      "2023-01-22 15:07:58,591:INFO:Running the algorithm... 17\n",
      "2023-01-22 15:07:58,810:INFO:Writing results...\n",
      "2023-01-22 15:07:59,221:INFO:Processing file 18...\n",
      "2023-01-22 15:08:01,336:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:08:48,450:INFO:Encoding context data...\n",
      "2023-01-22 15:08:49,194:INFO:Processing batch 0...\n",
      "2023-01-22 15:08:53,448:INFO:Running the algorithm... 18\n",
      "2023-01-22 15:08:53,996:INFO:Processing batch 10000...\n",
      "2023-01-22 15:08:59,169:INFO:Running the algorithm... 18\n",
      "2023-01-22 15:08:59,717:INFO:Processing batch 20000...\n",
      "2023-01-22 15:09:03,999:INFO:Running the algorithm... 18\n",
      "2023-01-22 15:09:04,523:INFO:Processing batch 30000...\n",
      "2023-01-22 15:09:09,712:INFO:Running the algorithm... 18\n",
      "2023-01-22 15:09:10,237:INFO:Processing batch 40000...\n",
      "2023-01-22 15:09:14,553:INFO:Running the algorithm... 18\n",
      "2023-01-22 15:09:15,026:INFO:Processing batch 50000...\n",
      "2023-01-22 15:09:16,465:INFO:Running the algorithm... 18\n",
      "2023-01-22 15:09:16,809:INFO:Writing results...\n",
      "2023-01-22 15:09:17,212:INFO:Processing file 19...\n",
      "2023-01-22 15:09:18,369:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:10:05,136:INFO:Encoding context data...\n",
      "2023-01-22 15:10:05,664:INFO:Processing batch 0...\n",
      "2023-01-22 15:10:09,322:INFO:Running the algorithm... 19\n",
      "2023-01-22 15:10:09,898:INFO:Processing batch 10000...\n",
      "2023-01-22 15:10:14,331:INFO:Running the algorithm... 19\n",
      "2023-01-22 15:10:14,839:INFO:Processing batch 20000...\n",
      "2023-01-22 15:10:18,369:INFO:Running the algorithm... 19\n",
      "2023-01-22 15:10:18,880:INFO:Processing batch 30000...\n",
      "2023-01-22 15:10:23,452:INFO:Running the algorithm... 19\n",
      "2023-01-22 15:10:24,100:INFO:Processing batch 40000...\n",
      "2023-01-22 15:10:27,572:INFO:Running the algorithm... 19\n",
      "2023-01-22 15:10:28,034:INFO:Processing batch 50000...\n",
      "2023-01-22 15:10:29,301:INFO:Running the algorithm... 19\n",
      "2023-01-22 15:10:29,548:INFO:Writing results...\n",
      "2023-01-22 15:10:29,941:INFO:Processing file 20...\n",
      "2023-01-22 15:10:30,533:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:11:17,194:INFO:Encoding context data...\n",
      "2023-01-22 15:11:17,828:INFO:Processing batch 0...\n",
      "2023-01-22 15:11:21,652:INFO:Running the algorithm... 20\n",
      "2023-01-22 15:11:22,224:INFO:Processing batch 10000...\n",
      "2023-01-22 15:11:27,367:INFO:Running the algorithm... 20\n",
      "2023-01-22 15:11:27,856:INFO:Processing batch 20000...\n",
      "2023-01-22 15:11:31,765:INFO:Running the algorithm... 20\n",
      "2023-01-22 15:11:32,227:INFO:Processing batch 30000...\n",
      "2023-01-22 15:11:37,056:INFO:Running the algorithm... 20\n",
      "2023-01-22 15:11:37,545:INFO:Processing batch 40000...\n",
      "2023-01-22 15:11:41,348:INFO:Running the algorithm... 20\n",
      "2023-01-22 15:11:41,822:INFO:Processing batch 50000...\n",
      "2023-01-22 15:11:42,811:INFO:Running the algorithm... 20\n",
      "2023-01-22 15:11:43,001:INFO:Writing results...\n",
      "2023-01-22 15:11:43,350:INFO:Processing file 21...\n",
      "2023-01-22 15:11:44,998:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:12:36,373:INFO:Encoding context data...\n",
      "2023-01-22 15:12:37,562:INFO:Processing batch 0...\n",
      "2023-01-22 15:12:43,458:INFO:Running the algorithm... 21\n",
      "2023-01-22 15:12:43,930:INFO:Processing batch 10000...\n",
      "2023-01-22 15:12:50,723:INFO:Running the algorithm... 21\n",
      "2023-01-22 15:12:51,214:INFO:Processing batch 20000...\n",
      "2023-01-22 15:12:57,120:INFO:Running the algorithm... 21\n",
      "2023-01-22 15:12:57,576:INFO:Processing batch 30000...\n",
      "2023-01-22 15:13:04,383:INFO:Running the algorithm... 21\n",
      "2023-01-22 15:13:04,909:INFO:Processing batch 40000...\n",
      "2023-01-22 15:13:10,867:INFO:Running the algorithm... 21\n",
      "2023-01-22 15:13:11,362:INFO:Processing batch 50000...\n",
      "2023-01-22 15:13:14,569:INFO:Running the algorithm... 21\n",
      "2023-01-22 15:13:14,919:INFO:Writing results...\n",
      "2023-01-22 15:13:15,335:INFO:Processing file 22...\n",
      "2023-01-22 15:13:16,118:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:14:08,875:INFO:Encoding context data...\n",
      "2023-01-22 15:14:12,279:INFO:Processing batch 0...\n",
      "2023-01-22 15:14:26,163:INFO:Running the algorithm... 22\n",
      "2023-01-22 15:14:26,741:INFO:Processing batch 10000...\n",
      "2023-01-22 15:14:41,341:INFO:Running the algorithm... 22\n",
      "2023-01-22 15:14:41,844:INFO:Processing batch 20000...\n",
      "2023-01-22 15:14:55,697:INFO:Running the algorithm... 22\n",
      "2023-01-22 15:14:56,186:INFO:Processing batch 30000...\n",
      "2023-01-22 15:15:10,704:INFO:Running the algorithm... 22\n",
      "2023-01-22 15:15:11,197:INFO:Processing batch 40000...\n",
      "2023-01-22 15:15:25,078:INFO:Running the algorithm... 22\n",
      "2023-01-22 15:15:25,592:INFO:Processing batch 50000...\n",
      "2023-01-22 15:15:28,845:INFO:Running the algorithm... 22\n",
      "2023-01-22 15:15:29,040:INFO:Writing results...\n",
      "2023-01-22 15:15:29,477:INFO:Processing file 23...\n",
      "2023-01-22 15:15:31,329:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:16:14,398:INFO:Encoding context data...\n",
      "2023-01-22 15:16:15,340:INFO:Processing batch 0...\n",
      "2023-01-22 15:16:20,596:INFO:Running the algorithm... 23\n",
      "2023-01-22 15:16:21,225:INFO:Processing batch 10000...\n",
      "2023-01-22 15:16:27,459:INFO:Running the algorithm... 23\n",
      "2023-01-22 15:16:27,953:INFO:Processing batch 20000...\n",
      "2023-01-22 15:16:33,321:INFO:Running the algorithm... 23\n",
      "2023-01-22 15:16:33,837:INFO:Processing batch 30000...\n",
      "2023-01-22 15:16:40,063:INFO:Running the algorithm... 23\n",
      "2023-01-22 15:16:40,548:INFO:Processing batch 40000...\n",
      "2023-01-22 15:16:45,872:INFO:Running the algorithm... 23\n",
      "2023-01-22 15:16:46,373:INFO:Processing batch 50000...\n",
      "2023-01-22 15:16:47,094:INFO:Running the algorithm... 23\n",
      "2023-01-22 15:16:47,210:INFO:Writing results...\n",
      "2023-01-22 15:16:47,588:INFO:Processing file 24...\n",
      "2023-01-22 15:16:48,281:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:17:28,925:INFO:Encoding context data...\n",
      "2023-01-22 15:17:29,941:INFO:Processing batch 0...\n",
      "2023-01-22 15:17:34,807:INFO:Running the algorithm... 24\n",
      "2023-01-22 15:17:35,252:INFO:Processing batch 10000...\n",
      "2023-01-22 15:17:41,018:INFO:Running the algorithm... 24\n",
      "2023-01-22 15:17:41,385:INFO:Processing batch 20000...\n",
      "2023-01-22 15:17:46,258:INFO:Running the algorithm... 24\n",
      "2023-01-22 15:17:46,603:INFO:Processing batch 30000...\n",
      "2023-01-22 15:17:52,285:INFO:Running the algorithm... 24\n",
      "2023-01-22 15:17:52,682:INFO:Processing batch 40000...\n",
      "2023-01-22 15:17:57,523:INFO:Running the algorithm... 24\n",
      "2023-01-22 15:17:57,903:INFO:Processing batch 50000...\n",
      "2023-01-22 15:17:59,626:INFO:Running the algorithm... 24\n",
      "2023-01-22 15:17:59,779:INFO:Writing results...\n",
      "2023-01-22 15:18:00,129:INFO:Processing file 25...\n",
      "2023-01-22 15:18:00,691:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:18:32,432:INFO:Encoding context data...\n",
      "2023-01-22 15:18:33,059:INFO:Processing batch 0...\n",
      "2023-01-22 15:18:36,396:INFO:Running the algorithm... 25\n",
      "2023-01-22 15:18:36,884:INFO:Processing batch 10000...\n",
      "2023-01-22 15:18:41,044:INFO:Running the algorithm... 25\n",
      "2023-01-22 15:18:41,388:INFO:Processing batch 20000...\n",
      "2023-01-22 15:18:44,806:INFO:Running the algorithm... 25\n",
      "2023-01-22 15:18:45,150:INFO:Processing batch 30000...\n",
      "2023-01-22 15:18:49,322:INFO:Running the algorithm... 25\n",
      "2023-01-22 15:18:49,724:INFO:Processing batch 40000...\n",
      "2023-01-22 15:18:53,215:INFO:Running the algorithm... 25\n",
      "2023-01-22 15:18:53,621:INFO:Processing batch 50000...\n",
      "2023-01-22 15:18:54,866:INFO:Running the algorithm... 25\n",
      "2023-01-22 15:18:55,018:INFO:Writing results...\n",
      "2023-01-22 15:18:55,370:INFO:Processing file 26...\n",
      "2023-01-22 15:18:55,926:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:19:28,869:INFO:Encoding context data...\n",
      "2023-01-22 15:19:29,449:INFO:Processing batch 0...\n",
      "2023-01-22 15:19:32,821:INFO:Running the algorithm... 26\n",
      "2023-01-22 15:19:33,247:INFO:Processing batch 10000...\n",
      "2023-01-22 15:19:37,425:INFO:Running the algorithm... 26\n",
      "2023-01-22 15:19:37,789:INFO:Processing batch 20000...\n",
      "2023-01-22 15:19:41,193:INFO:Running the algorithm... 26\n",
      "2023-01-22 15:19:41,570:INFO:Processing batch 30000...\n",
      "2023-01-22 15:19:45,676:INFO:Running the algorithm... 26\n",
      "2023-01-22 15:19:46,066:INFO:Processing batch 40000...\n",
      "2023-01-22 15:19:49,419:INFO:Running the algorithm... 26\n",
      "2023-01-22 15:19:49,782:INFO:Processing batch 50000...\n",
      "2023-01-22 15:19:51,224:INFO:Running the algorithm... 26\n",
      "2023-01-22 15:19:51,398:INFO:Writing results...\n",
      "2023-01-22 15:19:51,740:INFO:Processing file 27...\n",
      "2023-01-22 15:19:52,312:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:20:29,953:INFO:Encoding context data...\n",
      "2023-01-22 15:20:30,972:INFO:Processing batch 0...\n",
      "2023-01-22 15:20:35,754:INFO:Running the algorithm... 27\n",
      "2023-01-22 15:20:36,219:INFO:Processing batch 10000...\n",
      "2023-01-22 15:20:41,907:INFO:Running the algorithm... 27\n",
      "2023-01-22 15:20:42,287:INFO:Processing batch 20000...\n",
      "2023-01-22 15:20:47,079:INFO:Running the algorithm... 27\n",
      "2023-01-22 15:20:47,416:INFO:Processing batch 30000...\n",
      "2023-01-22 15:20:53,023:INFO:Running the algorithm... 27\n",
      "2023-01-22 15:20:53,386:INFO:Processing batch 40000...\n",
      "2023-01-22 15:20:58,254:INFO:Running the algorithm... 27\n",
      "2023-01-22 15:20:58,626:INFO:Processing batch 50000...\n",
      "2023-01-22 15:21:01,030:INFO:Running the algorithm... 27\n",
      "2023-01-22 15:21:01,234:INFO:Writing results...\n",
      "2023-01-22 15:21:01,600:INFO:Processing file 28...\n",
      "2023-01-22 15:21:03,255:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:21:40,129:INFO:Encoding context data...\n",
      "2023-01-22 15:21:40,918:INFO:Processing batch 0...\n",
      "2023-01-22 15:21:44,901:INFO:Running the algorithm... 28\n",
      "2023-01-22 15:21:45,393:INFO:Processing batch 10000...\n",
      "2023-01-22 15:21:50,239:INFO:Running the algorithm... 28\n",
      "2023-01-22 15:21:50,618:INFO:Processing batch 20000...\n",
      "2023-01-22 15:21:54,586:INFO:Running the algorithm... 28\n",
      "2023-01-22 15:21:54,926:INFO:Processing batch 30000...\n",
      "2023-01-22 15:21:59,720:INFO:Running the algorithm... 28\n",
      "2023-01-22 15:22:00,119:INFO:Processing batch 40000...\n",
      "2023-01-22 15:22:04,178:INFO:Running the algorithm... 28\n",
      "2023-01-22 15:22:04,526:INFO:Processing batch 50000...\n",
      "2023-01-22 15:22:07,673:INFO:Running the algorithm... 28\n",
      "2023-01-22 15:22:07,931:INFO:Writing results...\n",
      "2023-01-22 15:22:08,308:INFO:Processing file 29...\n",
      "2023-01-22 15:22:08,862:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:22:40,210:INFO:Encoding context data...\n",
      "2023-01-22 15:22:40,785:INFO:Processing batch 0...\n",
      "2023-01-22 15:22:44,144:INFO:Running the algorithm... 29\n",
      "2023-01-22 15:22:44,651:INFO:Processing batch 10000...\n",
      "2023-01-22 15:22:49,567:INFO:Running the algorithm... 29\n",
      "2023-01-22 15:22:49,985:INFO:Processing batch 20000...\n",
      "2023-01-22 15:22:53,534:INFO:Running the algorithm... 29\n",
      "2023-01-22 15:22:53,972:INFO:Processing batch 30000...\n",
      "2023-01-22 15:22:58,271:INFO:Running the algorithm... 29\n",
      "2023-01-22 15:22:58,653:INFO:Processing batch 40000...\n",
      "2023-01-22 15:23:02,021:INFO:Running the algorithm... 29\n",
      "2023-01-22 15:23:02,349:INFO:Processing batch 50000...\n",
      "2023-01-22 15:23:03,200:INFO:Running the algorithm... 29\n",
      "2023-01-22 15:23:03,312:INFO:Writing results...\n",
      "2023-01-22 15:23:03,653:INFO:Processing file 30...\n",
      "2023-01-22 15:23:04,360:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:23:43,720:INFO:Encoding context data...\n",
      "2023-01-22 15:23:45,232:INFO:Processing batch 0...\n",
      "2023-01-22 15:23:51,737:INFO:Running the algorithm... 30\n",
      "2023-01-22 15:23:52,281:INFO:Processing batch 10000...\n",
      "2023-01-22 15:23:59,772:INFO:Running the algorithm... 30\n",
      "2023-01-22 15:24:00,135:INFO:Processing batch 20000...\n",
      "2023-01-22 15:24:06,604:INFO:Running the algorithm... 30\n",
      "2023-01-22 15:24:06,975:INFO:Processing batch 30000...\n",
      "2023-01-22 15:24:14,230:INFO:Running the algorithm... 30\n",
      "2023-01-22 15:24:14,608:INFO:Processing batch 40000...\n",
      "2023-01-22 15:24:21,081:INFO:Running the algorithm... 30\n",
      "2023-01-22 15:24:21,425:INFO:Processing batch 50000...\n",
      "2023-01-22 15:24:25,804:INFO:Running the algorithm... 30\n",
      "2023-01-22 15:24:26,076:INFO:Writing results...\n",
      "2023-01-22 15:24:26,474:INFO:Processing file 31...\n",
      "2023-01-22 15:24:27,054:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:25:04,699:INFO:Encoding context data...\n",
      "2023-01-22 15:25:06,540:INFO:Processing batch 0...\n",
      "2023-01-22 15:25:13,672:INFO:Running the algorithm... 31\n",
      "2023-01-22 15:25:14,138:INFO:Processing batch 10000...\n",
      "2023-01-22 15:25:22,479:INFO:Running the algorithm... 31\n",
      "2023-01-22 15:25:22,877:INFO:Processing batch 20000...\n",
      "2023-01-22 15:25:30,112:INFO:Running the algorithm... 31\n",
      "2023-01-22 15:25:30,497:INFO:Processing batch 30000...\n",
      "2023-01-22 15:25:38,213:INFO:Running the algorithm... 31\n",
      "2023-01-22 15:25:38,623:INFO:Processing batch 40000...\n",
      "2023-01-22 15:25:45,972:INFO:Running the algorithm... 31\n",
      "2023-01-22 15:25:46,326:INFO:Processing batch 50000...\n",
      "2023-01-22 15:25:50,080:INFO:Running the algorithm... 31\n",
      "2023-01-22 15:25:50,296:INFO:Writing results...\n",
      "2023-01-22 15:25:50,680:INFO:Processing file 32...\n",
      "2023-01-22 15:25:51,232:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:26:27,974:INFO:Encoding context data...\n",
      "2023-01-22 15:26:29,068:INFO:Processing batch 0...\n",
      "2023-01-22 15:26:34,104:INFO:Running the algorithm... 32\n",
      "2023-01-22 15:26:34,544:INFO:Processing batch 10000...\n",
      "2023-01-22 15:26:40,512:INFO:Running the algorithm... 32\n",
      "2023-01-22 15:26:40,888:INFO:Processing batch 20000...\n",
      "2023-01-22 15:26:46,029:INFO:Running the algorithm... 32\n",
      "2023-01-22 15:26:46,375:INFO:Processing batch 30000...\n",
      "2023-01-22 15:26:52,180:INFO:Running the algorithm... 32\n",
      "2023-01-22 15:26:52,557:INFO:Processing batch 40000...\n",
      "2023-01-22 15:26:57,753:INFO:Running the algorithm... 32\n",
      "2023-01-22 15:26:58,097:INFO:Processing batch 50000...\n",
      "2023-01-22 15:27:00,278:INFO:Running the algorithm... 32\n",
      "2023-01-22 15:27:00,456:INFO:Writing results...\n",
      "2023-01-22 15:27:00,810:INFO:Processing file 33...\n",
      "2023-01-22 15:27:02,363:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:27:37,663:INFO:Encoding context data...\n",
      "2023-01-22 15:27:38,856:INFO:Processing batch 0...\n",
      "2023-01-22 15:27:44,373:INFO:Running the algorithm... 33\n",
      "2023-01-22 15:27:44,789:INFO:Processing batch 10000...\n",
      "2023-01-22 15:27:50,853:INFO:Running the algorithm... 33\n",
      "2023-01-22 15:27:51,242:INFO:Processing batch 20000...\n",
      "2023-01-22 15:27:56,694:INFO:Running the algorithm... 33\n",
      "2023-01-22 15:27:57,028:INFO:Processing batch 30000...\n",
      "2023-01-22 15:28:03,287:INFO:Running the algorithm... 33\n",
      "2023-01-22 15:28:03,678:INFO:Processing batch 40000...\n",
      "2023-01-22 15:28:09,145:INFO:Running the algorithm... 33\n",
      "2023-01-22 15:28:09,498:INFO:Processing batch 50000...\n",
      "2023-01-22 15:28:11,592:INFO:Running the algorithm... 33\n",
      "2023-01-22 15:28:11,769:INFO:Writing results...\n",
      "2023-01-22 15:28:12,126:INFO:Processing file 34...\n",
      "2023-01-22 15:28:12,681:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:28:48,899:INFO:Encoding context data...\n",
      "2023-01-22 15:28:49,907:INFO:Processing batch 0...\n",
      "2023-01-22 15:28:54,714:INFO:Running the algorithm... 34\n",
      "2023-01-22 15:28:55,078:INFO:Processing batch 10000...\n",
      "2023-01-22 15:29:00,822:INFO:Running the algorithm... 34\n",
      "2023-01-22 15:29:01,189:INFO:Processing batch 20000...\n",
      "2023-01-22 15:29:06,125:INFO:Running the algorithm... 34\n",
      "2023-01-22 15:29:06,491:INFO:Processing batch 30000...\n",
      "2023-01-22 15:29:12,258:INFO:Running the algorithm... 34\n",
      "2023-01-22 15:29:12,641:INFO:Processing batch 40000...\n",
      "2023-01-22 15:29:17,635:INFO:Running the algorithm... 34\n",
      "2023-01-22 15:29:17,999:INFO:Processing batch 50000...\n",
      "2023-01-22 15:29:20,308:INFO:Running the algorithm... 34\n",
      "2023-01-22 15:29:20,499:INFO:Writing results...\n",
      "2023-01-22 15:29:20,873:INFO:Processing file 35...\n",
      "2023-01-22 15:29:21,424:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:29:56,867:INFO:Encoding context data...\n",
      "2023-01-22 15:29:57,551:INFO:Processing batch 0...\n",
      "2023-01-22 15:30:01,278:INFO:Running the algorithm... 35\n",
      "2023-01-22 15:30:01,646:INFO:Processing batch 10000...\n",
      "2023-01-22 15:30:06,076:INFO:Running the algorithm... 35\n",
      "2023-01-22 15:30:06,426:INFO:Processing batch 20000...\n",
      "2023-01-22 15:30:10,150:INFO:Running the algorithm... 35\n",
      "2023-01-22 15:30:10,548:INFO:Processing batch 30000...\n",
      "2023-01-22 15:30:15,006:INFO:Running the algorithm... 35\n",
      "2023-01-22 15:30:15,384:INFO:Processing batch 40000...\n",
      "2023-01-22 15:30:19,039:INFO:Running the algorithm... 35\n",
      "2023-01-22 15:30:19,376:INFO:Processing batch 50000...\n",
      "2023-01-22 15:30:21,867:INFO:Running the algorithm... 35\n",
      "2023-01-22 15:30:22,104:INFO:Writing results...\n",
      "2023-01-22 15:30:22,653:INFO:Processing file 36...\n",
      "2023-01-22 15:30:23,219:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:31:01,187:INFO:Encoding context data...\n",
      "2023-01-22 15:31:02,357:INFO:Processing batch 0...\n",
      "2023-01-22 15:31:07,639:INFO:Running the algorithm... 36\n",
      "2023-01-22 15:31:08,070:INFO:Processing batch 10000...\n",
      "2023-01-22 15:31:14,100:INFO:Running the algorithm... 36\n",
      "2023-01-22 15:31:14,445:INFO:Processing batch 20000...\n",
      "2023-01-22 15:31:19,662:INFO:Running the algorithm... 36\n",
      "2023-01-22 15:31:20,032:INFO:Processing batch 30000...\n",
      "2023-01-22 15:31:25,979:INFO:Running the algorithm... 36\n",
      "2023-01-22 15:31:26,340:INFO:Processing batch 40000...\n",
      "2023-01-22 15:31:31,777:INFO:Running the algorithm... 36\n",
      "2023-01-22 15:31:32,146:INFO:Processing batch 50000...\n",
      "2023-01-22 15:31:36,105:INFO:Running the algorithm... 36\n",
      "2023-01-22 15:31:36,420:INFO:Writing results...\n",
      "2023-01-22 15:31:36,811:INFO:Processing file 37...\n",
      "2023-01-22 15:31:37,351:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:32:16,798:INFO:Encoding context data...\n",
      "2023-01-22 15:32:17,749:INFO:Processing batch 0...\n",
      "2023-01-22 15:32:22,077:INFO:Running the algorithm... 37\n",
      "2023-01-22 15:32:22,411:INFO:Processing batch 10000...\n",
      "2023-01-22 15:32:27,590:INFO:Running the algorithm... 37\n",
      "2023-01-22 15:32:27,994:INFO:Processing batch 20000...\n",
      "2023-01-22 15:32:32,434:INFO:Running the algorithm... 37\n",
      "2023-01-22 15:32:32,781:INFO:Processing batch 30000...\n",
      "2023-01-22 15:32:38,148:INFO:Running the algorithm... 37\n",
      "2023-01-22 15:32:38,653:INFO:Processing batch 40000...\n",
      "2023-01-22 15:32:43,525:INFO:Running the algorithm... 37\n",
      "2023-01-22 15:32:43,873:INFO:Processing batch 50000...\n",
      "2023-01-22 15:32:49,037:INFO:Running the algorithm... 37\n",
      "2023-01-22 15:32:49,394:INFO:Writing results...\n",
      "2023-01-22 15:32:49,810:INFO:Processing file 38...\n",
      "2023-01-22 15:32:51,773:INFO:Extracting context data from dbpedia properties...\n",
      "2023-01-22 15:33:45,336:INFO:Encoding context data...\n",
      "2023-01-22 15:33:46,252:INFO:Processing batch 0...\n",
      "2023-01-22 15:33:51,161:INFO:Running the algorithm... 38\n",
      "2023-01-22 15:33:51,754:INFO:Processing batch 10000...\n",
      "2023-01-22 15:33:57,473:INFO:Running the algorithm... 38\n",
      "2023-01-22 15:33:58,034:INFO:Processing batch 20000...\n",
      "2023-01-22 15:34:03,068:INFO:Running the algorithm... 38\n",
      "2023-01-22 15:34:03,608:INFO:Processing batch 30000...\n",
      "2023-01-22 15:34:09,266:INFO:Running the algorithm... 38\n",
      "2023-01-22 15:34:09,864:INFO:Processing batch 40000...\n",
      "2023-01-22 15:34:14,664:INFO:Running the algorithm... 38\n",
      "2023-01-22 15:34:15,166:INFO:Processing batch 50000...\n",
      "2023-01-22 15:34:17,964:INFO:Running the algorithm... 38\n",
      "2023-01-22 15:34:18,337:INFO:Writing results...\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,39):\n",
    "    process_batch_v2(i, tfidf, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aef4d4b-16e7-4d56-8804-8c4aa720124f",
   "metadata": {},
   "source": [
    "### CONCAT BATCH RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c0d30db6-45f5-4f37-81bf-50976d7d1175",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file 1\n",
      "Reading file 2\n",
      "Reading file 3\n",
      "Reading file 4\n",
      "Reading file 5\n",
      "Reading file 6\n",
      "Reading file 7\n",
      "Reading file 8\n",
      "Reading file 9\n",
      "Reading file 10\n",
      "Reading file 11\n",
      "Reading file 12\n",
      "Reading file 13\n",
      "Reading file 14\n",
      "Reading file 15\n",
      "Reading file 16\n",
      "Reading file 17\n",
      "Reading file 18\n",
      "Reading file 19\n",
      "Reading file 20\n",
      "Reading file 21\n",
      "Reading file 22\n",
      "Reading file 23\n",
      "Reading file 24\n",
      "Reading file 25\n",
      "Reading file 26\n",
      "Reading file 27\n",
      "Reading file 28\n",
      "Reading file 29\n",
      "Reading file 30\n",
      "Reading file 31\n",
      "Reading file 32\n",
      "Reading file 33\n",
      "Reading file 34\n",
      "Reading file 35\n",
      "Reading file 36\n",
      "Reading file 37\n",
      "Reading file 38\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "concatenated,header_row = [], None\n",
    "for i in range(1,39):\n",
    "    print('Reading file %d' % i)\n",
    "    with open('determine_topics_results_v2/{}.csv'.format(i)) as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                header_row = row\n",
    "                line_count += 1\n",
    "            else:\n",
    "                concatenated.append(row)\n",
    "                line_count += 1\n",
    "\n",
    "with open('determine_topics_results_v2/determine_topics_results_v2.csv'.format(i),mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "    csv_writer.writerow(header_row)\n",
    "    for row in concatenated:\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3904ab-ab1b-4e49-8fe2-cbd5df48465c",
   "metadata": {},
   "source": [
    "### Concatenating v1 and v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2a093f1f-6410-4c7d-9085-552fa7480c4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing v1...\n",
      "Processing v2...\n",
      "Creating index map...\n",
      "Concatenating v1 and v2...\n",
      "Processing row 0\n",
      "Processing row 1000000\n",
      "Processing row 2000000\n",
      "Processing row 3000000\n",
      "Processing row 4000000\n",
      "Processing row 5000000\n",
      "Sorting...\n",
      "Writing results...\n"
     ]
    }
   ],
   "source": [
    "v1, v2, header_row = [], [], None\n",
    "print('Processing v1...')\n",
    "with open('determine_topics_results_v2/determine_topics_results_v1.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            header_row = row\n",
    "            line_count += 1\n",
    "        else:\n",
    "            v1.append(row)\n",
    "            line_count += 1\n",
    "\n",
    "print('Processing v2...')\n",
    "with open('determine_topics_results_v2/determine_topics_results_v2.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            header_row = row\n",
    "            line_count += 1\n",
    "        else:\n",
    "            v2.append(row)\n",
    "            line_count += 1\n",
    "\n",
    "print('Creating index map...')\n",
    "v2_dbpedia_uri_map = HashTable(len(v2))\n",
    "for i,row in enumerate(v2):\n",
    "    v2_dbpedia_uri_map.set_val(row[0],i)\n",
    "\n",
    "print('Concatenating v1 and v2...')\n",
    "concatenated = []\n",
    "for i,row1 in enumerate(v1):\n",
    "    print('Processing row %d' % i) if i % 1000000 == 0 else None\n",
    "    try:\n",
    "        index = v2_dbpedia_uri_map.get_val(row1[0])\n",
    "        concatenated.append(v2[index])\n",
    "    except ValueError:\n",
    "        concatenated.append(row1)\n",
    "\n",
    "print('Sorting...')\n",
    "concatenated = sorted(concatenated, key=lambda row: row[0])\n",
    "\n",
    "print('Writing results...')\n",
    "with open('determine_topics_results_v2/determine_topics_results_v2_concatenated.csv'.format(i),mode='w') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "    csv_writer.writerow(header_row)\n",
    "    for row in concatenated:\n",
    "        csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "96223822-97d8-4d0c-82a4-b7e24537b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "by_category = [{'label': c, 'rows': []} for c in cluster_names]\n",
    "for row in concatenated:\n",
    "    category_name = row[1]\n",
    "    category_name += '-' + row[2] if row[2] != '' else ''\n",
    "    category_name += '-' + row[3] if row[3] != '' else ''\n",
    "    category = [c for c in by_category if c['label'] == category_name]\n",
    "    if len(category) > 0:\n",
    "        category = category[0]\n",
    "        category['rows'].append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "03e2d75d-c9b9-448e-ac47-d86dd78efd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in by_category:\n",
    "    with open('determine_topics_results_v2/by_category/{}.csv'.format(category['label']),mode='w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "        csv_writer.writerow(header_row)\n",
    "        for row in category['rows']:\n",
    "            csv_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "35f3847c-af11-4de5-b629-877149574365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_instance_of(dbpedia_uris):\n",
    "    results, meta = db.cypher_query('MATCH(e:Entity)-[rel:`https://www.wikidata.org/wiki/Property:P31`]-(e2:Entity) ' \n",
    "                                    ' where e.dbpedia_uri in $dbpedia_uris return e.dbpedia_uri, e2.dbpedia_uri, e2.wikidata_id, e2.name', {'dbpedia_uris': dbpedia_uris})\n",
    "    \n",
    "    before_uri, instances = None, []\n",
    "    for row in results:\n",
    "        instanceof_type = 'dbpedia' if row[1] else 'name' if row[3] else 'wikidata'\n",
    "        instanceof = row[1] if row[1] else row[3] if row[3] else row[2]\n",
    "        instances.append((before_uri, instanceof_type, instanceof))\n",
    "        \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "69bd613d-b78e-4e54-9278-a77a16b09d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(380066, 10000)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = [i for i, uri in enumerate(dbpedia_uris) if uri == \"http://dbpedia.org/resource/'s-Graveland\"][0]\n",
    "index, len(result_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "83b5850f-8c0b-4c5f-b86f-03302c024d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['http://dbpedia.org/resource/%22As_the_Old_Sing,_So_Pipe_the_Young%22_(Jan_Steen)',\n",
       "  'Art',\n",
       "  'Painting',\n",
       "  ''],\n",
       " ['http://dbpedia.org/resource/%22Holy...%22', 'Art', 'Painting', ''],\n",
       " [\"http://dbpedia.org/resource/'Tronie'_of_a_Young_Man_with_Gorget_and_Beret\",\n",
       "  'Art',\n",
       "  'Painting',\n",
       "  ''],\n",
       " [\"http://dbpedia.org/resource/'s-Graveland\", 'Art', 'Painting', ''],\n",
       " [\"http://dbpedia.org/resource/'s-Gravenpolder\", 'Art', 'Painting', ''],\n",
       " [\"http://dbpedia.org/resource/'s-Heer_Abtskerke\", 'Art', 'Painting', ''],\n",
       " [\"http://dbpedia.org/resource/'s-Heer_Arendskerke\", 'Art', 'Painting', ''],\n",
       " [\"http://dbpedia.org/resource/'s-Heer_Hendrikskinderen\",\n",
       "  'Art',\n",
       "  'Painting',\n",
       "  ''],\n",
       " [\"http://dbpedia.org/resource/'s-Heerenberg\", 'Art', 'Painting', ''],\n",
       " [\"http://dbpedia.org/resource/'s-Heerenhoek\", 'Art', 'Painting', '']]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "by_category[0]['rows'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2619a603-fde5-4676-8b20-98339f304b88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing category Art-Painting...\n",
      "Writing results...\n",
      "Processing category Art-Painting-Artist...\n",
      "Writing results...\n",
      "Processing category Art-Sculpting...\n",
      "Writing results...\n",
      "Processing category Art-Sculpting-Artist...\n",
      "Writing results...\n",
      "Processing category Art-Music...\n",
      "Writing results...\n",
      "Processing category Art-Music-Instrument...\n",
      "Writing results...\n",
      "Processing category Art-Cinema...\n",
      "Writing results...\n",
      "Processing category Art-Cinema-Actor...\n",
      "2023-01-22 22:41:21,317:ERROR:Failed to read from defunct connection ResolvedIPv4Address(('151.106.35.64', 7687)) (IPv4Address(('151.106.35.64', 7687)))\n",
      "2023-01-22 22:41:21,384:ERROR:Unable to retrieve routing information\n"
     ]
    },
    {
     "ename": "ServiceUnavailable",
     "evalue": "Unable to retrieve routing information",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-132-e0f0e0d9f5ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0minstances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_instance_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0minstances_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minstances\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-91-43e08a5b63a2>\u001b[0m in \u001b[0;36mget_instance_of\u001b[0;34m(dbpedia_uris)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_instance_of\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdbpedia_uris\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     results, meta = db.cypher_query('MATCH(e:Entity)-[rel:`https://www.wikidata.org/wiki/Property:P31`]-(e2:Entity) ' \n\u001b[0;32m----> 3\u001b[0;31m                                     ' where e.dbpedia_uri in $dbpedia_uris return e.dbpedia_uri, e2.dbpedia_uri, e2.wikidata_id, e2.name', {'dbpedia_uris': dbpedia_uris})\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mbefore_uri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstances\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neomodel/util.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0m_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATABASE_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neomodel/util.py\u001b[0m in \u001b[0;36mcypher_query\u001b[0;34m(self, query, params, handle_unique, retry_on_session_expire, resolve_objects)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# Retrieve the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neo4j/work/simple.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, parameters, **kwparameters)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_access_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0mcx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mprotocol_version\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_VERSION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neo4j/work/simple.py\u001b[0m in \u001b[0;36m_connect\u001b[0;34m(self, access_mode, database)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection_acquisition_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mbookmarks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bookmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         )\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neo4j/io/__init__.py\u001b[0m in \u001b[0;36macquire\u001b[0;34m(self, access_mode, timeout, database, bookmarks)\u001b[0m\n\u001b[1;32m   1135\u001b[0m                 address = self._select_address(\n\u001b[1;32m   1136\u001b[0m                     \u001b[0maccess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccess_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1137\u001b[0;31m                     \u001b[0mbookmarks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbookmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1138\u001b[0m                 )\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[#0000]  C: <ACQUIRE ADDRESS> database=%r address=%r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neo4j/io/__init__.py\u001b[0m in \u001b[0;36m_select_address\u001b[0;34m(self, access_mode, database, bookmarks)\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_routing_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m         self.ensure_routing_table_is_fresh(\n\u001b[0;32m-> 1106\u001b[0;31m             \u001b[0maccess_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccess_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbookmarks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbookmarks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m         )\n\u001b[1;32m   1108\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[#0000]  C: <ROUTING TABLE ENSURE FRESH> %r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrouting_tables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neo4j/io/__init__.py\u001b[0m in \u001b[0;36mensure_routing_table_is_fresh\u001b[0;34m(self, access_mode, database, bookmarks)\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1088\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_routing_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbookmarks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbookmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1089\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_connection_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/neo4j/io/__init__.py\u001b[0m in \u001b[0;36mupdate_routing_table\u001b[0;34m(self, database, bookmarks)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         \u001b[0;31m# None of the routers have been successful, so just fail\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to retrieve routing information\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mServiceUnavailable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unable to retrieve routing information\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_connection_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatabase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m: Unable to retrieve routing information"
     ]
    }
   ],
   "source": [
    "for category in by_category:\n",
    "    print('Processing category %s...' % category['label'])\n",
    "    rows = []\n",
    "    with open('determine_topics_results_v2/by_category/{}.csv'.format(category['label']),mode='r') as csv_file:\n",
    "        csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "        line_count = 0\n",
    "        for row in csv_reader:\n",
    "            if line_count == 0:\n",
    "                header_row = row\n",
    "                line_count += 1\n",
    "            else:\n",
    "                rows.append(row)\n",
    "                line_count += 1\n",
    "        \n",
    "        instances, BATCH_SIZE = [], 100\n",
    "        for i in range(0,len(rows),BATCH_SIZE):\n",
    "            batch = [row[0] for row in rows[i: min(i + BATCH_SIZE, len(rows))]]\n",
    "            instances.extend(get_instance_of(batch))\n",
    "\n",
    "        instances_ = [instance[1] + '-' + instance[2] for instance in instances]\n",
    "        counts = [(instance,instances_.count(instance)) for instance in set(instances_)]\n",
    "        counts = sorted(counts, key=lambda c: c[1], reverse=True)\n",
    "        \n",
    "        print('Writing results...')\n",
    "        \n",
    "    with open('determine_topics_results_v2/instance_counts/{}.csv'.format(category['label']),mode='w') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file, delimiter=',')\n",
    "        csv_writer.writerow(['Instance', 'Count'])\n",
    "        for row in counts:\n",
    "            csv_writer.writerow(row)\n",
    "                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3110697d-77fc-463a-a8dc-4499c7b2c918",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
