{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30d53814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pickle5 as pickle\n",
    "import tempfile\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.layers import Dense, Input, Embedding, Activation, Softmax\n",
    "from keras.layers import CuDNNLSTM\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv1D,Conv2D, ZeroPadding2D, MaxPooling1D, MaxPooling2D\n",
    "from keras.layers import RepeatVector, Permute, Add, Concatenate, Reshape, Dot\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from stellargraph.data import BiasedRandomWalk\n",
    "from stellargraph import StellarGraph, IndexedArray\n",
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from graph_visualization import GraphVisualization\n",
    "\n",
    "np.random.seed(100)\n",
    "\n",
    "MAX_NUM_WORDS = 20000\n",
    "VALIDATION_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.2\n",
    "\n",
    "NUM_RELATIONS_PER_CLUSTER = 67\n",
    "NUM_ENTITIES_PER_CLUSTER = 400\n",
    "NUM_CLUSTERS = 20\n",
    "\n",
    "def get_clusters(cluster_file, num_things_per_cluster):\n",
    "    clusters = []  # np.ones(shape=(NUM_RELATIONS_PER_CLUSTER,KG_EMBEDDING_DIM))\n",
    "\n",
    "    with open(cluster_file, 'r', encoding='utf8') as f:\n",
    "        lines = []\n",
    "        for line in f:\n",
    "            elements = line.split()\n",
    "            x = [[e] for e in elements]\n",
    "            lines.append(x)\n",
    "\n",
    "    for i in range(0, len(lines) - num_things_per_cluster + 1, num_things_per_cluster):\n",
    "        # print(\"appending: {} to {}\".format(i,i+num_things_per_cluster))\n",
    "        clusters.append(lines[i:i + num_things_per_cluster])\n",
    "\n",
    "    clusters = np.asarray(clusters, dtype='float32')\n",
    "    return clusters\n",
    "\n",
    "\n",
    "def pickler(path, pkl_name, obj):\n",
    "    with open(os.path.join(path, pkl_name), 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "def unpickler(path, pkl_name):\n",
    "    with open(os.path.join(path, pkl_name), 'rb') as f:\n",
    "        obj = pickle.load(f)\n",
    "    return obj\n",
    "\n",
    "def get_labels(data):\n",
    "    labels = []\n",
    "    for d in data:\n",
    "        labels.append(d['label']) if d['label'] not in labels else None\n",
    "\n",
    "    return labels\n",
    "\n",
    "\n",
    "def get_x_and_y(data):\n",
    "    x, y = [], []\n",
    "    for d in data:\n",
    "        for dd in d['data']:\n",
    "            tmp = dd['text'].replace('\\n', '').replace('_', '')  # clean\n",
    "            x.append({'label': d['label'], 'dbpedia_uri': dd['dbpedia_uri'], 'context_data': dd['context_data'], 'text': tmp, 'graph': dd['graph']}) if len(tmp) > 0 else None\n",
    "            y.append(d['label']) if len(tmp) > 0 else None\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_label_index(label):\n",
    "    return [index for index, _label in enumerate(unique_labels) if label == _label][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c31bb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_clusters(arr):    \n",
    "    result = []\n",
    "    for el in arr:\n",
    "        all_mappings = node_cluster_mapping_with_count[el] if el in node_cluster_mapping_with_count else []\n",
    "        filtered_mappings = [e[0] for e in all_mappings[:min(len(all_mappings),3)]]\n",
    "        result.extend(filtered_mappings)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_context_data(data):\n",
    "    for d in data:\n",
    "        for dd in d['data']:\n",
    "            dd['context_data'] = [c for c in set(find_clusters(dd['context_graph']['nodes']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9453ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_embeddings_v3 = unpickler('data','node_embeddings_v3.pkl')\n",
    "node_embeddings_v4 = unpickler('data','node_embeddings_v4.pkl')\n",
    "graph = unpickler('data','graph_v3.pkl')\n",
    "clusters = unpickler('data','node_clusters_v5.pkl')\n",
    "clusters_with_count = unpickler('data','node_clusters_v5_with_count.pkl')\n",
    "node_cluster_mapping_with_count= unpickler('data','node_cluster_mapping_v5_with_count.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9156d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = unpickler('data','classification_data_with_graphs_v5.pkl')\n",
    "\n",
    "unique_labels = get_labels(data)\n",
    "\n",
    "get_context_data(data)\n",
    "\n",
    "x,y = get_x_and_y(data)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "x_val, y_val,x_test, y_test = x_test[:500], y_test[:500], x_test[500:], y_test[500:]\n",
    "y_train_int, y_test_int = [get_label_index(label) for label in y_train],[get_label_index(label) for label in y_test]\n",
    "x_train_text,x_val_text,x_test_text = [xx['text'] for xx in x_train],[xx['text'] for xx in x_val], [xx['text'] for xx in x_test]\n",
    "x_train_context, x_val_context, x_test_context = [xx['context_data'] for xx in x_train],[xx['context_data'] for xx in x_val], [xx['context_data'] for xx in x_test]\n",
    "x_train_text, x_test_text, y_train_int = np.array(x_train_text), np.array(x_test_text), np.array(y_train_int)\n",
    "\n",
    "doc_indexes = list(range(0,len(x_train_text)))\n",
    "# doc_indexes = list(map(lambda x: x + len(unique_labels), doc_indexes))\n",
    "doc_indexes = np.array(doc_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "116d1656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    1,    2, ..., 4490, 4491, 4492])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdc03d3",
   "metadata": {},
   "source": [
    "## DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d9f63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "sequence_length = 200\n",
    "\n",
    "vectorize_layer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length\n",
    ")\n",
    "\n",
    "with tf.device(\"/CPU:0\"):\n",
    "    vectorize_layer.adapt(x_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac911605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "text_vectorization (TextVectori (None, 200)          0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 200, 100)     500100      text_vectorization[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 100)     500100      text_vectorization[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 100)          0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 100)          0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 78)           7878        global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 4493)         453793      global_average_pooling1d_1[0][0] \n",
      "==================================================================================================\n",
      "Total params: 1,461,871\n",
      "Trainable params: 1,461,871\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_TOKENS_NUM = 5000  # Maximum vocab size.\n",
    "EMBEDDING_DIMS = 100\n",
    "\n",
    "text_input = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "vectorized_layer = vectorize_layer(text_input)\n",
    "\n",
    "text_layer = tf.keras.layers.Embedding(MAX_TOKENS_NUM + 1, EMBEDDING_DIMS)(vectorized_layer)\n",
    "text_layer = tf.keras.layers.GlobalAveragePooling1D()(text_layer)\n",
    "\n",
    "text_layer_doc = tf.keras.layers.Embedding(MAX_TOKENS_NUM + 1, EMBEDDING_DIMS)(vectorized_layer)\n",
    "text_layer_doc = tf.keras.layers.GlobalAveragePooling1D()(text_layer_doc)\n",
    "\n",
    "text_layer_output = tf.keras.layers.Dense(units=len(unique_labels))(text_layer)\n",
    "text_layer_doc_output = tf.keras.layers.Dense(units=len(x_train))(text_layer_doc)\n",
    "\n",
    "# final_output = tf.keras.layers.Concatenate(axis=1)([text_layer_output,text_layer_doc_output])\n",
    "\n",
    "model_simple = tf.keras.models.Model(inputs=[text_input], outputs=[text_layer_output,text_layer_doc_output])\n",
    "model_simple.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=tf.metrics.SparseCategoricalAccuracy())\n",
    "model_simple.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a21c17a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 1.2883 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.7723 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 2/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 1.2190 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.7823 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 3/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 1.1554 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.7932 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 4/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 1.0965 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.7984 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 5/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 1.0415 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8139 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 6/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.9902 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8177 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 7/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.9425 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8233 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 8/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.8980 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8335 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 9/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.8559 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8402 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 10/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.8161 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8455 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 11/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.7790 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8551 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 12/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.7435 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8613 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 13/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.7110 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8653 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 14/20\n",
      "141/141 [==============================] - 2s 15ms/step - loss: nan - dense_loss: 0.6790 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8709 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 15/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.6497 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8760 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 16/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.6213 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8832 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 17/20\n",
      "141/141 [==============================] - 2s 15ms/step - loss: nan - dense_loss: 0.5946 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8905 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 18/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.5694 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8954 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 19/20\n",
      "141/141 [==============================] - 2s 14ms/step - loss: nan - dense_loss: 0.5453 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.8976 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n",
      "Epoch 20/20\n",
      "141/141 [==============================] - 2s 15ms/step - loss: nan - dense_loss: 0.5226 - dense_1_loss: nan - dense_sparse_categorical_accuracy: 0.9030 - dense_1_sparse_categorical_accuracy: 2.2257e-04\n"
     ]
    }
   ],
   "source": [
    "epochs =  20\n",
    "history = model_simple.fit(\n",
    "    x_train_text,\n",
    "    y=(y_train_int,doc_indexes),\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "91c5ba29",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sparse_categorical_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-6ee3f6f99238>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sparse_categorical_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sparse_categorical_accuracy'"
     ]
    }
   ],
   "source": [
    "plt.plot(history.history['sparse_categorical_accuracy'])\n",
    "plt.show()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79583e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.63\n",
      "                              precision    recall  f1-score   support\n",
      "\n",
      "                Architecture       0.84      0.84      0.84        25\n",
      "      Architecture-Structure       0.77      0.77      0.77        26\n",
      "                  Art-Cinema       0.60      0.50      0.55        24\n",
      "            Art-Cinema-Actor       0.12      0.04      0.06        23\n",
      "                   Art-Dance       0.88      0.67      0.76        21\n",
      "            Art-Dance-Dancer       0.47      0.68      0.56        22\n",
      "                 Art-Fashion       0.76      0.67      0.71        24\n",
      "        Art-Fashion-Designer       0.56      0.58      0.57        24\n",
      "           Art-Fashion-Model       0.00      0.00      0.00        11\n",
      "              Art-Literature       0.60      0.54      0.57        28\n",
      "       Art-Literature-Writer       0.52      0.53      0.52        30\n",
      "                   Art-Music       0.82      0.87      0.84        31\n",
      "        Art-Music-Instrument       0.87      0.80      0.83        25\n",
      "                Art-Painting       0.73      0.86      0.79        28\n",
      "         Art-Painting-Artist       0.43      0.54      0.48        28\n",
      "             Art-Photography       0.00      0.00      0.00         5\n",
      "Art-Photography-Photographer       0.64      0.31      0.42        29\n",
      "               Art-Sculpting       0.66      0.66      0.66        29\n",
      "        Art-Sculpting-Artist       0.75      0.72      0.74        29\n",
      "                 Art-Theatre       0.70      0.88      0.78        24\n",
      "           Art-Theatre-Actor       0.45      0.87      0.60        23\n",
      "                   Astronomy       0.65      0.68      0.67        22\n",
      "           Astronomy-Program       0.00      0.00      0.00         3\n",
      "                     Culture       0.43      0.50      0.47        20\n",
      "             Culture-Country       0.26      0.18      0.21        28\n",
      "   Culture-Historical Figure       0.40      0.62      0.49        16\n",
      "                       Media       0.64      0.58      0.61        24\n",
      "                 Media-Anime       0.76      0.79      0.78        24\n",
      "               Media-Cartoon       0.74      0.69      0.71        29\n",
      "           Media-Documentary       0.66      0.83      0.73        23\n",
      "                  Media-News       0.38      0.50      0.43        16\n",
      "     Media-TV Series & Shows       0.91      0.42      0.57        24\n",
      "                    Military       0.33      0.15      0.21        27\n",
      "           Military-Aviation       0.65      0.58      0.61        26\n",
      "               Military-Land       0.87      0.83      0.85        24\n",
      "              Military-Naval       0.71      0.87      0.78        23\n",
      "             Military-Weapon       0.78      0.72      0.75        25\n",
      "                   Mythology       0.55      0.50      0.52        24\n",
      "      Mythology-Supernatural       0.61      0.48      0.54        29\n",
      "               Nature-Animal       0.47      0.56      0.51        27\n",
      "                 Nature-Food       0.20      0.21      0.20        24\n",
      "        Nature-Microorganism       0.64      0.47      0.54        15\n",
      "                Nature-Plant       0.50      0.50      0.50        30\n",
      "                       Other       0.25      1.00      0.41        26\n",
      "                  Philosophy       0.41      0.38      0.39        29\n",
      "                    Religion       0.58      0.79      0.67        24\n",
      "         Science-Agriculture       0.81      0.50      0.62        26\n",
      "         Science-Antropology       0.58      0.58      0.58        19\n",
      "          Science-Archeology       0.86      0.92      0.89        26\n",
      "             Science-Biology       0.67      0.87      0.75        23\n",
      "           Science-Chemistry       0.68      0.50      0.58        26\n",
      "      Science-Earth Sciences       0.89      0.74      0.81        23\n",
      "           Science-Economics       0.70      0.70      0.70        27\n",
      "         Science-Mathematics       0.75      0.94      0.83        16\n",
      "            Science-Medicine       0.00      0.00      0.00        12\n",
      "         Science-Meteorology       0.79      0.62      0.70        24\n",
      "       Science-Paleonthology       0.88      0.75      0.81        20\n",
      "             Science-Physics       0.68      0.83      0.75        23\n",
      "            Science-Politics       0.64      0.67      0.65        21\n",
      "          Science-Psychology       0.64      0.32      0.43        28\n",
      "     Science-Social Sciences       0.46      0.57      0.51        28\n",
      "                      Sports       0.80      0.83      0.82        24\n",
      "Technology-Civil Engineering       0.79      0.65      0.71        23\n",
      " Technology-Computer Science       0.56      0.41      0.47        22\n",
      "      Technology-Electronics       0.53      0.59      0.56        27\n",
      "        Technology-Mechanics       0.33      0.60      0.43        15\n",
      "     Technology-Mechatronics       0.00      0.00      0.00         1\n",
      "         Technology-Robotics       0.88      0.67      0.76        21\n",
      "       Technology-Video Game       0.92      0.77      0.84        31\n",
      "              Transportation       0.93      0.93      0.93        29\n",
      "     Transportation-Aviation       0.00      0.00      0.00        10\n",
      "         Transportation-Land       0.93      0.96      0.95        27\n",
      "        Transportation-Naval       1.00      0.85      0.92        26\n",
      "      Transportation-Railway       0.96      1.00      0.98        24\n",
      "\n",
      "                    accuracy                           0.63      1713\n",
      "                   macro avg       0.60      0.59      0.58      1713\n",
      "                weighted avg       0.64      0.63      0.62      1713\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/cnytync/Dev/anaconda3/envs/Python36/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = model_simple.predict(x_test_text)\n",
    "def get_result_labels(results):\n",
    "    return [unique_labels[np.where(row==max(row))[0][0]] for row in results]\n",
    "\n",
    "result_labels = get_result_labels(results[0])\n",
    "print('Accuracy score: %.2f' % accuracy_score(result_labels, y_test))\n",
    "print(classification_report(y_test, result_labels))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python36",
   "language": "python",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
